{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109575ce",
   "metadata": {},
   "source": [
    "# HP AI Blueprints - BERT QA Deployment\n",
    "Downloaded from: https://github.com/HPInc/AI-Blueprints/blob/main/deep-learning/bert_qa/notebooks/Deployment.ipynb\n",
    "\n",
    "This notebook demonstrates the official HP AI Studio deployment patterns for MLflow model registration and deployment.\n",
    "\n",
    "## Key HP AI Studio Patterns Demonstrated:\n",
    "1. **Phoenix MLflow Configuration**: `/phoenix/mlflow` tracking URI\n",
    "2. **Model Registry Integration**: Proper model versioning and registration\n",
    "3. **Model Signatures**: Input/output schema definitions\n",
    "4. **Artifact Management**: Model artifact storage and retrieval\n",
    "5. **Production Deployment**: HP AI Studio compatible model classes\n",
    "\n",
    "This serves as the reference implementation for integrating Orpheus Engine with HP AI Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80058a5",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Install requirements and Imports Dependencies\n",
    "- Configurations\n",
    "- Model Load\n",
    "- Model Registry\n",
    "- Testing latest model registred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52cb340",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import logging\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Third-Party Libraries\n",
    "import shutil\n",
    "\n",
    "# MLflow for Experiment Tracking and Model Management\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.types import ParamSchema, ParamSpec\n",
    "from mlflow.models import ModelSignature\n",
    "\n",
    "# Transformers\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbdf223",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56072ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7128fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global experiment and run names to be used throughout the notebook\n",
    "MODEL_PERSONAL_NAME = \"morgana-rodrigues/bert_qa\"\n",
    "EXPERIMENT_NAME = \"BERT model for Q&A\"\n",
    "MODEL_NAME = \"BERT_QA\"\n",
    "RUN_NAME = 'BERT_QA'\n",
    "NAME = 'BERT_QA'\n",
    "\n",
    "\n",
    "# Set up the chunk separator for text processing\n",
    "CHUNK_SEPARATOR = \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9609ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create logger ===\n",
    "logger = logging.getLogger(\"deployment-notebook\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", \n",
    "                             datefmt=\"%Y-%m-%d %H:%M:%S\") \n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b791b",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "Load a Transformer model from Hugging Face for local use in a pipeline object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43afa510",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = MODEL_PERSONAL_NAME\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    'question-answering',\n",
    "    model=model_name,\n",
    "    device=0 # -1 means running on CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f3d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipeline(context=\"Take me down to Paradise City where the grass is green and the girls are pretty\", question=\"What colour is the grass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220ab4c2",
   "metadata": {},
   "source": [
    "## HP AI Studio Model Class\n",
    "\n",
    "This class encapsulates the model in the format required for HP AI Studio MLflow deployment. It follows the official HP AI Blueprints pattern for model packaging and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81221384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBERTModel(mlflow.pyfunc.PythonModel):\n",
    "    def _preprocess(self, inputs):\n",
    "        \"\"\"\n",
    "        Preprocesses the input data.\n",
    "\n",
    "        Args:\n",
    "            inputs: A dictionary containing two keys:\n",
    "                - 'context': A list with the context text.\n",
    "                - 'question': A list with the question to be answered.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the context (str) and the question (str).\n",
    "        \"\"\"\n",
    "        try:\n",
    "                context = inputs['context'][0]\n",
    "                question = inputs['question'][0]\n",
    "                print(\"pre processing\", context,question)\n",
    "                return context, question\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error preprocessing the input data: {str(e)}\")  \n",
    "\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Loads the question-answering pipeline using the saved model artifact.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context object \n",
    "                containing the loaded artifacts.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model = pipeline(\n",
    "            'question-answering',\n",
    "            model=context.artifacts[\"model\"],\n",
    "             device=0\n",
    "    )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading the question-answering pipeline: {str(e)}\")     \n",
    "\n",
    "    def predict(self, context, model_input, params):\n",
    "        \"\"\"\n",
    "        Runs inference using the loaded model and input data.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context object \n",
    "                with access to artifacts.\n",
    "            model_input: A dictionary containing 'context' and 'question' keys.\n",
    "\n",
    "        Returns:\n",
    "            The output from the model containing the predicted answer and optionally the score.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            in_ctx, question = self._preprocess(model_input)\n",
    "            output = self.model(context=in_ctx, question=question)\n",
    "            return output\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error running inference: {str(e)}\")  \n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, model_name, source_trainer = None, source_pipeline = None, demo_folder=\"../demo\"):\n",
    "        \"\"\"\n",
    "        Logs the model to MLflow, including artifacts, dependencies, and input/output signatures.\n",
    "\n",
    "        Args:\n",
    "            model_name: Path where the model will be temporarily saved before logging.\n",
    "            source_trainer: A trainer object with a `.save_model()` method. Defaults to None.\n",
    "            source_pipeline: A pipeline object with a `.save_pretrained()` method. Defaults to None.\n",
    "            demo_folder: Path to the folder containing the compiled demo UI. Defaults to \"demo\".\n",
    "        \"\"\"\n",
    "        try:\n",
    "            input_schema = Schema(\n",
    "            [\n",
    "                ColSpec(\"string\", \"context\"),\n",
    "                ColSpec(\"string\", \"question\"),\n",
    "            ]\n",
    "            )\n",
    "            output_schema = Schema(\n",
    "                [\n",
    "                    ColSpec(\"string\", \"answer\")\n",
    "                ]\n",
    "            )\n",
    "        \n",
    "            params_schema = ParamSchema(\n",
    "                [\n",
    "                    ParamSpec(\"show_score\", \"boolean\", False)\n",
    "                ]\n",
    "            )\n",
    "          \n",
    "            signature = ModelSignature(inputs=input_schema, outputs=output_schema, params=params_schema)\n",
    "            if source_trainer is not None:\n",
    "                source_trainer.save_model(model_name)\n",
    "            elif source_pipeline is not None:\n",
    "                source_pipeline.save_pretrained(model_name)\n",
    "                 \n",
    "            requirements = [\n",
    "                \"transformers==4.48.0\",\n",
    "                \"tf_keras\"\n",
    "            ]\n",
    "            mlflow.pyfunc.log_model(\n",
    "                model_name,\n",
    "                python_model=cls(),\n",
    "                artifacts={\"model\": model_name, \"demo\": demo_folder},\n",
    "                signature=signature,\n",
    "                pip_requirements=requirements\n",
    "            )\n",
    "            shutil.rmtree(model_name)\n",
    "            logger.info(\"Logging model to MLflow done successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error logging model to MLflow: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687593ed",
   "metadata": {},
   "source": [
    "# HP AI Studio Model Registry\n",
    "\n",
    "This section demonstrates the official HP AI Studio pattern for model registration using Phoenix MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e98b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HP AI Studio Phoenix MLflow Configuration\n",
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "mlflow.set_experiment(experiment_name = EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f427d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Model to HP AI Studio\n",
    "with mlflow.start_run(run_name= RUN_NAME) as run:\n",
    "    logger.info(f\"Run's Artifact URI: {run.info.artifact_uri}\")\n",
    "    DistilBERTModel.log_model(model_name = MODEL_NAME, source_pipeline=qa_pipeline)\n",
    "    mlflow.register_model(model_uri = f\"runs:/{run.info.run_id}/{MODEL_NAME}\", name = NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ecaf45",
   "metadata": {},
   "source": [
    "# Testing HP AI Studio Registered Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26967934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Latest Model Version from HP AI Studio Registry\n",
    "client = mlflow.MlflowClient()\n",
    "model_metadata = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])\n",
    "latest_model_version = model_metadata[0].version\n",
    "print(latest_model_version, mlflow.models.get_model_info(f\"models:/BERT_QA/{latest_model_version}\").signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2201dc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Model Inference\n",
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/BERT_QA/{latest_model_version}\")\n",
    "context = \"Marta is mother of John and Amanda\"\n",
    "question = \"what is the name of Marta's daugther?\"\n",
    "model.predict({\"context\": [context], \"question\":[question]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed00875",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
