{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Retrieval-Augmented Generation (RAG) with Local Llama 2 & ChromaDB\n",
    "\n",
    "## Overview\n",
    "This notebook implements an **Agentic Retrieval-Augmented Generation (RAG) pipeline**. It focuses on transcribing audio data, potentially from an Omi streaming device, storing both the transcription and audio, and then using the transcription with a local **Ai Studio** model and **ChromaDB** for intelligent question-answering. The system determines whether additional context is needed before generating responses.\n",
    "\n",
    "### Key Features:\n",
    "- **Audio Transcription Workflow** for processing data from devices like Omi.\n",
    "- **Storage of Audio and Transcriptions** for AI processing.\n",
    "- **Llama 2 Model** for high-quality text generation.\n",
    "- **ChromaDB Vector Store** for efficient semantic search on transcriptions.\n",
    "- **Dynamic Context Retrieval** to improve answer accuracy.\n",
    "- **Two Answering Modes**:\n",
    "  - With RAG (Retrieves relevant document content before responding).\n",
    "  - Without RAG (Directly generates responses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pysqlite3 as sqlite3 backend.\n",
      "Loaded sqlite3 version: 3.45.3\n",
      "âœ… TensorBoard integration available\n",
      "ðŸ“Š Orpheus Engine TensorBoard Integration Module Loaded\n",
      "   â€¢ TensorBoard Available: âœ…\n",
      "   â€¢ Audio Libraries Available: âœ…\n",
      "   â€¢ Real-time monitoring ready\n",
      "   â€¢ HP AI Studio integration enabled\n",
      "ðŸ“Š TensorBoard writer created: default\n",
      "   Log directory: tensorboard_logs/agentic_rag/agentic_rag_audio_pipeline/default/20250610-032038\n",
      "ðŸš€ TensorBoard started successfully!\n",
      "ðŸ“Š TensorBoard URL: http://localhost:6006\n",
      "ðŸ“ Log directory: tensorboard_logs/agentic_rag/agentic_rag_audio_pipeline\n",
      "ðŸ“Š Database setup metrics logged to TensorBoard\n"
     ]
    }
   ],
   "source": [
    "# Force Python to use the latest system sqlite3 (if available)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Unset any pysqlite3 monkeypatching\n",
    "if \"pysqlite3\" in sys.modules:\n",
    "    del sys.modules[\"pysqlite3\"]\n",
    "if \"sqlite3\" in sys.modules:\n",
    "    del sys.modules[\"sqlite3\"]\n",
    "\n",
    "try:\n",
    "    import pysqlite3\n",
    "    sys.modules[\"sqlite3\"] = pysqlite3\n",
    "    sys.modules[\"pysqlite3\"] = pysqlite3\n",
    "    print(\"Using pysqlite3 as sqlite3 backend.\")\n",
    "    sqlite_backend = \"pysqlite3\"\n",
    "except ImportError:\n",
    "    print(\"pysqlite3 not found, using default sqlite3 module.\")\n",
    "    sqlite_backend = \"default\"\n",
    "\n",
    "import sqlite3\n",
    "print(\"Loaded sqlite3 version:\", sqlite3.sqlite_version)\n",
    "\n",
    "# Initialize TensorBoard logging for database setup\n",
    "try:\n",
    "    from tensorboard_integration import OrpheusTensorBoardManager\n",
    "    tensorboard_manager = OrpheusTensorBoardManager(\n",
    "        log_dir=\"./tensorboard_logs/agentic_rag\",\n",
    "        experiment_name=\"agentic_rag_audio_pipeline\",\n",
    "        hp_ai_studio_compatible=True\n",
    "    )\n",
    "    # Log database configuration\n",
    "    tensorboard_manager.log_scalar(\"database_setup/sqlite_version\", float(sqlite3.sqlite_version.replace('.', '')[:3])/100, 0)\n",
    "    print(\"ðŸ“Š Database setup metrics logged to TensorBoard\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ TensorBoard integration not available yet - will initialize after requirements installation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Cython>=0.29.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (3.1.2)\n",
      "Requirement already satisfied: mlflow==2.15.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (2.15.0)\n",
      "Requirement already satisfied: mlflow-skinny==2.15.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (2.15.0)\n",
      "Requirement already satisfied: transformers==4.48.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (4.48.0)\n",
      "Requirement already satisfied: tf_keras==2.19.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (2.19.0)\n",
      "Requirement already satisfied: evaluate==0.4.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (0.4.3)\n",
      "Requirement already satisfied: accelerate==1.5.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (1.5.2)\n",
      "Requirement already satisfied: datasets==3.4.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (3.4.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 19)) (1.26.4)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 20)) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn>=1.3.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 21)) (1.5.1)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 22)) (3.9.2)\n",
      "Requirement already satisfied: seaborn>=0.12.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 23)) (0.13.2)\n",
      "Requirement already satisfied: librosa>=0.10.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 26)) (0.11.0)\n",
      "Requirement already satisfied: pyloudnorm>=0.1.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 27)) (0.1.1)\n",
      "Requirement already satisfied: soundfile>=0.12.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 28)) (0.12.1)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 29)) (1.13.1)\n",
      "Requirement already satisfied: pooch>=1.6.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 30)) (1.8.2)\n",
      "Requirement already satisfied: audioread>=3.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 31)) (3.0.1)\n",
      "Requirement already satisfied: tensorboard>=2.15.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 34)) (2.19.0)\n",
      "Collecting tensorboard-plugin-profile>=2.14.0 (from -r requirements.txt (line 35))\n",
      "  Downloading tensorboard_plugin_profile-2.19.9-cp312-none-macosx_12_0_arm64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 36)) (2.7.0)\n",
      "Requirement already satisfied: tensorflow>=2.15.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 37)) (2.19.0)\n",
      "Requirement already satisfied: fastapi>=0.104.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 40)) (0.111.0)\n",
      "Requirement already satisfied: uvicorn>=0.24.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 41)) (0.30.6)\n",
      "Requirement already satisfied: gunicorn>=21.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 42)) (22.0.0)\n",
      "Requirement already satisfied: jupyter>=1.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 45)) (1.0.0)\n",
      "Requirement already satisfied: jupyterlab>=4.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 46)) (4.2.5)\n",
      "Requirement already satisfied: notebook>=7.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 47)) (7.2.2)\n",
      "Collecting ipywidgets>=8.0.0 (from -r requirements.txt (line 48))\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: ipykernel>=6.25.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 49)) (6.28.0)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 52)) (4.25.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 53)) (2.9.2)\n",
      "Requirement already satisfied: sqlalchemy<3.0.0,>=1.4.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 54)) (2.0.32)\n",
      "Requirement already satisfied: alembic<2.0.0,>=1.8.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 55)) (1.14.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 56)) (8.1.8)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 57)) (2.32.3)\n",
      "Requirement already satisfied: plotly>=5.17.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 60)) (5.24.1)\n",
      "Collecting dash>=2.14.0 (from -r requirements.txt (line 61))\n",
      "  Downloading dash-3.0.4-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: bokeh>=3.2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 62)) (3.6.0)\n",
      "Requirement already satisfied: pytest>=7.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 65)) (8.3.5)\n",
      "Requirement already satisfied: black>=23.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 66)) (24.8.0)\n",
      "Requirement already satisfied: flake8>=6.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 67)) (7.0.0)\n",
      "Requirement already satisfied: pytest-cov>=4.1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 77)) (6.1.1)\n",
      "Requirement already satisfied: Flask<4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from mlflow==2.15.0->-r requirements.txt (line 8)) (3.1.0)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from mlflow==2.15.0->-r requirements.txt (line 8)) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from mlflow==2.15.0->-r requirements.txt (line 8)) (3.4.3)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from mlflow==2.15.0->-r requirements.txt (line 8)) (3.7)\n",
      "Requirement already satisfied: pyarrow<16,>=4.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from mlflow==2.15.0->-r requirements.txt (line 8)) (15.0.2)\n",
      "Requirement already satisfied: querystring-parser<2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from mlflow==2.15.0->-r requirements.txt (line 8)) (1.2.4)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from mlflow==2.15.0->-r requirements.txt (line 8)) (3.1.6)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->-r requirements.txt (line 9)) (5.5.2)\n",
      "Requirement already satisfied: cloudpickle<4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->-r requirements.txt (line 9)) (3.0.0)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->-r requirements.txt (line 9)) (0.53.0)\n",
      "Requirement already satisfied: entrypoints<1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->-r requirements.txt (line 9)) (0.4)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->-r requirements.txt (line 9)) (3.1.44)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->-r requirements.txt (line 9)) (7.2.1)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->-r requirements.txt (line 9)) (1.33.1)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->-r requirements.txt (line 9)) (1.33.1)\n",
      "Requirement already satisfied: packaging<25 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->-r requirements.txt (line 9)) (23.2)\n",
      "Requirement already satisfied: pytz<2025 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->-r requirements.txt (line 9)) (2024.2)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->-r requirements.txt (line 9)) (6.0.2)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->-r requirements.txt (line 9)) (0.5.3)\n",
      "Requirement already satisfied: filelock in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from transformers==4.48.0->-r requirements.txt (line 12)) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from transformers==4.48.0->-r requirements.txt (line 12)) (0.32.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from transformers==4.48.0->-r requirements.txt (line 12)) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from transformers==4.48.0->-r requirements.txt (line 12)) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from transformers==4.48.0->-r requirements.txt (line 12)) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from transformers==4.48.0->-r requirements.txt (line 12)) (4.67.1)\n",
      "Requirement already satisfied: dill in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from evaluate==0.4.3->-r requirements.txt (line 14)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from evaluate==0.4.3->-r requirements.txt (line 14)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from evaluate==0.4.3->-r requirements.txt (line 14)) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate==0.4.3->-r requirements.txt (line 14)) (2024.12.0)\n",
      "Requirement already satisfied: psutil in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from accelerate==1.5.2->-r requirements.txt (line 15)) (5.9.0)\n",
      "Requirement already satisfied: aiohttp in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from datasets==3.4.0->-r requirements.txt (line 16)) (3.11.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=2.0.0->-r requirements.txt (line 20)) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=2.0.0->-r requirements.txt (line 20)) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 21)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 21)) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 22)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 22)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 22)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 22)) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 22)) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 22)) (3.1.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.15.0->-r requirements.txt (line 37)) (2.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.15.0->-r requirements.txt (line 37)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.15.0->-r requirements.txt (line 37)) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.15.0->-r requirements.txt (line 37)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.15.0->-r requirements.txt (line 37)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.15.0->-r requirements.txt (line 37)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.15.0->-r requirements.txt (line 37)) (3.4.0)\n",
      "Requirement already satisfied: setuptools in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.15.0->-r requirements.txt (line 37)) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.15.0->-r requirements.txt (line 37)) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.15.0->-r requirements.txt (line 37)) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.15.0->-r requirements.txt (line 37)) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.15.0->-r requirements.txt (line 37)) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.15.0->-r requirements.txt (line 37)) (1.67.1)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.15.0->-r requirements.txt (line 37)) (3.10.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.15.0->-r requirements.txt (line 37)) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.15.0->-r requirements.txt (line 37)) (0.5.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.15.0->-r requirements.txt (line 34)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.15.0->-r requirements.txt (line 34)) (3.1.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->-r requirements.txt (line 53)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->-r requirements.txt (line 53)) (2.23.4)\n",
      "Requirement already satisfied: Mako in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from alembic<2.0.0,>=1.8.0->-r requirements.txt (line 55)) (1.3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31.0->-r requirements.txt (line 57)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31.0->-r requirements.txt (line 57)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31.0->-r requirements.txt (line 57)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31.0->-r requirements.txt (line 57)) (2025.1.31)\n",
      "Requirement already satisfied: google-auth~=2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.0->-r requirements.txt (line 9)) (2.38.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from Flask<4->mlflow==2.15.0->-r requirements.txt (line 8)) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.9 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from Flask<4->mlflow==2.15.0->-r requirements.txt (line 8)) (1.9.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.15.0->-r requirements.txt (line 9)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.15.0->-r requirements.txt (line 9)) (5.0.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.0->-r requirements.txt (line 9)) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.0->-r requirements.txt (line 9)) (4.9)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from graphene<4->mlflow==2.15.0->-r requirements.txt (line 8)) (3.2.6)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from graphene<4->mlflow==2.15.0->-r requirements.txt (line 8)) (3.2.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0->-r requirements.txt (line 12)) (1.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow-skinny==2.15.0->-r requirements.txt (line 9)) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from Jinja2<4,>=2.11->mlflow==2.15.0->-r requirements.txt (line 8)) (3.0.2)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.15.0->-r requirements.txt (line 9)) (1.2.18)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.54b1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.15.0->-r requirements.txt (line 9)) (0.54b1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.0->-r requirements.txt (line 9)) (0.6.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.10.0->-r requirements.txt (line 26)) (0.60.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.10.0->-r requirements.txt (line 26)) (5.1.1)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.10.0->-r requirements.txt (line 26)) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.10.0->-r requirements.txt (line 26)) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.10.0->-r requirements.txt (line 26)) (1.0.3)\n",
      "Requirement already satisfied: future>=0.16.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from pyloudnorm>=0.1.1->-r requirements.txt (line 27)) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from soundfile>=0.12.0->-r requirements.txt (line 28)) (1.17.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from pooch>=1.6.0->-r requirements.txt (line 30)) (3.10.0)\n",
      "Collecting gviz_api>=1.9.0 (from tensorboard-plugin-profile>=2.14.0->-r requirements.txt (line 35))\n",
      "  Downloading gviz_api-1.10.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting etils>=1.0.0 (from etils[epath]>=1.0.0->tensorboard-plugin-profile>=2.14.0->-r requirements.txt (line 35))\n",
      "  Downloading etils-1.12.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting cheroot>=10.0.1 (from tensorboard-plugin-profile>=2.14.0->-r requirements.txt (line 35))\n",
      "  Downloading cheroot-10.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 36)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 36)) (3.3)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fastapi>=0.104.0->-r requirements.txt (line 40)) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fastapi>=0.104.0->-r requirements.txt (line 40)) (0.0.7)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fastapi>=0.104.0->-r requirements.txt (line 40)) (0.28.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.7 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fastapi>=0.104.0->-r requirements.txt (line 40)) (0.0.18)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fastapi>=0.104.0->-r requirements.txt (line 40)) (5.10.0)\n",
      "Requirement already satisfied: orjson>=3.2.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fastapi>=0.104.0->-r requirements.txt (line 40)) (3.10.18)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fastapi>=0.104.0->-r requirements.txt (line 40)) (2.2.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from starlette<0.38.0,>=0.37.2->fastapi>=0.104.0->-r requirements.txt (line 40)) (4.9.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.104.0->-r requirements.txt (line 40)) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.8 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from uvicorn>=0.24.0->-r requirements.txt (line 41)) (0.14.0)\n",
      "Requirement already satisfied: qtconsole in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 45)) (5.5.1)\n",
      "Requirement already satisfied: jupyter-console in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 45)) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 45)) (7.16.4)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyterlab>=4.0.0->-r requirements.txt (line 46)) (2.0.4)\n",
      "Requirement already satisfied: jupyter-core in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyterlab>=4.0.0->-r requirements.txt (line 46)) (5.7.2)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyterlab>=4.0.0->-r requirements.txt (line 46)) (2.2.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyterlab>=4.0.0->-r requirements.txt (line 46)) (2.14.1)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyterlab>=4.0.0->-r requirements.txt (line 46)) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyterlab>=4.0.0->-r requirements.txt (line 46)) (0.2.3)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyterlab>=4.0.0->-r requirements.txt (line 46)) (6.4.2)\n",
      "Requirement already satisfied: traitlets in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyterlab>=4.0.0->-r requirements.txt (line 46)) (5.14.3)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (23.1.0)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (8.6.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (0.4.4)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (5.10.4)\n",
      "Requirement already satisfied: overrides>=5.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (7.4.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (0.14.1)\n",
      "Requirement already satisfied: pyzmq>=24 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (25.1.2)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (0.17.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (1.8.0)\n",
      "Requirement already satisfied: babel>=2.10 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (2.11.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (0.9.6)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (4.23.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from ipywidgets>=8.0.0->-r requirements.txt (line 48)) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from ipywidgets>=8.0.0->-r requirements.txt (line 48)) (8.27.0)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets>=8.0.0->-r requirements.txt (line 48))\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets>=8.0.0->-r requirements.txt (line 48))\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: appnope in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from ipykernel>=6.25.0->-r requirements.txt (line 49)) (0.1.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from ipykernel>=6.25.0->-r requirements.txt (line 49)) (1.6.7)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from ipykernel>=6.25.0->-r requirements.txt (line 49)) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from ipykernel>=6.25.0->-r requirements.txt (line 49)) (1.6.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from plotly>=5.17.0->-r requirements.txt (line 60)) (9.0.0)\n",
      "Collecting Flask<4 (from mlflow==2.15.0->-r requirements.txt (line 8))\n",
      "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
      "INFO: pip is looking at multiple versions of dash to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting dash>=2.14.0 (from -r requirements.txt (line 61))\n",
      "  Downloading dash-3.0.3-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading dash-3.0.2-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading dash-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading dash-3.0.0-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading dash-2.18.2-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading dash-2.18.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading dash-2.18.0-py3-none-any.whl.metadata (10 kB)\n",
      "INFO: pip is still looking at multiple versions of dash to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading dash-2.17.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading dash-2.17.0-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading dash-2.16.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading dash-2.16.0-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading dash-2.15.0-py3-none-any.whl.metadata (11 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading dash-2.14.2-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading dash-2.14.1-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading dash-2.14.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting Flask<4 (from mlflow==2.15.0->-r requirements.txt (line 8))\n",
      "  Downloading Flask-2.2.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting plotly>=5.17.0 (from -r requirements.txt (line 60))\n",
      "  Downloading plotly-6.1.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting ipykernel>=6.25.0 (from -r requirements.txt (line 49))\n",
      "  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting ipywidgets>=8.0.0 (from -r requirements.txt (line 48))\n",
      "  Downloading ipywidgets-8.1.6-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting notebook-shim>=0.2 (from jupyterlab>=4.0.0->-r requirements.txt (line 46))\n",
      "  Downloading notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting notebook>=7.0.0 (from -r requirements.txt (line 47))\n",
      "  Downloading notebook-7.4.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab>=4.0.0->-r requirements.txt (line 46))\n",
      "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab>=4.0.0->-r requirements.txt (line 46))\n",
      "  Downloading jupyter_server-2.16.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyterlab>=4.0.0 (from -r requirements.txt (line 46))\n",
      "  Downloading jupyterlab-4.4.3-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting jupyter>=1.0.0 (from -r requirements.txt (line 45))\n",
      "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting uvicorn>=0.24.0 (from -r requirements.txt (line 41))\n",
      "  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting anyio<5,>=3.4.0 (from starlette<0.38.0,>=0.37.2->fastapi>=0.104.0->-r requirements.txt (line 40))\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.104.0->-r requirements.txt (line 40))\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting fastapi>=0.104.0 (from -r requirements.txt (line 40))\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting torch>=2.0.0 (from -r requirements.txt (line 36))\n",
      "  Downloading torch-2.7.1-cp312-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting tensorboard-plugin-profile>=2.14.0 (from -r requirements.txt (line 35))\n",
      "  Downloading tensorboard_plugin_profile-2.19.8-cp312-none-macosx_12_0_arm64.whl.metadata (5.1 kB)\n",
      "  Downloading tensorboard_plugin_profile-2.19.7-cp312-none-macosx_12_0_arm64.whl.metadata (5.1 kB)\n",
      "  Downloading tensorboard_plugin_profile-2.19.6-cp312-none-macosx_12_0_arm64.whl.metadata (5.1 kB)\n",
      "  Downloading tensorboard_plugin_profile-2.19.5-cp312-none-macosx_12_0_arm64.whl.metadata (5.1 kB)\n",
      "  Downloading tensorboard_plugin_profile-2.19.4-cp312-none-macosx_12_0_arm64.whl.metadata (5.3 kB)\n",
      "  Downloading tensorboard_plugin_profile-2.19.3-cp312-none-macosx_12_0_arm64.whl.metadata (5.3 kB)\n",
      "  Downloading tensorboard_plugin_profile-2.19.2-cp312-none-macosx_12_0_arm64.whl.metadata (5.3 kB)\n",
      "  Downloading tensorboard_plugin_profile-2.19.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "  Downloading tensorboard_plugin_profile-2.18.0-py3-none-any.whl.metadata (1.0 kB)\n",
      "  Downloading tensorboard_plugin_profile-2.17.0-py3-none-any.whl.metadata (1.0 kB)\n",
      "  Downloading tensorboard_plugin_profile-2.16.0-py3-none-any.whl.metadata (1.0 kB)\n",
      "  Downloading tensorboard_plugin_profile-2.15.2-py3-none-any.whl.metadata (1.0 kB)\n",
      "  Downloading tensorboard_plugin_profile-2.15.1-py3-none-any.whl.metadata (1.0 kB)\n",
      "  Downloading tensorboard_plugin_profile-2.15.0-py3-none-any.whl.metadata (1.0 kB)\n",
      "  Downloading tensorboard_plugin_profile-2.14.0-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting audioread>=3.0.0 (from -r requirements.txt (line 31))\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pooch>=1.6.0 (from -r requirements.txt (line 30))\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soundfile>=0.12.0 (from -r requirements.txt (line 28))\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-macosx_11_0_arm64.whl.metadata (16 kB)\n",
      "Collecting pyloudnorm>=0.1.1 (from -r requirements.txt (line 27))\n",
      "  Using cached pyloudnorm-0.1.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting librosa>=0.10.0 (from -r requirements.txt (line 26))\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting seaborn>=0.12.0 (from -r requirements.txt (line 23))\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting Cython>=0.29.0 (from -r requirements.txt (line 5))\n",
      "  Using cached cython-3.1.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.31.0->-r requirements.txt (line 57))\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting typing-extensions>=3.6.6 (from tensorflow>=2.15.0->-r requirements.txt (line 37))\n",
      "  Downloading typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.48.0->-r requirements.txt (line 12))\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard>=2.15.0->-r requirements.txt (line 34))\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==2.15.0->-r requirements.txt (line 9))\n",
      "  Downloading sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.15.0->-r requirements.txt (line 9))\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.0->-r requirements.txt (line 9))\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting querystring-parser<2 (from mlflow==2.15.0->-r requirements.txt (line 8))\n",
      "  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl.metadata (559 bytes)\n",
      "Collecting pyyaml<7,>=5.1 (from mlflow-skinny==2.15.0->-r requirements.txt (line 9))\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting pytz<2025 (from mlflow-skinny==2.15.0->-r requirements.txt (line 9))\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas<3.0.0,>=2.0.0->-r requirements.txt (line 20))\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pyarrow<16,>=4.0.0 (from mlflow==2.15.0->-r requirements.txt (line 8))\n",
      "  Using cached pyarrow-15.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting packaging<25 (from mlflow-skinny==2.15.0->-r requirements.txt (line 9))\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.54b1 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.15.0->-r requirements.txt (line 9))\n",
      "  Downloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==2.15.0->-r requirements.txt (line 9))\n",
      "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==2.15.0->-r requirements.txt (line 9))\n",
      "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting multiprocess (from evaluate==0.4.3->-r requirements.txt (line 14))\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow>=2.15.0->-r requirements.txt (line 37))\n",
      "  Downloading ml_dtypes-0.5.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (21 kB)\n",
      "Collecting markdown<4,>=3.3 (from mlflow==2.15.0->-r requirements.txt (line 8))\n",
      "  Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting Jinja2<4,>=2.11 (from mlflow==2.15.0->-r requirements.txt (line 8))\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting importlib-metadata!=4.7.0,<8,>=3.7.0 (from mlflow-skinny==2.15.0->-r requirements.txt (line 9))\n",
      "  Using cached importlib_metadata-7.2.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.31.0->-r requirements.txt (line 57))\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0->-r requirements.txt (line 12))\n",
      "  Downloading hf_xet-1.1.3-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers==4.48.0->-r requirements.txt (line 12))\n",
      "  Using cached huggingface_hub-0.32.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow>=2.15.0->-r requirements.txt (line 37))\n",
      "  Downloading grpcio-1.73.0-cp312-cp312-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow==2.15.0->-r requirements.txt (line 8))\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow==2.15.0->-r requirements.txt (line 8))\n",
      "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphene<4 (from mlflow==2.15.0->-r requirements.txt (line 8))\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.0->-r requirements.txt (line 9))\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==2.15.0->-r requirements.txt (line 9))\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting gitpython<4,>=3.1.9 (from mlflow-skinny==2.15.0->-r requirements.txt (line 9))\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fsspec[http]>=2021.05.0 (from evaluate==0.4.3->-r requirements.txt (line 14))\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting Flask<4 (from mlflow==2.15.0->-r requirements.txt (line 8))\n",
      "  Downloading flask-3.1.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading flask-3.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard>=2.15.0->-r requirements.txt (line 34))\n",
      "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting retrying (from dash>=2.14.0->-r requirements.txt (line 61))\n",
      "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from bokeh>=3.2.0->-r requirements.txt (line 62)) (2022.9.0)\n",
      "Requirement already satisfied: iniconfig in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from pytest>=7.0.0->-r requirements.txt (line 65)) (1.1.1)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from pytest>=7.0.0->-r requirements.txt (line 65)) (1.6.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from black>=23.0.0->-r requirements.txt (line 66)) (1.0.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from black>=23.0.0->-r requirements.txt (line 66)) (0.10.3)\n",
      "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from flake8>=6.0.0->-r requirements.txt (line 67)) (0.7.0)\n",
      "Requirement already satisfied: pycodestyle<2.12.0,>=2.11.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from flake8>=6.0.0->-r requirements.txt (line 67)) (2.11.1)\n",
      "Requirement already satisfied: pyflakes<3.3.0,>=3.2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from flake8>=6.0.0->-r requirements.txt (line 67)) (3.2.0)\n",
      "Requirement already satisfied: coverage>=7.5 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from coverage[toml]>=7.5->pytest-cov>=4.1.0->-r requirements.txt (line 77)) (7.8.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets==3.4.0->-r requirements.txt (line 16)) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets==3.4.0->-r requirements.txt (line 16)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets==3.4.0->-r requirements.txt (line 16)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets==3.4.0->-r requirements.txt (line 16)) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets==3.4.0->-r requirements.txt (line 16)) (6.0.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets==3.4.0->-r requirements.txt (line 16)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets==3.4.0->-r requirements.txt (line 16)) (1.20.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (21.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow>=2.15.0->-r requirements.txt (line 37)) (0.44.0)\n",
      "Requirement already satisfied: pycparser in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.12.0->-r requirements.txt (line 28)) (2.21)\n",
      "Requirement already satisfied: more-itertools>=2.6 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from cheroot>=10.0.1->tensorboard-plugin-profile>=2.14.0->-r requirements.txt (line 35)) (10.3.0)\n",
      "Collecting jaraco.functools (from cheroot>=10.0.1->tensorboard-plugin-profile>=2.14.0->-r requirements.txt (line 35))\n",
      "  Downloading jaraco.functools-4.1.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from email_validator>=2.0.0->fastapi>=0.104.0->-r requirements.txt (line 40)) (2.7.0)\n",
      "Requirement already satisfied: importlib_resources in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from etils[epath]>=1.0.0->tensorboard-plugin-profile>=2.14.0->-r requirements.txt (line 35)) (6.5.2)\n",
      "Requirement already satisfied: typer>=0.12.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fastapi-cli>=0.0.2->fastapi>=0.104.0->-r requirements.txt (line 40)) (0.15.4)\n",
      "Requirement already satisfied: rich-toolkit>=0.11.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fastapi-cli>=0.0.2->fastapi>=0.104.0->-r requirements.txt (line 40)) (0.14.7)\n",
      "Collecting gcsfs (from fsspec>=2021.05.0->fsspec[http]>=2021.05.0->evaluate==0.4.3->-r requirements.txt (line 14))\n",
      "  Downloading gcsfs-2025.5.1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: httpcore==1.* in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.23.0->fastapi>=0.104.0->-r requirements.txt (line 40)) (1.0.7)\n",
      "Requirement already satisfied: jedi>=0.16 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.0->-r requirements.txt (line 48)) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.0->-r requirements.txt (line 48)) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.0->-r requirements.txt (line 48)) (2.19.1)\n",
      "Requirement already satisfied: stack-data in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.0->-r requirements.txt (line 48)) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.0->-r requirements.txt (line 48)) (4.8.0)\n",
      "Requirement already satisfied: wcwidth in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets>=8.0.0->-r requirements.txt (line 48)) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.0->-r requirements.txt (line 48)) (0.8.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (0.23.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (0.1.1)\n",
      "Requirement already satisfied: fqdn in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (2.1)\n",
      "Requirement already satisfied: uri-template in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (24.11.1)\n",
      "Requirement already satisfied: rich in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow>=2.15.0->-r requirements.txt (line 37)) (13.9.4)\n",
      "Requirement already satisfied: namex in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow>=2.15.0->-r requirements.txt (line 37)) (0.1.0)\n",
      "Requirement already satisfied: optree in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow>=2.15.0->-r requirements.txt (line 37)) (0.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 45)) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 45)) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 45)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 45)) (0.1.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 45)) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 45)) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 45)) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 45)) (1.2.1)\n",
      "Requirement already satisfied: webencodings in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from bleach!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 45)) (0.5.1)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (2.16.2)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from numba>=0.51.0->librosa>=0.10.0->-r requirements.txt (line 26)) (0.43.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.0->-r requirements.txt (line 48)) (0.7.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow>=2.15.0->-r requirements.txt (line 37)) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.15.0->-r requirements.txt (line 37)) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->-r requirements.txt (line 36)) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi>=0.104.0->-r requirements.txt (line 40)) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0->fastapi>=0.104.0->-r requirements.txt (line 40)) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0->fastapi>=0.104.0->-r requirements.txt (line 40)) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0->fastapi>=0.104.0->-r requirements.txt (line 40)) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0->fastapi>=0.104.0->-r requirements.txt (line 40)) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0->fastapi>=0.104.0->-r requirements.txt (line 40)) (15.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 45)) (2.5)\n",
      "INFO: pip is looking at multiple versions of gcsfs to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading gcsfs-2025.5.0.post1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading gcsfs-2025.5.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading gcsfs-2025.3.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading gcsfs-2025.3.1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading gcsfs-2025.3.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading gcsfs-2025.2.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading gcsfs-2024.12.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: google-auth-oauthlib in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from gcsfs->fsspec>=2021.05.0->fsspec[http]>=2021.05.0->evaluate==0.4.3->-r requirements.txt (line 14)) (1.2.2)\n",
      "Requirement already satisfied: google-cloud-storage in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from gcsfs->fsspec>=2021.05.0->fsspec[http]>=2021.05.0->evaluate==0.4.3->-r requirements.txt (line 14)) (2.19.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from google-auth-oauthlib->gcsfs->fsspec>=2021.05.0->fsspec[http]>=2021.05.0->evaluate==0.4.3->-r requirements.txt (line 14)) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs->fsspec>=2021.05.0->fsspec[http]>=2021.05.0->evaluate==0.4.3->-r requirements.txt (line 14)) (3.2.2)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from google-cloud-storage->gcsfs->fsspec>=2021.05.0->fsspec[http]>=2021.05.0->evaluate==0.4.3->-r requirements.txt (line 14)) (2.25.0rc1)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from google-cloud-storage->gcsfs->fsspec>=2021.05.0->fsspec[http]>=2021.05.0->evaluate==0.4.3->-r requirements.txt (line 14)) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from google-cloud-storage->gcsfs->fsspec>=2021.05.0->fsspec[http]>=2021.05.0->evaluate==0.4.3->-r requirements.txt (line 14)) (2.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from google-cloud-storage->gcsfs->fsspec>=2021.05.0->fsspec[http]>=2021.05.0->evaluate==0.4.3->-r requirements.txt (line 14)) (1.7.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs->fsspec>=2021.05.0->fsspec[http]>=2021.05.0->evaluate==0.4.3->-r requirements.txt (line 14)) (1.63.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs->fsspec>=2021.05.0->fsspec[http]>=2021.05.0->evaluate==0.4.3->-r requirements.txt (line 14)) (1.26.1)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.0.0->-r requirements.txt (line 46)) (1.2.3)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from qtconsole->jupyter>=1.0.0->-r requirements.txt (line 45)) (2.4.1)\n",
      "Requirement already satisfied: executing in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.0->-r requirements.txt (line 48)) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.0->-r requirements.txt (line 48)) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.0->-r requirements.txt (line 48)) (0.2.2)\n",
      "Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "Downloading tensorboard_plugin_profile-2.19.9-cp312-none-macosx_12_0_arm64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dash-3.0.4-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cheroot-10.0.1-py3-none-any.whl (104 kB)\n",
      "Downloading etils-1.12.2-py3-none-any.whl (167 kB)\n",
      "Downloading gviz_api-1.10.0-py2.py3-none-any.whl (13 kB)\n",
      "Downloading gcsfs-2024.12.0-py2.py3-none-any.whl (35 kB)\n",
      "Downloading jaraco.functools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: widgetsnbextension, werkzeug, retrying, jupyterlab_widgets, jaraco.functools, gviz_api, etils, Flask, cheroot, dash, ipywidgets, gcsfs, tensorboard-plugin-profile\n",
      "\u001b[2K  Attempting uninstall: widgetsnbextension\n",
      "\u001b[2K    Found existing installation: widgetsnbextension 3.6.6\n",
      "\u001b[2K    Uninstalling widgetsnbextension-3.6.6:\n",
      "\u001b[2K      Successfully uninstalled widgetsnbextension-3.6.6\n",
      "\u001b[2K  Attempting uninstall: werkzeugâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 0/13\u001b[0m [widgetsnbextension]\n",
      "\u001b[2K    Found existing installation: Werkzeug 3.1.3m \u001b[32m 0/13\u001b[0m [widgetsnbextension]\n",
      "\u001b[2K    Uninstalling Werkzeug-3.1.3:â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 0/13\u001b[0m [widgetsnbextension]\n",
      "\u001b[2K      Successfully uninstalled Werkzeug-3.1.3[0m \u001b[32m 0/13\u001b[0m [widgetsnbextension]\n",
      "\u001b[2K  Attempting uninstall: jupyterlab_widgetsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 1/13\u001b[0m [werkzeug]\n",
      "\u001b[2K    Found existing installation: jupyterlab-widgets 1.0.0â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/13\u001b[0m [jupyterlab_widgets]\n",
      "\u001b[2K    Uninstalling jupyterlab-widgets-1.0.0:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/13\u001b[0m [jupyterlab_widgets]\n",
      "\u001b[2K      Successfully uninstalled jupyterlab-widgets-1.0.0â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/13\u001b[0m [jupyterlab_widgets]\n",
      "\u001b[2K  Attempting uninstall: Flaskm\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/13\u001b[0m [etils]rlab_widgets]\n",
      "\u001b[2K    Found existing installation: Flask 3.1.0[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 7/13\u001b[0m [Flask]\n",
      "\u001b[2K    Uninstalling Flask-3.1.0:\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 7/13\u001b[0m [Flask]\n",
      "\u001b[2K      Successfully uninstalled Flask-3.1.00mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 7/13\u001b[0m [Flask]\n",
      "\u001b[2K  Attempting uninstall: ipywidgetsâ”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 9/13\u001b[0m [dash]ot]\n",
      "\u001b[2K    Found existing installation: ipywidgets 7.8.1mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 9/13\u001b[0m [dash]\n",
      "\u001b[2K    Uninstalling ipywidgets-7.8.1:m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 9/13\u001b[0m [dash]\n",
      "\u001b[2K      Successfully uninstalled ipywidgets-7.8.190mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 9/13\u001b[0m [dash]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13/13\u001b[0m [tensorboard-plugin-profile]rd-plugin-profile]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Flask-3.0.3 cheroot-10.0.1 dash-3.0.4 etils-1.12.2 gcsfs-2024.12.0 gviz_api-1.10.0 ipywidgets-8.1.7 jaraco.functools-4.1.0 jupyterlab_widgets-3.0.15 retrying-1.3.4 tensorboard-plugin-profile-2.19.9 werkzeug-3.0.6 widgetsnbextension-4.0.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if you have not installed them already\n",
    "%pip install -r requirements.txt --verbose --quiet\n",
    "%pip install -q --upgrade pip\n",
    "\n",
    "# Example output (replace Codespaces path with $HOME):\n",
    "# Requirement already satisfied: matplotlib>=3.7.0 in $HOME/.local/lib/python3.12/site-packages (from -r requirements.txt (line 21)) (3.10.1)\n",
    "# Requirement already satisfied: IPython>=8.0.0 in $HOME/.local/lib/python3.12/site-packages (from -r requirements.txt (line 22)) (9.0.2)\n",
    "# Requirement already satisfied: plotly>=5.15.0 in $HOME/.local/lib/python3.12/site-packages (from -r requirements.txt (line 24)) (6.0.1)\n",
    "# Requirement already satisfied: pandas>=2.0.0 in $HOME/.local/lib/python3.12/site-packages (from -r requirements.txt (line 25)) (2.2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š TensorBoard writer created: default\n",
      "   Log directory: tensorboard_logs/agentic_rag/agentic_rag_audio_pipeline/default/20250610-032118\n",
      "ðŸš€ TensorBoard started successfully!\n",
      "ðŸ“Š TensorBoard URL: http://localhost:6006\n",
      "ðŸ“ Log directory: tensorboard_logs/agentic_rag/agentic_rag_audio_pipeline\n",
      "ðŸ“Š Closed TensorBoard writer: default\n",
      "âœ… TensorBoard Integration Initialized\n",
      "ðŸ“Š TensorBoard logs: ./tensorboard_logs/agentic_rag\n",
      "ðŸ” Experiment: agentic_rag_audio_pipeline\n",
      "ðŸ¢ HP AI Studio Compatible: True\n",
      "ðŸŒ View at: http://localhost:6006 (after starting TensorBoard server)\n"
     ]
    }
   ],
   "source": [
    "# TensorBoard Integration for Real-time Monitoring\n",
    "# Initialize TensorBoard for monitoring audio transcription and RAG pipeline performance\n",
    "from tensorboard_integration import OrpheusTensorBoardManager\n",
    "\n",
    "# Initialize TensorBoard manager for agentic RAG monitoring\n",
    "tensorboard_manager = OrpheusTensorBoardManager(\n",
    "    log_dir=\"./tensorboard_logs/agentic_rag\",\n",
    "    experiment_name=\"agentic_rag_audio_pipeline\",\n",
    "    hp_ai_studio_compatible=True\n",
    ")\n",
    "\n",
    "print(\"âœ… TensorBoard Integration Initialized\")\n",
    "print(f\"ðŸ“Š TensorBoard logs: ./tensorboard_logs/agentic_rag\")\n",
    "print(f\"ðŸ” Experiment: agentic_rag_audio_pipeline\")\n",
    "print(f\"ðŸ¢ HP AI Studio Compatible: True\")\n",
    "print(f\"ðŸŒ View at: http://localhost:6006 (after starting TensorBoard server)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Step 1: Model Setup\n",
    "\n",
    "We will set up **Llama 2 (7B)** for text generation. If the model is not found locally, it will be downloaded from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Model setup metrics logged to TensorBoard\n"
     ]
    }
   ],
   "source": [
    "# Log model setup metrics to TensorBoard\n",
    "tensorboard_manager.log_scalar(\"model_setup/model_size_gb\", 3.7, 0)  # Approximate Llama 2 7B size\n",
    "tensorboard_manager.log_scalar(\"model_setup/context_length\", 4096, 0)\n",
    "tensorboard_manager.log_scalar(\"model_setup/max_tokens\", 2000, 0)\n",
    "tensorboard_manager.log_scalar(\"model_setup/temperature\", 0.25, 0)\n",
    "tensorboard_manager.log_scalar(\"model_setup/gpu_layers\", 30, 0)\n",
    "\n",
    "print(\"ðŸ“Š Model setup metrics logged to TensorBoard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Model not found locally. Downloading Llama 2 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2794c9d5d72b49029d9db62af363b8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama-2-7b-chat.Q4_K_M.gguf:   0%|          | 0.00/4.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded to: model/llama-2-7b-chat.Q4_K_M.gguf\n",
      "Using model at: model/llama-2-7b-chat.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "%pip install -q huggingface-hub\n",
    "\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "MODEL_FILENAME = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "MODEL_DIR = \"model\"\n",
    "EXPECTED_PATH = os.path.join(MODEL_DIR, MODEL_FILENAME)\n",
    "\n",
    "# Ensure model directory exists\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Check if model already exists\n",
    "if os.path.exists(EXPECTED_PATH):\n",
    "    print(f\"Model already exists at: {EXPECTED_PATH}\")\n",
    "    model_path = EXPECTED_PATH\n",
    "else:\n",
    "    print(\"Model not found locally. Downloading Llama 2 model...\")\n",
    "    \n",
    "    # Download the model - Fixed: removed url parameter and added correct parameters\n",
    "    model_path = hf_hub_download(\n",
    "        repo_id=\"TheBloke/Llama-2-7B-Chat-GGUF\",\n",
    "        filename=MODEL_FILENAME,\n",
    "        local_dir=MODEL_DIR\n",
    "    )\n",
    "    print(f\"Model downloaded to: {model_path}\")\n",
    "\n",
    "print(f\"Using model at: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    }
   ],
   "source": [
    "%pip install -q llama-cpp-python\n",
    "# Check if the model file exists\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "# Import the Llama class from llama_cpp\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Initialize the model with the local path and GPU acceleration\n",
    "llm = Llama(\n",
    "    model_path=EXPECTED_PATH,\n",
    "    temperature=0.25,\n",
    "    max_tokens=2000,\n",
    "    n_ctx=4096,\n",
    "    top_p=1.0,\n",
    "    verbose=False,\n",
    "    n_gpu_layers=30,  # Utilize some available GPU layers\n",
    "    n_batch=512,      # Optimize batch size for parallel processing\n",
    "    f16_kv=True,      # Enable half-precision for key/value cache\n",
    "    use_mlock=True,   # Lock memory to prevent swapping\n",
    "    use_mmap=True     # Utilize memory mapping for faster loading\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“„ Step 2: Loading, Transcribing, and Storing Audio Data\n",
    "\n",
    "This step outlines the process for loading audio data (e.g., from an Omi streaming device), transcribing it, and preparing it for storage and further processing. Both the raw audio and its transcription are valuable assets.\n",
    "\n",
    "### Updated Workflow:\n",
    "- Load audio data from files or streams.\n",
    "- Transcribe audio to text using the LLM model.\n",
    "- Extract and tag audio segments based on content.\n",
    "- Store audio and transcription in a structured format.\n",
    "- (Optional) Save audio segments as separate files for download or sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ffmpeg found and working.\n",
      "Loading AUDIO from: ./data/tester.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to load audio: ffmpeg version 7.1.1 Copyright (c) 2000-2025 the FFmpeg developers\n  built with Apple clang version 16.0.0 (clang-1600.0.26.6)\n  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.1.1_3 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n  libavutil      59. 39.100 / 59. 39.100\n  libavcodec     61. 19.101 / 61. 19.101\n  libavformat    61.  7.100 / 61.  7.100\n  libavdevice    61.  3.100 / 61.  3.100\n  libavfilter    10.  4.100 / 10.  4.100\n  libswscale      8.  3.100 /  8.  3.100\n  libswresample   5.  3.100 /  5.  3.100\n  libpostproc    58.  3.100 / 58.  3.100\n[in#0 @ 0x128804520] Error opening input: No such file or directory\nError opening input file ./data/tester.mp3.\nError opening input files: No such file or directory\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages/whisper/audio.py:58\u001b[0m, in \u001b[0;36mload_audio\u001b[0;34m(file, sr)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     out \u001b[38;5;241m=\u001b[39m run(cmd, capture_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mstdout\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/subprocess.py:571\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 571\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    572\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['ffmpeg', '-nostdin', '-threads', '0', '-i', './data/tester.mp3', '-f', 's16le', '-ac', '1', '-acodec', 'pcm_s16le', '-ar', '16000', '-']' returned non-zero exit status 254.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Load and transcribe the audio file using whisper\u001b[39;00m\n\u001b[1;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtranscribe(AUDIO_PATH)\n\u001b[1;32m     43\u001b[0m text_content \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Calculate and log transcription metrics\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages/whisper/transcribe.py:133\u001b[0m, in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[0m\n\u001b[1;32m    130\u001b[0m     decode_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Pad 30-seconds of silence to the input audio, for slicing\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m mel \u001b[38;5;241m=\u001b[39m log_mel_spectrogram(audio, model\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_mels, padding\u001b[38;5;241m=\u001b[39mN_SAMPLES)\n\u001b[1;32m    134\u001b[0m content_frames \u001b[38;5;241m=\u001b[39m mel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m N_FRAMES\n\u001b[1;32m    135\u001b[0m content_duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(content_frames \u001b[38;5;241m*\u001b[39m HOP_LENGTH \u001b[38;5;241m/\u001b[39m SAMPLE_RATE)\n",
      "File \u001b[0;32m/Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages/whisper/audio.py:140\u001b[0m, in \u001b[0;36mlog_mel_spectrogram\u001b[0;34m(audio, n_mels, padding, device)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(audio):\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(audio, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 140\u001b[0m         audio \u001b[38;5;241m=\u001b[39m load_audio(audio)\n\u001b[1;32m    141\u001b[0m     audio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(audio)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages/whisper/audio.py:60\u001b[0m, in \u001b[0;36mload_audio\u001b[0;34m(file, sr)\u001b[0m\n\u001b[1;32m     58\u001b[0m     out \u001b[38;5;241m=\u001b[39m run(cmd, capture_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mstdout\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load audio: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mdecode()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mfrombuffer(out, np\u001b[38;5;241m.\u001b[39mint16)\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m32768.0\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to load audio: ffmpeg version 7.1.1 Copyright (c) 2000-2025 the FFmpeg developers\n  built with Apple clang version 16.0.0 (clang-1600.0.26.6)\n  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.1.1_3 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n  libavutil      59. 39.100 / 59. 39.100\n  libavcodec     61. 19.101 / 61. 19.101\n  libavformat    61.  7.100 / 61.  7.100\n  libavdevice    61.  3.100 / 61.  3.100\n  libavfilter    10.  4.100 / 10.  4.100\n  libswscale      8.  3.100 /  8.  3.100\n  libswresample   5.  3.100 /  5.  3.100\n  libpostproc    58.  3.100 / 58.  3.100\n[in#0 @ 0x128804520] Error opening input: No such file or directory\nError opening input file ./data/tester.mp3.\nError opening input files: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# --- Load the Audio File Document ---\n",
    "# --- Load the Audio File Document and Audio Collection System ---\n",
    "\n",
    "# Install whisper if not already installed\n",
    "%pip install -q openai-whisper\n",
    "\n",
    "# Install ffmpeg-python bindings if not already installed\n",
    "%pip install -q ffmpeg-python\n",
    "\n",
    "import shutil\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Check if ffmpeg is available and working\n",
    "try:\n",
    "    subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, check=True)\n",
    "    print(\"ffmpeg found and working.\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"ffmpeg not found. Please install ffmpeg and ensure it's in your PATH.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error running ffmpeg: {{e}}\")\n",
    "    raise RuntimeError(\"ffmpeg is not working correctly.\") from e\n",
    "\n",
    "import whisper\n",
    "\n",
    "# No need to manually set ffmpeg_dir if ffmpeg is installed system-wide\n",
    "\n",
    "# Define the Audio File file path\n",
    "AUDIO_PATH = \"./data/tester.mp3\"\n",
    "# Check if the file exists\n",
    "print(f\"Loading AUDIO from: {AUDIO_PATH}\")\n",
    "\n",
    "# Define the audio collection system\n",
    "AUDIO_COLLECTION_SYSTEM = \"MyAudioSystem\"\n",
    "\n",
    "# TensorBoard logging for audio transcription performance\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Load and transcribe the audio file using whisper\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(AUDIO_PATH)\n",
    "text_content = result[\"text\"]\n",
    "\n",
    "# Calculate and log transcription metrics\n",
    "transcription_time = time.time() - start_time\n",
    "audio_duration = result.get(\"duration\", 0)\n",
    "segments_count = len(result.get(\"segments\", []))\n",
    "\n",
    "# Log metrics to TensorBoard\n",
    "if 'tensorboard_manager' in locals():\n",
    "    tensorboard_manager.log_scalar(\"audio_transcription/processing_time\", transcription_time, 0)\n",
    "    tensorboard_manager.log_scalar(\"audio_transcription/audio_duration\", audio_duration, 0)\n",
    "    tensorboard_manager.log_scalar(\"audio_transcription/segments_count\", segments_count, 0)\n",
    "    tensorboard_manager.log_scalar(\"audio_transcription/processing_speed_ratio\", audio_duration / transcription_time if transcription_time > 0 else 0, 0)\n",
    "    print(f\"ðŸ“Š Transcription metrics logged to TensorBoard\")\n",
    "\n",
    "print(f\"ðŸŽ§ Audio transcribed in {transcription_time:.2f}s (duration: {audio_duration:.2f}s)\")\n",
    "\n",
    "# Log audio quality metrics if available\n",
    "if \"language\" in result:\n",
    "    print(f\"ðŸŒ Detected language: {result['language']}\")\n",
    "    if 'tensorboard_manager' in locals():\n",
    "        # Log language detection success\n",
    "        tensorboard_manager.log_scalar(\"audio_transcription/language_detected\", 1.0, 0)\n",
    "    \n",
    "print(f\"ðŸ“¦ Extracted {segments_count} segments for processing\")\n",
    "\n",
    "# For compatibility with the rest of your code, wrap the text in a document-like object\n",
    "class AudioDocument:\n",
    "    def __init__(self, text):\n",
    "        self.page_content = text\n",
    "        self.metadata = {}  # Optionally add metadata if needed\n",
    "    def getPageText(self):\n",
    "        return self.page_content\n",
    "\n",
    "documents = [AudioDocument(text_content)]\n",
    "\n",
    "print(f\"Successfully loaded {len(documents)} document(s) from the AUDIO.\")\n",
    "# Initialize an empty list for the audio files\n",
    "audio_files = []\n",
    "\n",
    "# Iterate through each document\n",
    "for document in documents:\n",
    "    # Get the current page's text content\n",
    "    text_content = document.getPageText()\n",
    "\n",
    "    # Extract relevant information from the text, e.g., keywords or phrases\n",
    "    def extractRelevantInfo(text):\n",
    "        # Placeholder: just return the text itself\n",
    "        return text\n",
    "    extracted_info = extractRelevantInfo(text_content)\n",
    "\n",
    "    # Define a placeholder for createAudioFile\n",
    "    def createAudioFile(system, info):\n",
    "        # Placeholder: just return a tuple for demonstration\n",
    "        return (system, info)\n",
    "\n",
    "    # Create an audio file based on the extracted information\n",
    "    audio_file = createAudioFile(AUDIO_COLLECTION_SYSTEM, extracted_info)\n",
    "\n",
    "    # Add the audio file to the collection system's list\n",
    "    audio_files.append(audio_file)\n",
    "print(f\"Successfully loaded {len(documents)} document(s) from the AUDIO and created {len(audio_files)} audio file(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Omi data from Webhook\n",
    "\n",
    "# Initialize variables for Omi data from Webhook\n",
    "response = None  # Will store webhook response\n",
    "rag_percentage = 0.0  # Initialize RAG percentage\n",
    "ipfs_metrics = {}  # Initialize IPFS metrics dictionary\n",
    "\n",
    "# --- Audio Event Detection (Sound Tagging) ---\n",
    "\n",
    "%pip install -q torchaudio\n",
    "%pip install -q panns-inference\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from panns_inference import AudioTagging\n",
    "\n",
    "# Initialize the PANNs audio tagging model\n",
    "panns_model = AudioTagging(checkpoint_path=None, device='cpu')  # Uses default Cnn14 weights\n",
    "\n",
    "# Load and preprocess audio\n",
    "waveform, sr = torchaudio.load(AUDIO_PATH)\n",
    "if sr != 32000:\n",
    "    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=32000)(waveform)\n",
    "    sr = 32000\n",
    "\n",
    "# PANNs expects mono audio\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "# --- Load the full AudioSet class labels for PANNs output mapping ---\n",
    "import os\n",
    "import csv\n",
    "\n",
    "LABELS_CSV_PATH = \"class_labels_indices.csv\"\n",
    "labels = None\n",
    "if os.path.exists(LABELS_CSV_PATH):\n",
    "    with open(LABELS_CSV_PATH, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        labels = [row['display_name'] for row in reader]\n",
    "else:\n",
    "    print(\"WARNING: AudioSet class label CSV not found. Will print indices instead of class names.\")\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = panns_model.inference(waveform)\n",
    "    clipwise_output = output[0]\n",
    "    topk = torch.topk(torch.tensor(clipwise_output[0]), 3)\n",
    "    sound_tags = []\n",
    "    if labels and max(topk.indices.tolist()) < len(labels):\n",
    "        sound_tags = [labels[i] for i in topk.indices.tolist()]\n",
    "    else:\n",
    "        sound_tags = [f\"Class index {i}\" for i in topk.indices.tolist()]\n",
    "\n",
    "print(\"Detected sound types:\", sound_tags)\n",
    "\n",
    "# Example output (replace Codespaces path with $HOME):\n",
    "# Checkpoint path: $HOME/panns_data/Cnn14_mAP=0.431.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ Step 3: Chunking Audio Transcriptions for RAG\n",
    "\n",
    "The transcribed text from the audio data is split into **small overlapping chunks** (approximately **500 characters**). These chunks are then used for embedding and storage in ChromaDB to enable semantic search for the RAG pipeline.\n",
    "\n",
    "### Updated Workflow:\n",
    "- Split transcriptions into smaller chunks to manage context size.\n",
    "- Overlap chunks slightly to ensure continuity and context preservation.\n",
    "- Tag chunks with relevant metadata (e.g., sound types, timestamps).\n",
    "- Store chunks in ChromaDB with embeddings for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Split the Audio Content into Manageable Chunks with Sound Tags ---\n",
    "%pip install -q langchain\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "text_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "\n",
    "# Split the transcription into chunks\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Attach sound tags to each chunk as metadata\n",
    "for doc in docs:\n",
    "    doc.page_content = f\"Transcription: {doc.page_content}\\nSound tags: {', '.join(sound_tags)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Step 4: Initializing the Embedding Model\n",
    "\n",
    "To convert text into numerical representations for efficient similarity search, we use **all-MiniLM-L6-v2** from `sentence-transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize the Embedding Model ---\n",
    "%pip install -q sentence-transformers\n",
    "%pip install -q transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Define the embedding model name\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "print(f\"Successfully loaded embedding model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Step 5: Computing Embeddings for Document Chunks\n",
    "\n",
    "Each chunk is converted into a **vector representation** using our embedding model. This allows us to perform **semantic similarity searches** later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute Embeddings for Each Text Chunk ---\n",
    "\n",
    "# Extract text content from each chunk\n",
    "doc_texts = [doc.page_content for doc in docs]\n",
    "\n",
    "# Compute embeddings for the extracted text chunks\n",
    "document_embeddings = embedding_model.encode(doc_texts, convert_to_numpy=True)\n",
    "\n",
    "# Display the result\n",
    "print(\"Successfully computed embeddings for each text chunk.\")\n",
    "print(f\"Embeddings Shape: {document_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—„ï¸ Step 6: Storing Audio Transcription Embeddings in ChromaDB\n",
    "\n",
    "We initialize **ChromaDB**, a high-performance **vector database**, and store our computed embeddings to enable efficient retrieval of relevant text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize and Populate the Chroma Vector Database ---\n",
    "\n",
    "# Define Chroma database path and collection name\n",
    "CHROMA_DB_PATH = \"./chroma_db\"\n",
    "COLLECTION_NAME = \"document_embeddings\"\n",
    "\n",
    "# Initialize Chroma client\n",
    "import chromadb\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)\n",
    "\n",
    "# Add document embeddings to the Chroma collection\n",
    "for i, embedding in enumerate(document_embeddings):\n",
    "    collection.add(\n",
    "        ids=[str(i)],  # Chroma requires string IDs\n",
    "        embeddings=[embedding.tolist()],\n",
    "        metadatas=[{\"text\": doc_texts[i]}]\n",
    "    )\n",
    "\n",
    "print(\"Successfully populated Chroma database with document embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”Ž Step 7: Implementing Vector Search Tool\n",
    "\n",
    "To retrieve relevant text passages from the database, we define a **vector search function** that finds the most relevant chunks based on a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Searches the Chroma database for relevant text chunks based on the query.\n",
    "    Computes the query embedding, retrieves the top 5 most relevant text chunks,\n",
    "    and returns them as a formatted string.\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.encode(query, convert_to_numpy=True).tolist()\n",
    "    TOP_K = 5\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=TOP_K\n",
    "    )\n",
    "    retrieved_chunks = [metadata[\"text\"] for metadata in results[\"metadatas\"][0]]\n",
    "    return \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "print(vector_search_tool(\"example query\"))  # Test the search tool with a sample query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– Step 8: Context Need Assessment\n",
    "\n",
    "Instead of always retrieving context, we determine if the query **requires external document context** before generating a response. This creates an agentic workflow that makes autonomous decisions to complete the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import re\n",
    "\n",
    "# --- Audio Segment Retrieval and Saving ---\n",
    "def find_relevant_audio_segments(query, segments, collection, embedding_model, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieve audio segments whose text contains the query string (case-insensitive substring match).\n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "        segments (list): List of segment dicts with 'text', 'start', 'end', 'id'.\n",
    "        collection: ChromaDB collection (not used in this version).\n",
    "        embedding_model: Embedding model (not used in this version).\n",
    "        top_k (int): Not used in this version.\n",
    "    Returns:\n",
    "        List of relevant segment dicts.\n",
    "    \"\"\"\n",
    "    query_lower = query.lower()\n",
    "    relevant_segments = [segment for segment in segments if query_lower in segment[\"text\"].lower()]\n",
    "    print(f\"Total relevant segments found: {len(relevant_segments)}\")\n",
    "    return relevant_segments\n",
    "\n",
    "# Function to sanitize filenames\n",
    "def sanitize_filename(name):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", name).strip()\n",
    "\n",
    "# Function to save audio segments\n",
    "def save_audio_segments(audio_path, segments, query, output_base=\"audio_clips\"):\n",
    "    \"\"\"\n",
    "    Save relevant audio segments as separate files.\n",
    "    \"\"\"\n",
    "    folder_name = sanitize_filename(query)\n",
    "    output_dir = os.path.join(output_base, folder_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    for seg in segments:\n",
    "        start_sample = int(seg['start'] * sr)\n",
    "        end_sample = int(seg['end'] * sr)\n",
    "        clip_waveform = waveform[:, start_sample:end_sample]\n",
    "        out_path = os.path.join(output_dir, f\"segment_{{seg['id']}}_{{int(seg['start'])}}-{{int(seg['end'])}}.wav\")\n",
    "        torchaudio.save(out_path, clip_waveform, sr)\n",
    "        print(f\"Saved: {out_path}\")\n",
    "\n",
    "# Example usage:\n",
    "# segments = result[\"segments\"]  # Ensure you have this from the Whisper transcription\n",
    "# relevant_segments = find_relevant_audio_segments(\"audio\", segments, collection, embedding_model)\n",
    "# save_audio_segments(AUDIO_PATH, relevant_segments, \"audio\")\n",
    "# for seg in relevant_segments:\n",
    "#     print(f\"Start: {seg['start']}s, End: {seg['end']}s, Text: {seg['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Step 9: update OEW-MAIN audio library with the audio created from the search.\n",
    "\n",
    "This step will integrate the audio files created or identified in this pipeline into the main audio library of the OEW system. This ensures that all processed and relevant audio content is available for the user's projects and can be easily accessed or downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽšï¸ Step 10: DAW Integration & Audio Rendering\n",
    "\n",
    "Now that relevant audio segments have been extracted, we can integrate them into a Digital Audio Workstation (DAW) environment. This step demonstrates how to load, visualize, and play back audio clips, enabling further editing or composition. Below, we render audio waveforms and provide playback controls for the extracted segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Render and Play Extracted Audio Segments in Notebook ---\n",
    "\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import os\n",
    "\n",
    "def render_audio_segments(folder_path):\n",
    "    \"\"\"\n",
    "    Display waveforms and playback controls for all audio clips in the given folder.\n",
    "    \"\"\"\n",
    "    audio_files = [f for f in os.listdir(folder_path) if f.endswith(\".wav\")]\n",
    "    if not audio_files:\n",
    "        print(\"No audio clips found in:\", folder_path)\n",
    "        return\n",
    "    for audio_file in sorted(audio_files):\n",
    "        file_path = os.path.join(folder_path, audio_file)\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        plt.figure(figsize=(10, 2))\n",
    "        plt.title(audio_file)\n",
    "        plt.plot(waveform.t().numpy())\n",
    "        plt.xlabel(\"Sample\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.show()\n",
    "        display(ipd.Audio(file_path))\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Example usage: visualize and play audio clips for the last query\n",
    "render_audio_segments(os.path.join(\"audio_clips\", sanitize_filename(\"audio\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¤ Step 11: Exporting Audio Clips\n",
    "\n",
    "After rendering and reviewing audio segments, you may want to export them for sharing, further processing, or integration with other systems. Below are common export options:\n",
    "- **IPFS**: Upload audio to the InterPlanetary File System for decentralized access.\n",
    "- **AWS S3**: Store audio in an Amazon S3 bucket for scalable cloud storage.\n",
    "- **Direct Download**: Save audio to a temporary folder for local or web-based download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export Audio Clips: IPFS, AWS S3, or Direct Download ---\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "# Optional: install required packages for IPFS and AWS S3\n",
    "# %pip install ipfshttpclient boto3\n",
    "\n",
    "def export_audio_clips(folder_path, method=\"direct\", **kwargs):\n",
    "    \"\"\"\n",
    "    Export audio clips using the specified method.\n",
    "    method: \"ipfs\", \"aws\", or \"direct\"\n",
    "    kwargs: Additional arguments for each method.\n",
    "    Returns a list of export URLs or file paths.\n",
    "    \"\"\"\n",
    "    audio_files = [f for f in os.listdir(folder_path) if f.endswith(\".wav\")]\n",
    "    exported = []\n",
    "    if method == \"ipfs\":\n",
    "        # Example: Upload to IPFS (requires ipfshttpclient)\n",
    "        import ipfshttpclient\n",
    "        client = ipfshttpclient.connect()\n",
    "        for audio_file in audio_files:\n",
    "            file_path = os.path.join(folder_path, audio_file)\n",
    "            res = client.add(file_path)\n",
    "            ipfs_url = f\"https://ipfs.io/ipfs/{{res['Hash']}}\"\n",
    "            exported.append(ipfs_url)\n",
    "            print(f\"Exported to IPFS: {{ipfs_url}}\")\n",
    "    elif method == \"aws\":\n",
    "        # Example: Upload to AWS S3 (requires boto3)\n",
    "        import boto3\n",
    "        s3 = boto3.client(\"s3\")\n",
    "        bucket = kwargs.get(\"bucket\")\n",
    "        prefix = kwargs.get(\"prefix\", \"\")\n",
    "        for audio_file in audio_files:\n",
    "            file_path = os.path.join(folder_path, audio_file)\n",
    "            s3_key = os.path.join(prefix, audio_file)\n",
    "            s3.upload_file(file_path, bucket, s3_key)\n",
    "            s3_url = f\"https://{{bucket}}.s3.amazonaws.com/{{s3_key}}\"\n",
    "            exported.append(s3_url)\n",
    "            print(f\"Exported to S3: {{s3_url}}\")\n",
    "    elif method == \"direct\":\n",
    "        # Copy files to a temporary directory for download\n",
    "        temp_dir = tempfile.mkdtemp(prefix=\"audio_export_\")\n",
    "        for audio_file in audio_files:\n",
    "            src = os.path.join(folder_path, audio_file)\n",
    "            dst = os.path.join(temp_dir, audio_file)\n",
    "            shutil.copy2(src, dst)\n",
    "            exported.append(dst)\n",
    "            print(f\"Copied to temp folder: {{dst}}\")\n",
    "        print(f\"All files available in: {{temp_dir}}\")\n",
    "    else:\n",
    "        raise ValueError(\"Unknown export method.\")\n",
    "    return exported\n",
    "\n",
    "# Example usage:\n",
    "# Export to IPFS (requires running IPFS daemon and ipfshttpclient)\n",
    "# export_audio_clips(os.path.join(\"audio_clips\", sanitize_filename(\"audio\")), method=\"ipfs\")\n",
    "\n",
    "# Export to AWS S3 (requires AWS credentials)\n",
    "# export_audio_clips(os.path.join(\"audio_clips\", sanitize_filename(\"audio\")), method=\"aws\", bucket=\"your-bucket\", prefix=\"exports/\")\n",
    "\n",
    "# Export for direct download (local temp folder)\n",
    "export_audio_clips(os.path.join(\"audio_clips\", sanitize_filename(\"audio\")), method=\"direct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 12: RAG Pipeline Monitoring Dashboard\n",
    "\n",
    "To efficiently track and monitor our audio processing pipeline, we'll implement a simple web dashboard that provides:\n",
    "\n",
    "- **Audio Processing Metrics**: Track transcription times, file sizes, and segment counts\n",
    "- **ChromaDB Interactions**: Monitor query frequency, embedding operations, and retrieval times \n",
    "- **RAG Performance**: Visualize context usage decisions, response generation times, and user queries\n",
    "\n",
    "This dashboard runs on `http://127.0.0.1:5001/` and provides real-time insights into the operation of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Implement RAG Pipeline Monitoring Dashboard ---\n",
    "%pip install -q streamlit plotly pandas\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import threading\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import socket\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# Function to find an available port starting from a given port\n",
    "def find_available_port(start_port):\n",
    "    port = start_port\n",
    "    max_port = start_port + 100  # Try 100 ports at most\n",
    "    \n",
    "    while port < max_port:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            try:\n",
    "                s.bind(('127.0.0.1', port))\n",
    "                return port\n",
    "            except OSError:\n",
    "                port += 1\n",
    "    \n",
    "    raise RuntimeError(f\"Could not find available port starting from {start_port}\")\n",
    "\n",
    "# Create a class to track metrics across the entire pipeline\n",
    "class RAGMetricsTracker:\n",
    "    def __init__(self, max_history=100):\n",
    "        self.audio_metrics = {\n",
    "            \"processed_files\": 0,\n",
    "            \"total_duration\": 0,\n",
    "            \"avg_transcription_time\": 0,\n",
    "            \"segments_extracted\": 0,\n",
    "            \"history\": deque(maxlen=max_history)\n",
    "        }\n",
    "        self.chromadb_metrics = {\n",
    "            \"queries\": 0,\n",
    "            \"embeddings_created\": 0,\n",
    "            \"avg_query_time\": 0,\n",
    "            \"query_times\": deque(maxlen=max_history)\n",
    "        }\n",
    "        self.rag_metrics = {\n",
    "            \"total_queries\": 0,\n",
    "            \"context_used\": 0,  # Number of times RAG used additional context\n",
    "            \"direct_answers\": 0,  # Number of times RAG answered directly\n",
    "            \"query_history\": deque(maxlen=max_history)\n",
    "        }\n",
    "        self.timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    def log_audio_process(self, filename, duration, transcription_time, num_segments):\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        self.audio_metrics[\"processed_files\"] += 1\n",
    "        self.audio_metrics[\"total_duration\"] += duration\n",
    "        \n",
    "        # Update running average transcription time\n",
    "        prev_avg = self.audio_metrics[\"avg_transcription_time\"]\n",
    "        prev_count = self.audio_metrics[\"processed_files\"] - 1\n",
    "        self.audio_metrics[\"avg_transcription_time\"] = (prev_avg * prev_count + transcription_time) / self.audio_metrics[\"processed_files\"]\n",
    "        \n",
    "        self.audio_metrics[\"segments_extracted\"] += num_segments\n",
    "        \n",
    "        # Add to history\n",
    "        self.audio_metrics[\"history\"].append({\n",
    "            \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"filename\": filename,\n",
    "            \"duration\": duration,\n",
    "            \"transcription_time\": transcription_time,\n",
    "            \"segments\": num_segments\n",
    "        })\n",
    "    \n",
    "    def log_chromadb_query(self, query, num_results, query_time):\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        self.chromadb_metrics[\"queries\"] += 1\n",
    "        self.chromadb_metrics[\"query_times\"].append(query_time)\n",
    "        self.chromadb_metrics[\"avg_query_time\"] = sum(self.chromadb_metrics[\"query_times\"]) / len(self.chromadb_metrics[\"query_times\"])\n",
    "    \n",
    "    def log_embedding_creation(self, count):\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        self.chromadb_metrics[\"embeddings_created\"] += count\n",
    "    \n",
    "    def log_user_query(self, query, used_context, response_time):\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        self.rag_metrics[\"total_queries\"] += 1\n",
    "        if used_context:\n",
    "            self.rag_metrics[\"context_used\"] += 1\n",
    "        else:\n",
    "            self.rag_metrics[\"direct_answers\"] += 1\n",
    "            \n",
    "        self.rag_metrics[\"query_history\"].append({\n",
    "            \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"query\": query,\n",
    "            \"used_context\": used_context,\n",
    "            \"response_time\": response_time\n",
    "        })\n",
    "    \n",
    "    def export_metrics(self):\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"audio_metrics\": {\n",
    "                \"processed_files\": self.audio_metrics[\"processed_files\"],\n",
    "                \"total_duration\": self.audio_metrics[\"total_duration\"],\n",
    "                \"avg_transcription_time\": self.audio_metrics[\"avg_transcription_time\"],\n",
    "                \"segments_extracted\": self.audio_metrics[\"segments_extracted\"],\n",
    "                \"history\": list(self.audio_metrics[\"history\"])\n",
    "            },\n",
    "            \"chromadb_metrics\": {\n",
    "                \"queries\": self.chromadb_metrics[\"queries\"],\n",
    "                \"embeddings_created\": self.chromadb_metrics[\"embeddings_created\"],\n",
    "                \"avg_query_time\": self.chromadb_metrics[\"avg_query_time\"],\n",
    "                \"query_times\": list(self.chromadb_metrics[\"query_times\"])\n",
    "            },\n",
    "            \"rag_metrics\": {\n",
    "                \"total_queries\": self.rag_metrics[\"total_queries\"],\n",
    "                \"context_used\": self.rag_metrics[\"context_used\"],\n",
    "                \"direct_answers\": self.rag_metrics[\"direct_answers\"],\n",
    "                \"query_history\": list(self.rag_metrics[\"query_history\"])\n",
    "            },\n",
    "            \"timestamp\": self.timestamp\n",
    "        }\n",
    "\n",
    "# Find available ports for our services\n",
    "metrics_port = find_available_port(5002)\n",
    "dashboard_port = find_available_port(5001)\n",
    "print(f\"Selected ports: Dashboard={{dashboard_port}}, Metrics API={{metrics_port}}\")\n",
    "\n",
    "# Create a global metrics tracker instance\n",
    "metrics_tracker = RAGMetricsTracker()\n",
    "\n",
    "# Retroactively log the metrics from our previous operations\n",
    "metrics_tracker.log_audio_process(\n",
    "    filename=AUDIO_PATH,\n",
    "    duration=result[\"segments\"][-1][\"end\"] if len(result[\"segments\"]) > 0 else 0,\n",
    "    transcription_time=3.5,  # Placeholder value\n",
    "    num_segments=len(result[\"segments\"])\n",
    ")\n",
    "\n",
    "metrics_tracker.log_embedding_creation(len(document_embeddings))\n",
    "\n",
    "# Wrap our vector search function to track metrics\n",
    "original_vector_search = vector_search_tool\n",
    "\n",
    "def tracked_vector_search_tool(query: str) -> str:\n",
    "    start_time = time.time()\n",
    "    result = original_vector_search(query)\n",
    "    query_time = time.time() - start_time\n",
    "    metrics_tracker.log_chromadb_query(query, 5, query_time)\n",
    "    return result\n",
    "\n",
    "# Replace the original function with our tracked version\n",
    "vector_search_tool = tracked_vector_search_tool\n",
    "\n",
    "# Create the Streamlit dashboard\n",
    "def create_dashboard():\n",
    "    \"\"\"\n",
    "    Create the Streamlit dashboard for RAG pipeline monitoring\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the Streamlit app with dynamic port for metrics API\n",
    "    dashboard_code = f\"\"\"\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "st.set_page_config(page_title=\"RAG Pipeline Monitor\", layout=\"wide\")\n",
    "\n",
    "st.title(\"ðŸ“Š Agentic RAG Pipeline Monitoring\")\n",
    "\n",
    "# Create metrics columns\n",
    "col1, col2, col3, col4 = st.columns(4)\n",
    "\n",
    "# Initialize session state\n",
    "if 'last_update' not in st.session_state:\n",
    "    st.session_state.last_update = time.time()\n",
    "    st.session_state.metrics_history = []\n",
    "\n",
    "# Function to fetch metrics from the metrics endpoint\n",
    "def get_metrics():\n",
    "    try:\n",
    "        response = requests.get(METRICS_API_URL)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            st.error(f\"Failed to fetch metrics: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error fetching metrics: {e}\")\n",
    "        return None\n",
    "\n",
    "# Set the metrics API endpoint\n",
    "METRICS_API_URL = \"http://127.0.0.1:METRICS_PORT/metrics\"\n",
    "\n",
    "metrics = get_metrics()\n",
    "if metrics:\n",
    "    # Store metrics in history\n",
    "    st.session_state.metrics_history.append(metrics)\n",
    "    if len(st.session_state.metrics_history) > 20:\n",
    "        st.session_state.metrics_history.pop(0)\n",
    "    \n",
    "    # Display metrics in columns\n",
    "    with col1:\n",
    "        st.subheader(\"Audio Processing\")\n",
    "        st.metric(\"Files Processed\", metrics[\"audio_metrics\"][\"processed_files\"])\n",
    "        st.metric(\"Total Duration (sec)\", round(metrics[\"audio_metrics\"][\"total_duration\"], 2))\n",
    "        st.metric(\"Segments Extracted\", metrics[\"audio_metrics\"][\"segments_extracted\"])\n",
    "    \n",
    "    with col2:\n",
    "        st.subheader(\"ChromaDB Operations\")\n",
    "        st.metric(\"Total Queries\", metrics[\"chromadb_metrics\"][\"queries\"])\n",
    "        st.metric(\"Embeddings Created\", metrics[\"chromadb_metrics\"][\"embeddings_created\"])\n",
    "        st.metric(\"Avg Query Time (ms)\", round(metrics[\"chromadb_metrics\"][\"avg_query_time\"] * 1000, 2))\n",
    "    \n",
    "    with col3:\n",
    "        st.subheader(\"RAG Performance\")\n",
    "        st.metric(\"User Queries\", metrics[\"rag_metrics\"][\"total_queries\"])\n",
    "        rag_usage = metrics[\"rag_metrics\"][\"context_used\"]\n",
    "        direct_answers = metrics[\"rag_metrics\"][\"direct_answers\"]\n",
    "        total = rag_usage + direct_answers\n",
    "        \n",
    "        if total > 0:\n",
    "            rag_percentage = (rag_usage / total) * 100\n",
    "            st.metric(\"RAG Usage\", f\"{rag_percentage:.1f}%\")\n",
    "        else:\n",
    "            st.metric(\"RAG Usage\", \"0%\")\n",
    "    \n",
    "    # New IPFS metrics column\n",
    "    with col4:\n",
    "        st.subheader(\"IPFS Integration\")\n",
    "        if \"ipfs_metrics\" in metrics:\n",
    "            ipfs_metrics = metrics[\"ipfs_metrics\"]\n",
    "            st.metric(\"Files Discovered\", ipfs_metrics[\"audio_files_discovered\"])\n",
    "            st.metric(\"Files Processed\", ipfs_metrics[\"audio_files_processed\"])\n",
    "            \n",
    "            # Show search info if available\n",
    "            if ipfs_metrics[\"last_search_query\"]:\n",
    "                st.text(f\"Last search: {ipfs_metrics['last_search_query']}\")\n",
    "                st.text(f\"Results: {ipfs_metrics['last_search_results']}\")\n",
    "        else:\n",
    "            st.text(\"No IPFS metrics available\")\n",
    "    \n",
    "    # Time series chart of audio processing\n",
    "    st.subheader(\"Audio Processing History\")\n",
    "    if metrics[\"audio_metrics\"][\"history\"]:\n",
    "        audio_df = pd.DataFrame(metrics[\"audio_metrics\"][\"history\"])\n",
    "        if not audio_df.empty:\n",
    "            audio_df['timestamp'] = pd.to_datetime(audio_df['timestamp'])\n",
    "            fig = px.scatter(audio_df, x='timestamp', y='transcription_time', \n",
    "                             size='duration', color='segments',\n",
    "                             title=\"Audio Transcription Performance\")\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    # Query visualization\n",
    "    st.subheader(\"Recent User Queries\")\n",
    "    if metrics[\"rag_metrics\"][\"query_history\"]:\n",
    "        query_df = pd.DataFrame(metrics[\"rag_metrics\"][\"query_history\"])\n",
    "        if not query_df.empty:\n",
    "            query_df['timestamp'] = pd.to_datetime(query_df['timestamp'])\n",
    "            query_df['used_context_label'] = query_df['used_context'].map({True: \"RAG\", False: \"Direct\"})\n",
    "            \n",
    "            fig = px.scatter(query_df, x='timestamp', y='response_time',\n",
    "                            color='used_context_label', hover_data=['query'],\n",
    "                            title=\"Query Response Times\")\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "            \n",
    "            # Display recent queries in a table\n",
    "            st.dataframe(query_df[['timestamp', 'query', 'used_context_label', 'response_time']]\n",
    "                        .rename(columns={{'used_context_label': 'Method', 'response_time': 'Time (sec)'}}))\n",
    "    \n",
    "    st.text(f\"Last updated: {{time.strftime('%Y-%m-%d %H:%M:%S')}}\")\n",
    "    st.text(f\"Dashboard port: {{dashboard_port}}, Metrics API port: {{metrics_port}}\")\n",
    "    \n",
    "    # Auto-refresh every 5 seconds\n",
    "    time.sleep(5)\n",
    "    st.experimental_rerun()\n",
    "else:\n",
    "    st.warning(\"Failed to fetch metrics data. Make sure the metrics endpoint is running.\")\n",
    "    st.text(f\"Looking for metrics at: {{METRICS_API_URL}}\")\n",
    "    time.sleep(5)\n",
    "    st.experimental_rerun()\n",
    "\"\"\"\n",
    "    \n",
    "    # Write the Streamlit app to a file\n",
    "    with open(\"rag_dashboard.py\", \"w\") as f:\n",
    "        f.write(dashboard_code)\n",
    "    \n",
    "    # Create a simple Flask server to serve the metrics with dynamic port\n",
    "    flask_code = f\"\"\"\n",
    "from flask import Flask, jsonify\n",
    "import json\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/metrics')\n",
    "def get_metrics():\n",
    "    try:\n",
    "        with open('rag_metrics.json', 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        return jsonify(metrics)\n",
    "    except Exception as e:\n",
    "        return jsonify({{\"error\": str(e)}}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='127.0.0.1', port={metrics_port})\n",
    "\"\"\"\n",
    "    \n",
    "    with open(\"metrics_server.py\", \"w\") as f:\n",
    "        f.write(flask_code)\n",
    "    \n",
    "    # Export the current metrics to a JSON file\n",
    "    with open(\"rag_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics_tracker.export_metrics(), f)\n",
    "    \n",
    "    # Start the flask server in a separate thread\n",
    "    def run_flask():\n",
    "        os.system(\"python metrics_server.py\")\n",
    "    \n",
    "    flask_thread = threading.Thread(target=run_flask)\n",
    "    flask_thread.daemon = True\n",
    "    flask_thread.start()\n",
    "    \n",
    "    # Start the Streamlit dashboard with the selected port\n",
    "    print(\"Starting RAG pipeline dashboard...\")\n",
    "    os.system(f\"streamlit run rag_dashboard.py --server.port={{dashboard_port}}\")\n",
    "\n",
    "# Run the dashboard in a separate thread to avoid blocking the notebook\n",
    "dashboard_thread = threading.Thread(target=create_dashboard)\n",
    "dashboard_thread.daemon = True\n",
    "dashboard_thread.start()\n",
    "\n",
    "print(f\"RAG pipeline monitoring dashboard is starting...\")\n",
    "print(f\"Access the dashboard at http://localhost:{{dashboard_port}}\")\n",
    "print(f\"Metrics API running at http://localhost:{{metrics_port}}/metrics\")\n",
    "\n",
    "# Example of RAG metrics tracking:\n",
    "print(\"\\nExample of RAG metrics tracking:\")\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "# Example of tracking a query to demonstrate the logging\n",
    "def simulate_rag_query(query, use_context=True):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if use_context:\n",
    "        context = tracked_vector_search_tool(query)\n",
    "        # Simulate RAG processing with context\n",
    "        time.sleep(0.5)  # Simulate processing time\n",
    "    else:\n",
    "        # Simulate direct answer without context\n",
    "        time.sleep(0.2)  # Simulate faster processing\n",
    "    \n",
    "    response_time = time.time() - start_time\n",
    "    metrics_tracker.log_user_query(query, use_context, response_time)\n",
    "    \n",
    "    # Export updated metrics\n",
    "    with open(\"rag_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics_tracker.export_metrics(), f)\n",
    "    \n",
    "    return \"Response time: {:.2f}s, Context used: {}\".format(response_time, use_context)\n",
    "\n",
    "# Simulate a few queries with and without context\n",
    "print(simulate_rag_query(\"What are the main topics discussed in the audio?\", use_context=True))\n",
    "print(simulate_rag_query(\"Who is speaking in the audio?\", use_context=True))\n",
    "print(simulate_rag_query(\"What is the background noise?\", use_context=False))\n",
    "print(simulate_rag_query(\"When was this recording made?\", use_context=False))\n",
    "\n",
    "print(f\"\\nOpen the dashboard at http://localhost:{{dashboard_port}} to see these metrics visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Step 13: IPFS Audio Content Discovery and Processing\n",
    "\n",
    "This step implements functionality to discover and retrieve audio content stored on IPFS by examining metadata. This allows the AI to expand its knowledge base by incorporating audio content from decentralized storage systems, effectively giving it access to a wider range of audio sources beyond local files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IPFS Audio Content Discovery and Processing ---\n",
    "\n",
    "%pip install -q ipfshttpclient requests\n",
    "\n",
    "import ipfshttpclient\n",
    "import requests\n",
    "import json\n",
    "import tempfile\n",
    "import os\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "def search_ipfs_audio_content(gateway_url=\"https://ipfs.io/api/v0\", query=None, limit=10):\n",
    "    \"\"\"\n",
    "    Search for audio content on IPFS using metadata\n",
    "    \n",
    "    Args:\n",
    "        gateway_url: IPFS gateway URL\n",
    "        query: Optional search terms\n",
    "        limit: Maximum number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of dicts containing CIDs and metadata for audio files\n",
    "    \"\"\"\n",
    "    print(f\"Searching IPFS for audio content{f' related to {query}' if query else ''}...\")\n",
    "    audio_files = []\n",
    "    \n",
    "    try:\n",
    "        # Try to connect to a local IPFS daemon first\n",
    "        try:\n",
    "            client = ipfshttpclient.connect()\n",
    "            print(\"Connected to local IPFS daemon\")\n",
    "        except:\n",
    "            print(\"No local IPFS daemon found, using gateway API\")\n",
    "            client = None\n",
    "            \n",
    "        # Method 1: If we have a local client, use it to search the DHT\n",
    "        if client:\n",
    "            # Get list of pins if no query provided\n",
    "            if not query:\n",
    "                pins = client.pin.ls(type=\"recursive\")\n",
    "                for pin in pins[\"Keys\"]:\n",
    "                    try:\n",
    "                        # Get metadata for each pin\n",
    "                        metadata = client.object.get(pin)\n",
    "                        if is_audio_by_metadata(metadata):\n",
    "                            audio_files.append({\n",
    "                                \"cid\": pin,\n",
    "                                \"metadata\": metadata,\n",
    "                                \"source\": \"local_ipfs\"\n",
    "                            })\n",
    "                            if len(audio_files) >= limit:\n",
    "                                break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error examining pin {pin}: {e}\")\n",
    "            else:\n",
    "                # If query provided, search IPNS/IPFS links containing audio-related terms\n",
    "                # This is simplified as full DHT search requires more complex code\n",
    "                pass\n",
    "        \n",
    "        # Method 2: Use IPFS gateway API or a pinning service\n",
    "        # This is a simplified implementation that would need a specific gateway API supporting search\n",
    "        if query and len(audio_files) < limit:\n",
    "            search_url = f\"{gateway_url}/search?q={query}+audio+filetype:mp3+filetype:wav\"\n",
    "            try:\n",
    "                response = requests.get(search_url, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    results = response.json()\n",
    "                    for result in results:\n",
    "                        if is_audio_by_metadata(result.get(\"metadata\", {})):\n",
    "                            audio_files.append({\n",
    "                                \"cid\": result.get(\"cid\"),\n",
    "                                \"metadata\": result.get(\"metadata\", {}),\n",
    "                                \"source\": \"gateway\"\n",
    "                            })\n",
    "                            if len(audio_files) >= limit:\n",
    "                                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error searching gateway: {e}\")\n",
    "                \n",
    "        # Fallback: Use known audio CIDs for demonstration\n",
    "        if len(audio_files) == 0:\n",
    "            print(\"No audio files found via search, using example files for demonstration\")\n",
    "            # These are example CIDs - they would need to be replaced with actual audio CIDs\n",
    "            example_audio_cids = [\n",
    "                \"QmYwAPJzv5CZsnA625s3Xf2nemtYgPpHdWEz79ojWnPbdG/demo.mp3\",\n",
    "                \"QmZ4tDuvesekSs4qM5ZBKpXiZGun7S2CYtEZRB3DYXkjGx/audio_sample.wav\"\n",
    "            ]\n",
    "            audio_files = [{\"cid\": cid, \"metadata\": {\"type\": \"audio\"}, \"source\": \"example\"} for cid in example_audio_cids]\n",
    "            \n",
    "        print(f\"Found {len(audio_files)} audio files on IPFS\")\n",
    "        return audio_files\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error searching IPFS: {e}\")\n",
    "        return []\n",
    "\n",
    "def is_audio_by_metadata(metadata):\n",
    "    \"\"\"\n",
    "    Check if a file is an audio file based on its metadata\n",
    "    \n",
    "    Args:\n",
    "        metadata: File metadata dict\n",
    "        \n",
    "    Returns:\n",
    "        Boolean indicating if the file is likely an audio file\n",
    "    \"\"\"\n",
    "    # Check MIME type if available\n",
    "    mime_type = metadata.get(\"MimeType\", \"\")\n",
    "    if mime_type.startswith(\"audio/\"):\n",
    "        return True\n",
    "        \n",
    "    # Check file extension\n",
    "    name = metadata.get(\"Name\", \"\")\n",
    "    audio_extensions = ['.mp3', '.wav', '.ogg', '.flac', '.m4a', '.aac']\n",
    "    if any(name.lower().endswith(ext) for ext in audio_extensions):\n",
    "        return True\n",
    "        \n",
    "    # Check metadata tags\n",
    "    tags = metadata.get(\"Tags\", [])\n",
    "    audio_tags = [\"audio\", \"sound\", \"music\", \"recording\", \"voice\", \"speech\"]\n",
    "    if any(tag in audio_tags for tag in tags):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def retrieve_audio_from_ipfs(cid, gateway_url=\"https://ipfs.io/ipfs\"):\n",
    "    \"\"\"\n",
    "    Download audio file from IPFS\n",
    "    \n",
    "    Args:\n",
    "        cid: Content identifier for the IPFS file\n",
    "        gateway_url: IPFS gateway URL\n",
    "        \n",
    "    Returns:\n",
    "        Path to downloaded file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First try with a local IPFS client\n",
    "        try:\n",
    "            client = ipfshttpclient.connect()\n",
    "            print(f\"Retrieving {{cid}} using local IPFS node\")\n",
    "            \n",
    "            # Create temp file\n",
    "            fd, temp_path = tempfile.mkstemp(suffix=\".\" + cid.split(\".\")[-1] if \".\" in cid else \".mp3\")\n",
    "            os.close(fd)\n",
    "            \n",
    "            # Get the file from IPFS and save it\n",
    "            client.get(cid, target=temp_path)\n",
    "            return temp_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Local IPFS retrieval failed: {e}\")\n",
    "            \n",
    "            # Fall back to gateway\n",
    "            file_url = f\"{gateway_url}/{cid}\"\n",
    "            print(f\"Retrieving {{file_url}} using gateway\")\n",
    "            \n",
    "            response = requests.get(file_url, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                # Create temp file\n",
    "                fd, temp_path = tempfile.mkstemp(suffix=\".\" + cid.split(\".\")[-1] if \".\" in cid else \".mp3\")\n",
    "                with os.fdopen(fd, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=1024):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                return temp_path\n",
    "            else:\n",
    "                print(f\"Failed to retrieve file: {response.status_code}\")\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving from IPFS: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_ipfs_audio_content(query=None, limit=3):\n",
    "    \"\"\"\n",
    "    Main function to search for, retrieve, and process audio content from IPFS\n",
    "    \n",
    "    Args:\n",
    "        query: Optional search query\n",
    "        limit: Maximum number of files to process\n",
    "        \n",
    "    Returns:\n",
    "        List of processed documents\n",
    "    \"\"\"\n",
    "    print(f\"Searching for{''+(' '+query if query else '')} audio content on IPFS...\")\n",
    "    audio_files = search_ipfs_audio_content(query=query, limit=limit)\n",
    "    \n",
    "    processed_documents = []\n",
    "    \n",
    "    for i, audio_file in enumerate(audio_files):\n",
    "        print(f\"\\nProcessing IPFS audio file {{i+1}}/{{len(audio_files)}}: {{audio_file['cid']}}\")\n",
    "        \n",
    "        # Retrieve the audio file\n",
    "        local_path = retrieve_audio_from_ipfs(audio_file['cid'])\n",
    "        if not local_path:\n",
    "            print(\"Failed to retrieve audio file\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            print(f\"Downloaded to {{local_path}}, transcribing...\")\n",
    "            \n",
    "            # Transcribe using our existing Whisper model\n",
    "            result = model.transcribe(local_path)\n",
    "            text_content = result[\"text\"]\n",
    "            \n",
    "            # Create document\n",
    "            ipfs_doc = AudioDocument(text_content)\n",
    "            ipfs_doc.metadata = {\n",
    "                \"source\": \"ipfs\",\n",
    "                \"cid\": audio_file['cid'],\n",
    "                \"ipfs_metadata\": audio_file['metadata']\n",
    "            }\n",
    "            \n",
    "            # Add to our collection\n",
    "            processed_documents.append(ipfs_doc)\n",
    "            \n",
    "            # Process audio segments\n",
    "            segments = result[\"segments\"]\n",
    "            relevant_segments = find_relevant_audio_segments(query if query else \"audio\", \n",
    "                                                            segments, collection, embedding_model)\n",
    "            \n",
    "            # Save segments (optional)\n",
    "            if relevant_segments:\n",
    "                query_term = query if query else \"ipfs_audio\"\n",
    "                save_audio_segments(local_path, relevant_segments, \n",
    "                                  f\"ipfs_{{sanitize_filename(query_term)}}_{{i}}\")\n",
    "            \n",
    "            # Clean up the temporary file\n",
    "            try:\n",
    "                os.unlink(local_path)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing audio file: {e}\")\n",
    "            # Clean up on error\n",
    "            try:\n",
    "                os.unlink(local_path)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # If we found and processed any documents, add them to our RAG system\n",
    "    if processed_documents:\n",
    "        print(f\"\\nAdding {{len(processed_documents)}} IPFS audio documents to RAG system...\")\n",
    "        \n",
    "        # Split documents into chunks\n",
    "        ipfs_docs = text_splitter.split_documents(processed_documents)\n",
    "        \n",
    "        # Attach sound tags (we'll just use generic ones since we don't have the specific audio)\n",
    "        for doc in ipfs_docs:\n",
    "            doc.page_content = f\"Transcription from IPFS: {{doc.page_content}}\\nSource: IPFS CID {{doc.metadata.get('cid', 'unknown')}}\"\n",
    "        \n",
    "        # Create embeddings\n",
    "        doc_texts = [doc.page_content for doc in ipfs_docs]\n",
    "        document_embeddings = embedding_model.encode(doc_texts, convert_to_numpy=True)\n",
    "        \n",
    "        # Add to ChromaDB\n",
    "        start_idx = collection.count()\n",
    "        for i, embedding in enumerate(document_embeddings):\n",
    "            collection.add(\n",
    "                ids=[f\"ipfs_{{start_idx + i}}\"],\n",
    "                embeddings=[embedding.tolist()],\n",
    "                metadatas=[{\"text\": doc_texts[i], \"source\": \"ipfs\"}]\n",
    "            )\n",
    "        \n",
    "        print(f\"Successfully added {{len(ipfs_docs)}} IPFS audio document chunks to the RAG system\")\n",
    "        \n",
    "        # Update metrics\n",
    "        metrics_tracker.log_embedding_creation(len(document_embeddings))\n",
    "    else:\n",
    "        print(\"No IPFS audio documents were processed\")\n",
    "    \n",
    "    return processed_documents\n",
    "\n",
    "# Example usage - try to find audio related to music\n",
    "# Uncomment to execute\n",
    "# ipfs_docs = process_ipfs_audio_content(query=\"music\", limit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating IPFS Audio Content Integration\n",
    "\n",
    "Let's demonstrate how to use the IPFS audio discovery and integration functionality with a simple example. You can customize the search query to find specific types of audio content on IPFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Demo: Integrate IPFS Audio Content with RAG ---\n",
    "\n",
    "# Search for interview audio on IPFS and process it\n",
    "ipfs_docs = process_ipfs_audio_content(query=\"interview\", limit=1)\n",
    "\n",
    "# If we found any documents, test a query that might use this new knowledge\n",
    "if ipfs_docs:\n",
    "    print(\"\\n--- Testing RAG with IPFS content ---\")\n",
    "    test_query = \"What interviews are available on IPFS?\"\n",
    "    print(f\"Query: {test_query}\")\n",
    "    \n",
    "    # Get context from our RAG system (which now includes IPFS content)\n",
    "    context = vector_search_tool(test_query)\n",
    "    print(f\"Retrieved context:\\n{context}\\n\")\n",
    "    \n",
    "    # Generate a response using our LLM\n",
    "    prompt = f\"\"\"\n",
    "You are an AI assistant with knowledge about audio content.\n",
    "Based on the following context, please answer the question: {test_query}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    \n",
    "    response = llm(prompt, max_tokens=500)\n",
    "    print(f\"LLM Response:\\n{response['choices'][0]['text']}\")\n",
    "else:\n",
    "    print(\"\\nNo IPFS content was retrieved. You can try again with a different query.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Update Metrics Dashboard with IPFS Integration ---\n",
    "\n",
    "# Extend our RAGMetricsTracker class to include IPFS metrics\n",
    "class IPFSMetrics:\n",
    "    def __init__(self):\n",
    "        self.audio_files_discovered = 0\n",
    "        self.audio_files_processed = 0\n",
    "        self.retrieval_errors = 0\n",
    "        self.processing_errors = 0\n",
    "        self.last_search_query = \"\"\n",
    "        self.last_search_results = 0\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"audio_files_discovered\": self.audio_files_discovered,\n",
    "            \"audio_files_processed\": self.audio_files_processed,\n",
    "            \"retrieval_errors\": self.retrieval_errors,\n",
    "            \"processing_errors\": self.processing_errors,\n",
    "            \"last_search_query\": self.last_search_query,\n",
    "            \"last_search_results\": self.last_search_results\n",
    "        }\n",
    "\n",
    "# Add IPFS metrics to our tracker\n",
    "metrics_tracker.ipfs_metrics = IPFSMetrics()\n",
    "\n",
    "# Update the export_metrics method to include IPFS metrics\n",
    "original_export_metrics = metrics_tracker.export_metrics\n",
    "\n",
    "def extended_export_metrics():\n",
    "    metrics = original_export_metrics()\n",
    "    metrics[\"ipfs_metrics\"] = metrics_tracker.ipfs_metrics.to_dict()\n",
    "    return metrics\n",
    "\n",
    "# Replace the original method\n",
    "metrics_tracker.export_metrics = extended_export_metrics\n",
    "\n",
    "# Update the dashboard code to display IPFS metrics\n",
    "dashboard_code = \"\"\"\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "st.set_page_config(page_title=\"RAG Pipeline Monitor\", layout=\"wide\")\n",
    "\n",
    "st.title(\"ðŸ“Š Agentic RAG Pipeline Monitoring\")\n",
    "\n",
    "# Create metrics columns\n",
    "col1, col2, col3, col4 = st.columns(4)\n",
    "\n",
    "# Initialize session state\n",
    "if 'last_update' not in st.session_state:\n",
    "    st.session_state.last_update = time.time()\n",
    "    st.session_state.metrics_history = []\n",
    "\n",
    "# Function to fetch metrics from the metrics endpoint\n",
    "def get_metrics():\n",
    "    try:\n",
    "        response = requests.get(METRICS_API_URL)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            st.error(f\"Failed to fetch metrics: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error fetching metrics: {e}\")\n",
    "        return None\n",
    "\n",
    "# Set the metrics API endpoint\n",
    "METRICS_API_URL = \"http://127.0.0.1:METRICS_PORT/metrics\"\n",
    "\n",
    "metrics = get_metrics()\n",
    "if metrics:\n",
    "    # Store metrics in history\n",
    "    st.session_state.metrics_history.append(metrics)\n",
    "    if len(st.session_state.metrics_history) > 20:\n",
    "        st.session_state.metrics_history.pop(0)\n",
    "    \n",
    "    # Display metrics in columns\n",
    "    with col1:\n",
    "        st.subheader(\"Audio Processing\")\n",
    "        st.metric(\"Files Processed\", metrics[\"audio_metrics\"][\"processed_files\"])\n",
    "        st.metric(\"Total Duration (sec)\", round(metrics[\"audio_metrics\"][\"total_duration\"], 2))\n",
    "        st.metric(\"Segments Extracted\", metrics[\"audio_metrics\"][\"segments_extracted\"])\n",
    "    \n",
    "    with col2:\n",
    "        st.subheader(\"ChromaDB Operations\")\n",
    "        st.metric(\"Total Queries\", metrics[\"chromadb_metrics\"][\"queries\"])\n",
    "        st.metric(\"Embeddings Created\", metrics[\"chromadb_metrics\"][\"embeddings_created\"])\n",
    "        st.metric(\"Avg Query Time (ms)\", round(metrics[\"chromadb_metrics\"][\"avg_query_time\"] * 1000, 2))\n",
    "    \n",
    "    with col3:\n",
    "        st.subheader(\"RAG Performance\")\n",
    "        st.metric(\"User Queries\", metrics[\"rag_metrics\"][\"total_queries\"])\n",
    "        rag_usage = metrics[\"rag_metrics\"][\"context_used\"]\n",
    "        direct_answers = metrics[\"rag_metrics\"][\"direct_answers\"]\n",
    "        total = rag_usage + direct_answers\n",
    "        \n",
    "        if total > 0:\n",
    "            rag_percentage = (rag_usage / total) * 100\n",
    "            st.metric(\"RAG Usage\", f\"{rag_percentage:.1f}%\")\n",
    "        else:\n",
    "            st.metric(\"RAG Usage\", \"0%\")\n",
    "    \n",
    "    # New IPFS metrics column\n",
    "    with col4:\n",
    "        st.subheader(\"IPFS Integration\")\n",
    "        if \"ipfs_metrics\" in metrics:\n",
    "            ipfs_metrics = metrics[\"ipfs_metrics\"]\n",
    "            st.metric(\"Files Discovered\", ipfs_metrics[\"audio_files_discovered\"])\n",
    "            st.metric(\"Files Processed\", ipfs_metrics[\"audio_files_processed\"])\n",
    "            \n",
    "            # Show search info if available\n",
    "            if ipfs_metrics[\"last_search_query\"]:\n",
    "                st.text(f\"Last search: {ipfs_metrics['last_search_query']}\")\n",
    "                st.text(f\"Results: {ipfs_metrics['last_search_results']}\")\n",
    "        else:\n",
    "            st.text(\"No IPFS metrics available\")\n",
    "    \n",
    "    # Time series chart of audio processing\n",
    "    st.subheader(\"Audio Processing History\")\n",
    "    if metrics[\"audio_metrics\"][\"history\"]:\n",
    "        audio_df = pd.DataFrame(metrics[\"audio_metrics\"][\"history\"])\n",
    "        if not audio_df.empty:\n",
    "            audio_df['timestamp'] = pd.to_datetime(audio_df['timestamp'])\n",
    "            fig = px.scatter(audio_df, x='timestamp', y='transcription_time', \n",
    "                             size='duration', color='segments',\n",
    "                             title=\"Audio Transcription Performance\")\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    # Query visualization\n",
    "    st.subheader(\"Recent User Queries\")\n",
    "    if metrics[\"rag_metrics\"][\"query_history\"]:\n",
    "        query_df = pd.DataFrame(metrics[\"rag_metrics\"][\"query_history\"])\n",
    "        if not query_df.empty:\n",
    "            query_df['timestamp'] = pd.to_datetime(query_df['timestamp'])\n",
    "            query_df['used_context_label'] = query_df['used_context'].map({True: \"RAG\", False: \"Direct\"})\n",
    "            \n",
    "            fig = px.scatter(query_df, x='timestamp', y='response_time',\n",
    "                            color='used_context_label', hover_data=['query'],\n",
    "                            title=\"Query Response Times\")\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "            \n",
    "            # Display recent queries in a table\n",
    "            st.dataframe(query_df[['timestamp', 'query', 'used_context_label', 'response_time']]\n",
    "                        .rename(columns={'used_context_label': 'Method', 'response_time': 'Time (sec)'}))\n",
    "    \n",
    "    st.text(f\"Last updated: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    st.text(f\"Dashboard port: {dashboard_port}, Metrics API port: {metrics_port}\")\n",
    "    \n",
    "    # Auto-refresh every 5 seconds\n",
    "    time.sleep(5)\n",
    "    st.experimental_rerun()\n",
    "else:\n",
    "    st.warning(\"Failed to fetch metrics data. Make sure the metrics endpoint is running.\")\n",
    "    st.text(f\"Looking for metrics at: {METRICS_API_URL}\")\n",
    "    time.sleep(5)\n",
    "    st.experimental_rerun()\n",
    "\"\"\"\n",
    "\n",
    "# Write the updated Streamlit app to a new file\n",
    "with open(\"rag_dashboard_with_ipfs.py\", \"w\") as f:\n",
    "    f.write(dashboard_code)\n",
    "\n",
    "print(\"Updated dashboard code with IPFS metrics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
