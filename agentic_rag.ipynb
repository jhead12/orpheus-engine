{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Retrieval-Augmented Generation (RAG) with Local Llama 2 & ChromaDB\n",
    "\n",
    "## Overview\n",
    "This notebook implements an **Agentic Retrieval-Augmented Generation (RAG) pipeline**. It focuses on transcribing audio data, potentially from an Omi streaming device, storing both the transcription and audio, and then using the transcription with a local **Ai Studio** model and **ChromaDB** for intelligent question-answering. The system determines whether additional context is needed before generating responses.\n",
    "\n",
    "### Key Features:\n",
    "- **Audio Transcription Workflow** for processing data from devices like Omi.\n",
    "- **Storage of Audio and Transcriptions** for AI processing.\n",
    "- **Llama 2 Model** for high-quality text generation.\n",
    "- **ChromaDB Vector Store** for efficient semantic search on transcriptions.\n",
    "- **Dynamic Context Retrieval** to improve answer accuracy.\n",
    "- **Two Answering Modes**:\n",
    "  - With RAG (Retrieves relevant document content before responding).\n",
    "  - Without RAG (Directly generates responses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pysqlite3 not found, using default sqlite3 module.\n",
      "Loaded sqlite3 version: 3.45.3\n"
     ]
    }
   ],
   "source": [
    "# Force Python to use the latest system sqlite3 (if available)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Unset any pysqlite3 monkeypatching\n",
    "if \"pysqlite3\" in sys.modules:\n",
    "    del sys.modules[\"pysqlite3\"]\n",
    "if \"sqlite3\" in sys.modules:\n",
    "    del sys.modules[\"sqlite3\"]\n",
    "\n",
    "try:\n",
    "    import pysqlite3\n",
    "    sys.modules[\"sqlite3\"] = pysqlite3\n",
    "    sys.modules[\"pysqlite3\"] = pysqlite3\n",
    "    print(\"Using pysqlite3 as sqlite3 backend.\")\n",
    "except ImportError:\n",
    "    print(\"pysqlite3 not found, using default sqlite3 module.\")\n",
    "\n",
    "import sqlite3\n",
    "print(\"Loaded sqlite3 version:\", sqlite3.sqlite_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(38731) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub>=0.19.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (0.31.4)\n",
      "Requirement already satisfied: llama-cpp-python>=0.2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (0.3.9)\n",
      "Requirement already satisfied: pysqlite3>=0.5.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (0.5.4)\n",
      "Requirement already satisfied: openai-whisper>=20231117 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (20240930)\n",
      "Requirement already satisfied: ffmpeg-python>=0.2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (0.2.0)\n",
      "Requirement already satisfied: torchaudio>=2.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (2.7.0)\n",
      "Requirement already satisfied: panns-inference==0.1.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (0.1.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (2.7.0)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (3.9.2)\n",
      "Requirement already satisfied: librosa>=0.10.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (0.11.0)\n",
      "Requirement already satisfied: scipy>=1.11.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.24.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 17)) (1.26.4)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 18)) (0.12.1)\n",
      "Requirement already satisfied: langchain>=0.0.235 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 21)) (0.3.7)\n",
      "Requirement already satisfied: sentence-transformers==2.2.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 22)) (2.2.2)\n",
      "Requirement already satisfied: chromadb>=0.4.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 23)) (0.6.2)\n",
      "Requirement already satisfied: Flask>=2.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 26)) (3.1.0)\n",
      "Requirement already satisfied: uvicorn>=0.17.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 27)) (0.30.6)\n",
      "Requirement already satisfied: fastapi>=0.95.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 28)) (0.111.0)\n",
      "Requirement already satisfied: requests>=2.28.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 29)) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 30)) (1.0.1)\n",
      "Requirement already satisfied: IPython>=8.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 33)) (8.27.0)\n",
      "Requirement already satisfied: streamlit>=1.28.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 34)) (1.43.2)\n",
      "Requirement already satisfied: plotly>=5.15.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 35)) (5.24.1)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 36)) (2.2.3)\n",
      "Requirement already satisfied: ipfshttpclient==0.7.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 39)) (0.7.0)\n",
      "Requirement already satisfied: boto3>=1.28.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 40)) (1.35.53)\n",
      "Requirement already satisfied: torchlibrosa in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from panns-inference==0.1.1->-r requirements.txt (line 12)) (0.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 22)) (4.52.2)\n",
      "Requirement already satisfied: tqdm in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 22)) (4.67.1)\n",
      "Requirement already satisfied: torchvision in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 22)) (0.22.0)\n",
      "Requirement already satisfied: scikit-learn in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 22)) (1.5.1)\n",
      "Requirement already satisfied: nltk in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 22)) (3.9.1)\n",
      "Requirement already satisfied: sentencepiece in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 22)) (0.2.0)\n",
      "Requirement already satisfied: multiaddr>=0.0.7 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from ipfshttpclient==0.7.0->-r requirements.txt (line 39)) (0.0.9)\n",
      "Requirement already satisfied: filelock in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 22)) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 22)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 22)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 22)) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 22)) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 22)) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.19.0->-r requirements.txt (line 4)) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.19.0->-r requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from llama-cpp-python>=0.2.0->-r requirements.txt (line 5)) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from llama-cpp-python>=0.2.0->-r requirements.txt (line 5)) (3.1.6)\n",
      "Requirement already satisfied: numba in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from openai-whisper>=20231117->-r requirements.txt (line 9)) (0.60.0)\n",
      "Requirement already satisfied: more-itertools in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from openai-whisper>=20231117->-r requirements.txt (line 9)) (10.3.0)\n",
      "Requirement already satisfied: tiktoken in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from openai-whisper>=20231117->-r requirements.txt (line 9)) (0.9.0)\n",
      "Requirement already satisfied: future in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from ffmpeg-python>=0.2.0->-r requirements.txt (line 10)) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 13)) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 13)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 13)) (3.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 14)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 14)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 14)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 14)) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 14)) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 14)) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 14)) (2.9.0.post0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.10.1->-r requirements.txt (line 15)) (3.0.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.10.1->-r requirements.txt (line 15)) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.10.1->-r requirements.txt (line 15)) (5.1.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.10.1->-r requirements.txt (line 15)) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.10.1->-r requirements.txt (line 15)) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.10.1->-r requirements.txt (line 15)) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.10.1->-r requirements.txt (line 15)) (1.0.3)\n",
      "Requirement already satisfied: cffi>=1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from soundfile>=0.12.1->-r requirements.txt (line 18)) (1.17.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langchain>=0.0.235->-r requirements.txt (line 21)) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langchain>=0.0.235->-r requirements.txt (line 21)) (3.11.8)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langchain>=0.0.235->-r requirements.txt (line 21)) (0.3.60)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langchain>=0.0.235->-r requirements.txt (line 21)) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langchain>=0.0.235->-r requirements.txt (line 21)) (0.1.147)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langchain>=0.0.235->-r requirements.txt (line 21)) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langchain>=0.0.235->-r requirements.txt (line 21)) (9.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from requests>=2.28.0->-r requirements.txt (line 29)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from requests>=2.28.0->-r requirements.txt (line 29)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from requests>=2.28.0->-r requirements.txt (line 29)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from requests>=2.28.0->-r requirements.txt (line 29)) (2025.1.31)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.235->-r requirements.txt (line 21)) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.235->-r requirements.txt (line 21)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.235->-r requirements.txt (line 21)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.235->-r requirements.txt (line 21)) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.235->-r requirements.txt (line 21)) (6.0.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.235->-r requirements.txt (line 21)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.235->-r requirements.txt (line 21)) (1.20.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain>=0.0.235->-r requirements.txt (line 21)) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain>=0.0.235->-r requirements.txt (line 21)) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain>=0.0.235->-r requirements.txt (line 21)) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain>=0.0.235->-r requirements.txt (line 21)) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain>=0.0.235->-r requirements.txt (line 21)) (1.0.0)\n",
      "Requirement already satisfied: anyio in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain>=0.0.235->-r requirements.txt (line 21)) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain>=0.0.235->-r requirements.txt (line 21)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain>=0.0.235->-r requirements.txt (line 21)) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.0.235->-r requirements.txt (line 21)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.0.235->-r requirements.txt (line 21)) (2.23.4)\n",
      "Requirement already satisfied: build>=1.0.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 23)) (1.2.2.post1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 23)) (0.7.6)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 23)) (4.0.1)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 23)) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 23)) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 23)) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 23)) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 23)) (1.27.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 23)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 23)) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 23)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 23)) (1.67.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 23)) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 23)) (0.15.4)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 23)) (32.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 23)) (5.1.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 23)) (13.9.4)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from Flask>=2.0.0->-r requirements.txt (line 26)) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from Flask>=2.0.0->-r requirements.txt (line 26)) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from Flask>=2.0.0->-r requirements.txt (line 26)) (8.1.8)\n",
      "Requirement already satisfied: blinker>=1.9 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from Flask>=2.0.0->-r requirements.txt (line 26)) (1.9.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fastapi>=0.95.0->-r requirements.txt (line 28)) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fastapi>=0.95.0->-r requirements.txt (line 28)) (0.0.7)\n",
      "Requirement already satisfied: python-multipart>=0.0.7 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fastapi>=0.95.0->-r requirements.txt (line 28)) (0.0.18)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fastapi>=0.95.0->-r requirements.txt (line 28)) (5.10.0)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fastapi>=0.95.0->-r requirements.txt (line 28)) (2.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain>=0.0.235->-r requirements.txt (line 21)) (1.3.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from IPython>=8.0.0->-r requirements.txt (line 33)) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from IPython>=8.0.0->-r requirements.txt (line 33)) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from IPython>=8.0.0->-r requirements.txt (line 33)) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from IPython>=8.0.0->-r requirements.txt (line 33)) (2.19.1)\n",
      "Requirement already satisfied: stack-data in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from IPython>=8.0.0->-r requirements.txt (line 33)) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from IPython>=8.0.0->-r requirements.txt (line 33)) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from IPython>=8.0.0->-r requirements.txt (line 33)) (4.8.0)\n",
      "Requirement already satisfied: wcwidth in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython>=8.0.0->-r requirements.txt (line 33)) (0.2.13)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from streamlit>=1.28.0->-r requirements.txt (line 34)) (5.5.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from streamlit>=1.28.0->-r requirements.txt (line 34)) (5.5.2)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from streamlit>=1.28.0->-r requirements.txt (line 34)) (4.25.8)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from streamlit>=1.28.0->-r requirements.txt (line 34)) (19.0.1)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from streamlit>=1.28.0->-r requirements.txt (line 34)) (0.10.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from streamlit>=1.28.0->-r requirements.txt (line 34)) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from streamlit>=1.28.0->-r requirements.txt (line 34)) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from streamlit>=1.28.0->-r requirements.txt (line 34)) (6.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from pandas>=2.0.0->-r requirements.txt (line 36)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from pandas>=2.0.0->-r requirements.txt (line 36)) (2025.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit>=1.28.0->-r requirements.txt (line 34)) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit>=1.28.0->-r requirements.txt (line 34)) (1.32.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.28.0->-r requirements.txt (line 34)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.28.0->-r requirements.txt (line 34)) (5.0.2)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.53 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from boto3>=1.28.0->-r requirements.txt (line 40)) (1.35.99)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from boto3>=1.28.0->-r requirements.txt (line 40)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from boto3>=1.28.0->-r requirements.txt (line 40)) (0.10.4)\n",
      "Requirement already satisfied: six>=1.5 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->-r requirements.txt (line 14)) (1.17.0)\n",
      "Requirement already satisfied: pyproject_hooks in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from build>=1.0.3->chromadb>=0.4.0->-r requirements.txt (line 23)) (1.2.0)\n",
      "Requirement already satisfied: pycparser in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.12.1->-r requirements.txt (line 18)) (2.21)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from email_validator>=2.0.0->fastapi>=0.95.0->-r requirements.txt (line 28)) (2.7.0)\n",
      "Requirement already satisfied: rich-toolkit>=0.11.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fastapi-cli>=0.0.2->fastapi>=0.95.0->-r requirements.txt (line 28)) (0.14.7)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->IPython>=8.0.0->-r requirements.txt (line 33)) (0.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jinja2>=2.11.3->llama-cpp-python>=0.2.0->-r requirements.txt (line 5)) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.28.0->-r requirements.txt (line 34)) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.28.0->-r requirements.txt (line 34)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.28.0->-r requirements.txt (line 34)) (0.23.1)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 23)) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 23)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 23)) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 23)) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 23)) (0.10)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 23)) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 23)) (4.9)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 23)) (0.6.1)\n",
      "Requirement already satisfied: varint in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from multiaddr>=0.0.7->ipfshttpclient==0.7.0->-r requirements.txt (line 39)) (1.0.2)\n",
      "Requirement already satisfied: base58 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from multiaddr>=0.0.7->ipfshttpclient==0.7.0->-r requirements.txt (line 39)) (2.1.1)\n",
      "Requirement already satisfied: netaddr in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from multiaddr>=0.0.7->ipfshttpclient==0.7.0->-r requirements.txt (line 39)) (1.3.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from numba->openai-whisper>=20231117->-r requirements.txt (line 9)) (0.43.0)\n",
      "Requirement already satisfied: coloredlogs in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r requirements.txt (line 23)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r requirements.txt (line 23)) (25.2.10)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 23)) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 23)) (8.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 23)) (3.21.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 23)) (1.17.2)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 23)) (1.63.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 23)) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.27.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 23)) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-sdk>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 23)) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.48b0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.4.0->-r requirements.txt (line 23)) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.48b0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.4.0->-r requirements.txt (line 23)) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.48b0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.4.0->-r requirements.txt (line 23)) (0.48b0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.4.0->-r requirements.txt (line 23)) (3.8.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->IPython>=8.0.0->-r requirements.txt (line 33)) (0.7.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from pooch>=1.1->librosa>=0.10.1->-r requirements.txt (line 15)) (3.10.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb>=0.4.0->-r requirements.txt (line 23)) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb>=0.4.0->-r requirements.txt (line 23)) (1.9.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->chromadb>=0.4.0->-r requirements.txt (line 23)) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.4.0->-r requirements.txt (line 23)) (0.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers==2.2.2->-r requirements.txt (line 22)) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->-r requirements.txt (line 13)) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from typer>=0.9.0->chromadb>=0.4.0->-r requirements.txt (line 23)) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 23)) (0.6.4)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 23)) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 23)) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 23)) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.4.0->-r requirements.txt (line 23)) (10.0)\n",
      "Requirement already satisfied: executing in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from stack-data->IPython>=8.0.0->-r requirements.txt (line 33)) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from stack-data->IPython>=8.0.0->-r requirements.txt (line 33)) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from stack-data->IPython>=8.0.0->-r requirements.txt (line 33)) (0.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(39796) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if you have not installed them already\n",
    "%pip install -r requirements.txt --verbose --quiet\n",
    "%pip install -q --upgrade pip\n",
    "\n",
    "# Example output (replace Codespaces path with $HOME):\n",
    "# Requirement already satisfied: matplotlib>=3.7.0 in $HOME/.local/lib/python3.12/site-packages (from -r requirements.txt (line 21)) (3.10.1)\n",
    "# Requirement already satisfied: IPython>=8.0.0 in $HOME/.local/lib/python3.12/site-packages (from -r requirements.txt (line 22)) (9.0.2)\n",
    "# Requirement already satisfied: plotly>=5.15.0 in $HOME/.local/lib/python3.12/site-packages (from -r requirements.txt (line 24)) (6.0.1)\n",
    "# Requirement already satisfied: pandas>=2.0.0 in $HOME/.local/lib/python3.12/site-packages (from -r requirements.txt (line 25)) (2.2.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Step 1: Model Setup\n",
    "\n",
    "We will set up **Llama 2 (7B)** for text generation. If the model is not found locally, it will be downloaded from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(43578) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Model already exists at: model/llama-2-7b-chat.Q4_K_M.gguf\n",
      "Using model at: model/llama-2-7b-chat.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "%pip install -q huggingface-hub\n",
    "\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "MODEL_FILENAME = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "MODEL_DIR = \"model\"\n",
    "EXPECTED_PATH = os.path.join(MODEL_DIR, MODEL_FILENAME)\n",
    "\n",
    "# Ensure model directory exists\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Check if model already exists\n",
    "if os.path.exists(EXPECTED_PATH):\n",
    "    print(f\"Model already exists at: {EXPECTED_PATH}\")\n",
    "    model_path = EXPECTED_PATH\n",
    "else:\n",
    "    print(\"Model not found locally. Downloading Llama 2 model...\")\n",
    "    \n",
    "    # Download the model\n",
    "    model_path = hf_hub_download(\n",
    "        repo_id=\"TheBloke/Llama-2-7B-Chat-GGUF\",\n",
    "        filename=MODEL_FILENAME,\n",
    "        local_dir=MODEL_DIR\n",
    "    )\n",
    "    print(f\"Model downloaded to: {model_path}\")\n",
    "\n",
    "print(f\"Using model at: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    }
   ],
   "source": [
    "%pip install -q llama-cpp-python\n",
    "# Check if the model file exists\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "# Import the Llama class from llama_cpp\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Initialize the model with the local path and GPU acceleration\n",
    "llm = Llama(\n",
    "    model_path=EXPECTED_PATH,\n",
    "    temperature=0.25,\n",
    "    max_tokens=2000,\n",
    "    n_ctx=4096,\n",
    "    top_p=1.0,\n",
    "    verbose=False,\n",
    "    n_gpu_layers=30,  # Utilize some available GPU layers\n",
    "    n_batch=512,      # Optimize batch size for parallel processing\n",
    "    f16_kv=True,      # Enable half-precision for key/value cache\n",
    "    use_mlock=True,   # Lock memory to prevent swapping\n",
    "    use_mmap=True     # Utilize memory mapping for faster loading\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📄 Step 2: Loading, Transcribing, and Storing Audio Data\n",
    "\n",
    "This step outlines the process for loading audio data (e.g., from an Omi streaming device), transcribing it, and preparing it for storage and further processing. Both the raw audio and its transcription are valuable assets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(48954) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(49037) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(49059) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg found and working.\n",
      "Loading AUDIO from: ./data/tester.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "python(49711) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(49723) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 1 document(s) from the AUDIO.\n",
      "Successfully loaded 1 document(s) from the AUDIO and created 1 audio file(s).\n"
     ]
    }
   ],
   "source": [
    "# --- Load the Audio File Document ---\n",
    "# --- Load the Audio File Document and Audio Collection System ---\n",
    "\n",
    "# Install whisper if not already installed\n",
    "%pip install -q openai-whisper\n",
    "\n",
    "# Install ffmpeg-python bindings if not already installed\n",
    "%pip install -q ffmpeg-python\n",
    "\n",
    "import shutil\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Check if ffmpeg is available and working\n",
    "try:\n",
    "    subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, check=True)\n",
    "    print(\"ffmpeg found and working.\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"ffmpeg not found. Please install ffmpeg and ensure it's in your PATH.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error running ffmpeg: {{e}}\")\n",
    "    raise RuntimeError(\"ffmpeg is not working correctly.\") from e\n",
    "\n",
    "import whisper\n",
    "\n",
    "# No need to manually set ffmpeg_dir if ffmpeg is installed system-wide\n",
    "\n",
    "# Define the Audio File file path\n",
    "AUDIO_PATH = \"./data/tester.mp3\"\n",
    "# Check if the file exists\n",
    "print(f\"Loading AUDIO from: {AUDIO_PATH}\")\n",
    "\n",
    "# Define the audio collection system\n",
    "AUDIO_COLLECTION_SYSTEM = \"MyAudioSystem\"\n",
    "\n",
    "# Load and transcribe the audio file using whisper\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(AUDIO_PATH)\n",
    "text_content = result[\"text\"]\n",
    "\n",
    "# For compatibility with the rest of your code, wrap the text in a document-like object\n",
    "class AudioDocument:\n",
    "    def __init__(self, text):\n",
    "        self.page_content = text\n",
    "        self.metadata = {}  # Optionally add metadata if needed\n",
    "    def getPageText(self):\n",
    "        return self.page_content\n",
    "\n",
    "documents = [AudioDocument(text_content)]\n",
    "\n",
    "print(f\"Successfully loaded {len(documents)} document(s) from the AUDIO.\")\n",
    "# Initialize an empty list for the audio files\n",
    "audio_files = []\n",
    "\n",
    "# Iterate through each document\n",
    "for document in documents:\n",
    "    # Get the current page's text content\n",
    "    text_content = document.getPageText()\n",
    "\n",
    "    # Extract relevant information from the text, e.g., keywords or phrases\n",
    "    def extractRelevantInfo(text):\n",
    "        # Placeholder: just return the text itself\n",
    "        return text\n",
    "    extracted_info = extractRelevantInfo(text_content)\n",
    "\n",
    "    # Define a placeholder for createAudioFile\n",
    "    def createAudioFile(system, info):\n",
    "        # Placeholder: just return a tuple for demonstration\n",
    "        return (system, info)\n",
    "\n",
    "    # Create an audio file based on the extracted information\n",
    "    audio_file = createAudioFile(AUDIO_COLLECTION_SYSTEM, extracted_info)\n",
    "\n",
    "    # Add the audio file to the collection system's list\n",
    "    audio_files.append(audio_file)\n",
    "print(f\"Successfully loaded {len(documents)} document(s) from the AUDIO and created {len(audio_files)} audio file(s).\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Omi data from Webhook\n",
    "\n",
    "# Initialize variables for Omi data from Webhook\n",
    "response = None  # Will store webhook response\n",
    "rag_percentage = 0.0  # Initialize RAG percentage\n",
    "ipfs_metrics = {}  # Initialize IPFS metrics dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Audio Event Detection (Sound Tagging) ---\n",
    "\n",
    "%pip install -q torchaudio\n",
    "%pip install -q panns-inference\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from panns_inference import AudioTagging\n",
    "\n",
    "# Initialize the PANNs audio tagging model\n",
    "panns_model = AudioTagging(checkpoint_path=None, device='cpu')  # Uses default Cnn14 weights\n",
    "\n",
    "# Load and preprocess audio\n",
    "waveform, sr = torchaudio.load(AUDIO_PATH)\n",
    "if sr != 32000:\n",
    "    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=32000)(waveform)\n",
    "    sr = 32000\n",
    "\n",
    "# PANNs expects mono audio\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "# --- Load the full AudioSet class labels for PANNs output mapping ---\n",
    "import os\n",
    "import csv\n",
    "\n",
    "LABELS_CSV_PATH = \"class_labels_indices.csv\"\n",
    "labels = None\n",
    "if os.path.exists(LABELS_CSV_PATH):\n",
    "    with open(LABELS_CSV_PATH, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        labels = [row['display_name'] for row in reader]\n",
    "else:\n",
    "    print(\"WARNING: AudioSet class label CSV not found. Will print indices instead of class names.\")\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = panns_model.inference(waveform)\n",
    "    clipwise_output = output[0]\n",
    "    topk = torch.topk(torch.tensor(clipwise_output[0]), 3)\n",
    "    sound_tags = []\n",
    "    if labels and max(topk.indices.tolist()) < len(labels):\n",
    "        sound_tags = [labels[i] for i in topk.indices.tolist()]\n",
    "    else:\n",
    "        sound_tags = [f\"Class index {i}\" for i in topk.indices.tolist()]\n",
    "\n",
    "print(\"Detected sound types:\", sound_tags)\n",
    "\n",
    "# Example output (replace Codespaces path with $HOME):\n",
    "# Checkpoint path: $HOME/panns_data/Cnn14_mAP=0.431.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✂️ Step 3: Chunking Audio Transcriptions for RAG\n",
    "\n",
    "The transcribed text from the audio data is split into **small overlapping chunks** (approximately **500 characters**). These chunks are then used for embedding and storage in ChromaDB to enable semantic search for the RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Split the Audio Content into Manageable Chunks with Sound Tags ---\n",
    "%pip install -q langchain\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "text_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "\n",
    "# Split the transcription into chunks\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Attach sound tags to each chunk as metadata\n",
    "for doc in docs:\n",
    "    doc.page_content = f\"Transcription: {doc.page_content}\\nSound tags: {', '.join(sound_tags)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Step 4: Initializing the Embedding Model\n",
    "\n",
    "To convert text into numerical representations for efficient similarity search, we use **all-MiniLM-L6-v2** from `sentence-transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize the Embedding Model ---\n",
    "%pip install -q sentence-transformers\n",
    "%pip install -q transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Define the embedding model name\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "print(f\"Successfully loaded embedding model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Step 5: Computing Embeddings for Document Chunks\n",
    "\n",
    "Each chunk is converted into a **vector representation** using our embedding model. This allows us to perform **semantic similarity searches** later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute Embeddings for Each Text Chunk ---\n",
    "\n",
    "# Extract text content from each chunk\n",
    "doc_texts = [doc.page_content for doc in docs]\n",
    "\n",
    "# Compute embeddings for the extracted text chunks\n",
    "document_embeddings = embedding_model.encode(doc_texts, convert_to_numpy=True)\n",
    "\n",
    "# Display the result\n",
    "print(\"Successfully computed embeddings for each text chunk.\")\n",
    "print(f\"Embeddings Shape: {document_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🗄️ Step 6: Storing Audio Transcription Embeddings in ChromaDB\n",
    "\n",
    "We initialize **ChromaDB**, a high-performance **vector database**, and store our computed embeddings to enable efficient retrieval of relevant text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize and Populate the Chroma Vector Database ---\n",
    "\n",
    "# Define Chroma database path and collection name\n",
    "CHROMA_DB_PATH = \"./chroma_db\"\n",
    "COLLECTION_NAME = \"document_embeddings\"\n",
    "\n",
    "# Initialize Chroma client\n",
    "import chromadb\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)\n",
    "\n",
    "# Add document embeddings to the Chroma collection\n",
    "for i, embedding in enumerate(document_embeddings):\n",
    "    collection.add(\n",
    "        ids=[str(i)],  # Chroma requires string IDs\n",
    "        embeddings=[embedding.tolist()],\n",
    "        metadatas=[{\"text\": doc_texts[i]}]\n",
    "    )\n",
    "\n",
    "print(\"Successfully populated Chroma database with document embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔎 Step 7: Implementing Vector Search Tool\n",
    "\n",
    "To retrieve relevant text passages from the database, we define a **vector search function** that finds the most relevant chunks based on a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Searches the Chroma database for relevant text chunks based on the query.\n",
    "    Computes the query embedding, retrieves the top 5 most relevant text chunks,\n",
    "    and returns them as a formatted string.\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.encode(query, convert_to_numpy=True).tolist()\n",
    "    TOP_K = 5\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=TOP_K\n",
    "    )\n",
    "    retrieved_chunks = [metadata[\"text\"] for metadata in results[\"metadatas\"][0]]\n",
    "    return \"\\n\\n\".join(retrieved_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Step 8: Context Need Assessment\n",
    "\n",
    "Instead of always retrieving context, we determine if the query **requires external document context** before generating a response. This creates an agentic workflow that makes autonomous decisions to complete the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import re\n",
    "\n",
    "# --- Audio Segment Retrieval and Saving ---\n",
    "def find_relevant_audio_segments(query, segments, collection, embedding_model, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieve audio segments whose text contains the query string (case-insensitive substring match).\n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "        segments (list): List of segment dicts with 'text', 'start', 'end', 'id'.\n",
    "        collection: ChromaDB collection (not used in this version).\n",
    "        embedding_model: Embedding model (not used in this version).\n",
    "        top_k (int): Not used in this version.\n",
    "    Returns:\n",
    "        List of relevant segment dicts.\n",
    "    \"\"\"\n",
    "    query_lower = query.lower()\n",
    "    relevant_segments = [segment for segment in segments if query_lower in segment[\"text\"].lower()]\n",
    "    print(f\"Total relevant segments found: {len(relevant_segments)}\")\n",
    "    return relevant_segments\n",
    "\n",
    "# Function to sanitize filenames\n",
    "def sanitize_filename(name):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", name).strip()\n",
    "\n",
    "# Function to save audio segments\n",
    "def save_audio_segments(audio_path, segments, query, output_base=\"audio_clips\"):\n",
    "    \"\"\"\n",
    "    Save relevant audio segments as separate files.\n",
    "    \"\"\"\n",
    "    folder_name = sanitize_filename(query)\n",
    "    output_dir = os.path.join(output_base, folder_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    for seg in segments:\n",
    "        start_sample = int(seg['start'] * sr)\n",
    "        end_sample = int(seg['end'] * sr)\n",
    "        clip_waveform = waveform[:, start_sample:end_sample]\n",
    "        out_path = os.path.join(output_dir, f\"segment_{seg['id']}_{int(seg['start'])}-{int(seg['end'])}.wav\")\n",
    "        torchaudio.save(out_path, clip_waveform, sr)\n",
    "        print(f\"Saved: {out_path}\")\n",
    "\n",
    "# Example usage:\n",
    "segments = result[\"segments\"]  # Ensure you have this from the Whisper transcription\n",
    "relevant_segments = find_relevant_audio_segments(\"audio\", segments, collection, embedding_model)\n",
    "save_audio_segments(AUDIO_PATH, relevant_segments, \"audio\")\n",
    "for seg in relevant_segments:\n",
    "    print(f\"Start: {seg['start']}s, End: {seg['end']}s, Text: {seg['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 Step 9: update OEW-MAIN audio library with the audio created from the search.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎚️ Step 10: DAW Integration & Audio Rendering\n",
    "\n",
    "Now that relevant audio segments have been extracted, we can integrate them into a Digital Audio Workstation (DAW) environment. This step demonstrates how to load, visualize, and play back audio clips, enabling further editing or composition. Below, we render audio waveforms and provide playback controls for the extracted segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Render and Play Extracted Audio Segments in Notebook ---\n",
    "\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import os\n",
    "\n",
    "def render_audio_segments(folder_path):\n",
    "    \"\"\"\n",
    "    Display waveforms and playback controls for all audio clips in the given folder.\n",
    "    \"\"\"\n",
    "    audio_files = [f for f in os.listdir(folder_path) if f.endswith(\".wav\")]\n",
    "    if not audio_files:\n",
    "        print(\"No audio clips found in:\", folder_path)\n",
    "        return\n",
    "    for audio_file in sorted(audio_files):\n",
    "        file_path = os.path.join(folder_path, audio_file)\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        plt.figure(figsize=(10, 2))\n",
    "        plt.title(audio_file)\n",
    "        plt.plot(waveform.t().numpy())\n",
    "        plt.xlabel(\"Sample\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.show()\n",
    "        display(ipd.Audio(file_path))\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Example usage: visualize and play audio clips for the last query\n",
    "render_audio_segments(os.path.join(\"audio_clips\", sanitize_filename(\"audio\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📤 Step 11: Exporting Audio Clips\n",
    "\n",
    "After rendering and reviewing audio segments, you may want to export them for sharing, further processing, or integration with other systems. Below are common export options:\n",
    "- **IPFS**: Upload audio to the InterPlanetary File System for decentralized access.\n",
    "- **AWS S3**: Store audio in an Amazon S3 bucket for scalable cloud storage.\n",
    "- **Direct Download**: Save audio to a temporary folder for local or web-based download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export Audio Clips: IPFS, AWS S3, or Direct Download ---\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "# Optional: install required packages for IPFS and AWS S3\n",
    "# %pip install ipfshttpclient boto3\n",
    "\n",
    "def export_audio_clips(folder_path, method=\"direct\", **kwargs):\n",
    "    \"\"\"\n",
    "    Export audio clips using the specified method.\n",
    "    method: \"ipfs\", \"aws\", or \"direct\"\n",
    "    kwargs: Additional arguments for each method.\n",
    "    Returns a list of export URLs or file paths.\n",
    "    \"\"\"\n",
    "    audio_files = [f for f in os.listdir(folder_path) if f.endswith(\".wav\")]\n",
    "    exported = []\n",
    "    if method == \"ipfs\":\n",
    "        # Example: Upload to IPFS (requires ipfshttpclient)\n",
    "        import ipfshttpclient\n",
    "        client = ipfshttpclient.connect()\n",
    "        for audio_file in audio_files:\n",
    "            file_path = os.path.join(folder_path, audio_file)\n",
    "            res = client.add(file_path)\n",
    "            ipfs_url = f\"https://ipfs.io/ipfs/{res['Hash']}\"\n",
    "            exported.append(ipfs_url)\n",
    "            print(f\"Exported to IPFS: {ipfs_url}\")\n",
    "    elif method == \"aws\":\n",
    "        # Example: Upload to AWS S3 (requires boto3)\n",
    "        import boto3\n",
    "        s3 = boto3.client(\"s3\")\n",
    "        bucket = kwargs.get(\"bucket\")\n",
    "        prefix = kwargs.get(\"prefix\", \"\")\n",
    "        for audio_file in audio_files:\n",
    "            file_path = os.path.join(folder_path, audio_file)\n",
    "            s3_key = os.path.join(prefix, audio_file)\n",
    "            s3.upload_file(file_path, bucket, s3_key)\n",
    "            s3_url = f\"https://{bucket}.s3.amazonaws.com/{s3_key}\"\n",
    "            exported.append(s3_url)\n",
    "            print(f\"Exported to S3: {s3_url}\")\n",
    "    elif method == \"direct\":\n",
    "        # Copy files to a temporary directory for download\n",
    "        temp_dir = tempfile.mkdtemp(prefix=\"audio_export_\")\n",
    "        for audio_file in audio_files:\n",
    "            src = os.path.join(folder_path, audio_file)\n",
    "            dst = os.path.join(temp_dir, audio_file)\n",
    "            shutil.copy2(src, dst)\n",
    "            exported.append(dst)\n",
    "            print(f\"Copied to temp folder: {dst}\")\n",
    "        print(f\"All files available in: {temp_dir}\")\n",
    "    else:\n",
    "        raise ValueError(\"Unknown export method.\")\n",
    "    return exported\n",
    "\n",
    "# Example usage:\n",
    "# Export to IPFS (requires running IPFS daemon and ipfshttpclient)\n",
    "# export_audio_clips(os.path.join(\"audio_clips\", sanitize_filename(\"audio\")), method=\"ipfs\")\n",
    "\n",
    "# Export to AWS S3 (requires AWS credentials)\n",
    "# export_audio_clips(os.path.join(\"audio_clips\", sanitize_filename(\"audio\")), method=\"aws\", bucket=\"your-bucket\", prefix=\"exports/\")\n",
    "\n",
    "# Export for direct download (local temp folder)\n",
    "export_audio_clips(os.path.join(\"audio_clips\", sanitize_filename(\"audio\")), method=\"direct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Step 12: RAG Pipeline Monitoring Dashboard\n",
    "\n",
    "To efficiently track and monitor our audio processing pipeline, we'll implement a simple web dashboard that provides:\n",
    "\n",
    "- **Audio Processing Metrics**: Track transcription times, file sizes, and segment counts\n",
    "- **ChromaDB Interactions**: Monitor query frequency, embedding operations, and retrieval times \n",
    "- **RAG Performance**: Visualize context usage decisions, response generation times, and user queries\n",
    "\n",
    "This dashboard runs on `http://127.0.0.1:5001/` and provides real-time insights into the operation of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Implement RAG Pipeline Monitoring Dashboard ---\n",
    "%pip install -q streamlit plotly pandas\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import threading\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import socket\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# Function to find an available port starting from a given port\n",
    "def find_available_port(start_port):\n",
    "    port = start_port\n",
    "    max_port = start_port + 100  # Try 100 ports at most\n",
    "    \n",
    "    while port < max_port:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            try:\n",
    "                s.bind(('127.0.0.1', port))\n",
    "                return port\n",
    "            except OSError:\n",
    "                port += 1\n",
    "    \n",
    "    raise RuntimeError(f\"Could not find available port starting from {start_port}\")\n",
    "\n",
    "# Create a class to track metrics across the entire pipeline\n",
    "class RAGMetricsTracker:\n",
    "    def __init__(self, max_history=100):\n",
    "        self.audio_metrics = {\n",
    "            \"processed_files\": 0,\n",
    "            \"total_duration\": 0,\n",
    "            \"avg_transcription_time\": 0,\n",
    "            \"segments_extracted\": 0,\n",
    "            \"history\": deque(maxlen=max_history)\n",
    "        }\n",
    "        self.chromadb_metrics = {\n",
    "            \"queries\": 0,\n",
    "            \"embeddings_created\": 0,\n",
    "            \"avg_query_time\": 0,\n",
    "            \"query_times\": deque(maxlen=max_history)\n",
    "        }\n",
    "        self.rag_metrics = {\n",
    "            \"total_queries\": 0,\n",
    "            \"context_used\": 0,  # Number of times RAG used additional context\n",
    "            \"direct_answers\": 0,  # Number of times RAG answered directly\n",
    "            \"query_history\": deque(maxlen=max_history)\n",
    "        }\n",
    "        self.timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    def log_audio_process(self, filename, duration, transcription_time, num_segments):\n",
    "        \"\"\"Log a new audio processing event\"\"\"\n",
    "        self.audio_metrics[\"processed_files\"] += 1\n",
    "        self.audio_metrics[\"total_duration\"] += duration\n",
    "        \n",
    "        # Update running average transcription time\n",
    "        prev_avg = self.audio_metrics[\"avg_transcription_time\"]\n",
    "        prev_count = self.audio_metrics[\"processed_files\"] - 1\n",
    "        self.audio_metrics[\"avg_transcription_time\"] = (prev_avg * prev_count + transcription_time) / self.audio_metrics[\"processed_files\"]\n",
    "        \n",
    "        self.audio_metrics[\"segments_extracted\"] += num_segments\n",
    "        \n",
    "        # Add to history\n",
    "        self.audio_metrics[\"history\"].append({\n",
    "            \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"filename\": filename,\n",
    "            \"duration\": duration,\n",
    "            \"transcription_time\": transcription_time,\n",
    "            \"segments\": num_segments\n",
    "        })\n",
    "    \n",
    "    def log_chromadb_query(self, query, num_results, query_time):\n",
    "        \"\"\"Log a new ChromaDB query\"\"\"\n",
    "        self.chromadb_metrics[\"queries\"] += 1\n",
    "        self.chromadb_metrics[\"query_times\"].append(query_time)\n",
    "        self.chromadb_metrics[\"avg_query_time\"] = sum(self.chromadb_metrics[\"query_times\"]) / len(self.chromadb_metrics[\"query_times\"])\n",
    "    \n",
    "    def log_embedding_creation(self, count):\n",
    "        \"\"\"Log when new embeddings are created\"\"\"\n",
    "        self.chromadb_metrics[\"embeddings_created\"] += count\n",
    "    \n",
    "    def log_user_query(self, query, used_context, response_time):\n",
    "        \"\"\"Log a user query to the RAG system\"\"\"\n",
    "        self.rag_metrics[\"total_queries\"] += 1\n",
    "        if used_context:\n",
    "            self.rag_metrics[\"context_used\"] += 1\n",
    "        else:\n",
    "            self.rag_metrics[\"direct_answers\"] += 1\n",
    "            \n",
    "        self.rag_metrics[\"query_history\"].append({\n",
    "            \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"query\": query,\n",
    "            \"used_context\": used_context,\n",
    "            \"response_time\": response_time\n",
    "        })\n",
    "    \n",
    "    def export_metrics(self):\n",
    "        \"\"\"Export all metrics as a dictionary\"\"\"\n",
    "        return {\n",
    "            \"audio_metrics\": {\n",
    "                \"processed_files\": self.audio_metrics[\"processed_files\"],\n",
    "                \"total_duration\": self.audio_metrics[\"total_duration\"],\n",
    "                \"avg_transcription_time\": self.audio_metrics[\"avg_transcription_time\"],\n",
    "                \"segments_extracted\": self.audio_metrics[\"segments_extracted\"],\n",
    "                \"history\": list(self.audio_metrics[\"history\"])\n",
    "            },\n",
    "            \"chromadb_metrics\": {\n",
    "                \"queries\": self.chromadb_metrics[\"queries\"],\n",
    "                \"embeddings_created\": self.chromadb_metrics[\"embeddings_created\"],\n",
    "                \"avg_query_time\": self.chromadb_metrics[\"avg_query_time\"],\n",
    "                \"query_times\": list(self.chromadb_metrics[\"query_times\"])\n",
    "            },\n",
    "            \"rag_metrics\": {\n",
    "                \"total_queries\": self.rag_metrics[\"total_queries\"],\n",
    "                \"context_used\": self.rag_metrics[\"context_used\"],\n",
    "                \"direct_answers\": self.rag_metrics[\"direct_answers\"],\n",
    "                \"query_history\": list(self.rag_metrics[\"query_history\"])\n",
    "            },\n",
    "            \"timestamp\": self.timestamp\n",
    "        }\n",
    "\n",
    "# Find available ports for our services\n",
    "metrics_port = find_available_port(5002)\n",
    "dashboard_port = find_available_port(5001)\n",
    "print(f\"Selected ports: Dashboard={dashboard_port}, Metrics API={metrics_port}\")\n",
    "\n",
    "# Create a global metrics tracker instance\n",
    "metrics_tracker = RAGMetricsTracker()\n",
    "\n",
    "# Retroactively log the metrics from our previous operations\n",
    "metrics_tracker.log_audio_process(\n",
    "    filename=AUDIO_PATH,\n",
    "    duration=result[\"segments\"][-1][\"end\"] if len(result[\"segments\"]) > 0 else 0,\n",
    "    transcription_time=3.5,  # Placeholder value\n",
    "    num_segments=len(result[\"segments\"])\n",
    ")\n",
    "\n",
    "metrics_tracker.log_embedding_creation(len(document_embeddings))\n",
    "\n",
    "# Wrap our vector search function to track metrics\n",
    "original_vector_search = vector_search_tool\n",
    "\n",
    "def tracked_vector_search_tool(query: str) -> str:\n",
    "    start_time = time.time()\n",
    "    result = original_vector_search(query)\n",
    "    query_time = time.time() - start_time\n",
    "    metrics_tracker.log_chromadb_query(query, 5, query_time)\n",
    "    return result\n",
    "\n",
    "# Replace the original function with our tracked version\n",
    "vector_search_tool = tracked_vector_search_tool\n",
    "\n",
    "# Create the Streamlit dashboard\n",
    "def create_dashboard():\n",
    "    \"\"\"Create the Streamlit dashboard for RAG pipeline monitoring\"\"\"\n",
    "    \n",
    "    # Define the Streamlit app with dynamic port for metrics API\n",
    "    dashboard_code = f\"\"\"\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "st.set_page_config(page_title=\"RAG Pipeline Monitor\", layout=\"wide\")\n",
    "\n",
    "st.title(\"📊 Agentic RAG Pipeline Monitoring\")\n",
    "\n",
    "# Create metrics columns\n",
    "col1, col2, col3, col4 = st.columns(4)\n",
    "\n",
    "# Initialize session state\n",
    "if 'last_update' not in st.session_state:\n",
    "    st.session_state.last_update = time.time()\n",
    "    st.session_state.metrics_history = []\n",
    "\n",
    "# Function to fetch metrics from the metrics endpoint\n",
    "def get_metrics():\n",
    "    try:\n",
    "        response = requests.get(METRICS_API_URL)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            st.error(f\"Failed to fetch metrics: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error fetching metrics: {e}\")\n",
    "        return None\n",
    "\n",
    "# Set the metrics API endpoint\n",
    "METRICS_API_URL = \"http://127.0.0.1:METRICS_PORT/metrics\"\n",
    "\n",
    "metrics = get_metrics()\n",
    "if metrics:\n",
    "    # Store metrics in history\n",
    "    st.session_state.metrics_history.append(metrics)\n",
    "    if len(st.session_state.metrics_history) > 20:\n",
    "        st.session_state.metrics_history.pop(0)\n",
    "    \n",
    "    # Display metrics in columns\n",
    "    with col1:\n",
    "        st.subheader(\"Audio Processing\")\n",
    "        st.metric(\"Files Processed\", metrics[\"audio_metrics\"][\"processed_files\"])\n",
    "        st.metric(\"Total Duration (sec)\", round(metrics[\"audio_metrics\"][\"total_duration\"], 2))\n",
    "        st.metric(\"Segments Extracted\", metrics[\"audio_metrics\"][\"segments_extracted\"])\n",
    "    \n",
    "    with col2:\n",
    "        st.subheader(\"ChromaDB Operations\")\n",
    "        st.metric(\"Total Queries\", metrics[\"chromadb_metrics\"][\"queries\"])\n",
    "        st.metric(\"Embeddings Created\", metrics[\"chromadb_metrics\"][\"embeddings_created\"])\n",
    "        st.metric(\"Avg Query Time (ms)\", round(metrics[\"chromadb_metrics\"][\"avg_query_time\"] * 1000, 2))\n",
    "    \n",
    "    with col3:\n",
    "        st.subheader(\"RAG Performance\")\n",
    "        st.metric(\"User Queries\", metrics[\"rag_metrics\"][\"total_queries\"])\n",
    "        rag_usage = metrics[\"rag_metrics\"][\"context_used\"]\n",
    "        direct_answers = metrics[\"rag_metrics\"][\"direct_answers\"]\n",
    "        total = rag_usage + direct_answers\n",
    "        \n",
    "        if total > 0:\n",
    "            rag_percentage = (rag_usage / total) * 100\n",
    "            st.metric(\"RAG Usage\", f\"{rag_percentage:.1f}%\")\n",
    "        else:\n",
    "            st.metric(\"RAG Usage\", \"0%\")\n",
    "    \n",
    "    # New IPFS metrics column\n",
    "    with col4:\n",
    "        st.subheader(\"IPFS Integration\")\n",
    "        if \"ipfs_metrics\" in metrics:\n",
    "            ipfs_metrics = metrics[\"ipfs_metrics\"]\n",
    "            st.metric(\"Files Discovered\", ipfs_metrics[\"audio_files_discovered\"])\n",
    "            st.metric(\"Files Processed\", ipfs_metrics[\"audio_files_processed\"])\n",
    "            \n",
    "            # Show search info if available\n",
    "            if ipfs_metrics[\"last_search_query\"]:\n",
    "                st.text(f\"Last search: {ipfs_metrics['last_search_query']}\")\n",
    "                st.text(f\"Results: {ipfs_metrics['last_search_results']}\")\n",
    "        else:\n",
    "            st.text(\"No IPFS metrics available\")\n",
    "    \n",
    "    # Time series chart of audio processing\n",
    "    st.subheader(\"Audio Processing History\")\n",
    "    if metrics[\"audio_metrics\"][\"history\"]:\n",
    "        audio_df = pd.DataFrame(metrics[\"audio_metrics\"][\"history\"])\n",
    "        if not audio_df.empty:\n",
    "            audio_df['timestamp'] = pd.to_datetime(audio_df['timestamp'])\n",
    "            fig = px.scatter(audio_df, x='timestamp', y='transcription_time', \n",
    "                             size='duration', color='segments',\n",
    "                             title=\"Audio Transcription Performance\")\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    # Query visualization\n",
    "    st.subheader(\"Recent User Queries\")\n",
    "    if metrics[\"rag_metrics\"][\"query_history\"]:\n",
    "        query_df = pd.DataFrame(metrics[\"rag_metrics\"][\"query_history\"])\n",
    "        if not query_df.empty:\n",
    "            query_df['timestamp'] = pd.to_datetime(query_df['timestamp'])\n",
    "            query_df['used_context_label'] = query_df['used_context'].map({{True: \"RAG\", False: \"Direct\"}})\n",
    "            \n",
    "            fig = px.scatter(query_df, x='timestamp', y='response_time',\n",
    "                            color='used_context_label', hover_data=['query'],\n",
    "                            title=\"Query Response Times\")\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "            \n",
    "            # Display recent queries in a table\n",
    "            st.dataframe(query_df[['timestamp', 'query', 'used_context_label', 'response_time']]\n",
    "                        .rename(columns={{'used_context_label': 'Method', 'response_time': 'Time (sec)'}}))\n",
    "    \n",
    "    st.text(f\"Last updated: {{time.strftime('%Y-%m-%d %H:%M:%S')}}\")\n",
    "    st.text(f\"Dashboard port: {dashboard_port}, Metrics API port: {metrics_port}\")\n",
    "    \n",
    "    # Auto-refresh every 5 seconds\n",
    "    time.sleep(5)\n",
    "    st.experimental_rerun()\n",
    "else:\n",
    "    st.warning(\"Failed to fetch metrics data. Make sure the metrics endpoint is running.\")\n",
    "    st.text(f\"Looking for metrics at: {{METRICS_API_URL}}\")\n",
    "    time.sleep(5)\n",
    "    st.experimental_rerun()\n",
    "\"\"\"\n",
    "    \n",
    "    # Write the Streamlit app to a file\n",
    "    with open(\"rag_dashboard.py\", \"w\") as f:\n",
    "        f.write(dashboard_code)\n",
    "    \n",
    "    # Create a simple Flask server to serve the metrics with dynamic port\n",
    "    flask_code = f\"\"\"\n",
    "from flask import Flask, jsonify\n",
    "import json\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/metrics')\n",
    "def get_metrics():\n",
    "    try:\n",
    "        with open('rag_metrics.json', 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        return jsonify(metrics)\n",
    "    except Exception as e:\n",
    "        return jsonify({{\"error\": str(e)}}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='127.0.0.1', port={metrics_port})\n",
    "\"\"\"\n",
    "    \n",
    "    with open(\"metrics_server.py\", \"w\") as f:\n",
    "        f.write(flask_code)\n",
    "    \n",
    "    # Export the current metrics to a JSON file\n",
    "    with open(\"rag_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics_tracker.export_metrics(), f)\n",
    "    \n",
    "    # Start the flask server in a separate thread\n",
    "    def run_flask():\n",
    "        os.system(\"python metrics_server.py\")\n",
    "    \n",
    "    flask_thread = threading.Thread(target=run_flask)\n",
    "    flask_thread.daemon = True\n",
    "    flask_thread.start()\n",
    "    \n",
    "    # Start the Streamlit dashboard with the selected port\n",
    "    print(\"Starting RAG pipeline dashboard...\")\n",
    "    os.system(f\"streamlit run rag_dashboard.py --server.port={dashboard_port}\")\n",
    "\n",
    "# Run the dashboard in a separate thread to avoid blocking the notebook\n",
    "dashboard_thread = threading.Thread(target=create_dashboard)\n",
    "dashboard_thread.daemon = True\n",
    "dashboard_thread.start()\n",
    "\n",
    "print(f\"RAG pipeline monitoring dashboard is starting...\")\n",
    "print(f\"Access the dashboard at http://localhost:{dashboard_port}\")\n",
    "print(f\"Metrics API running at http://localhost:{metrics_port}/metrics\")\n",
    "print(\"\\nExample of RAG metrics tracking:\")\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "# Example of tracking a query to demonstrate the logging\n",
    "def simulate_rag_query(query, use_context=True):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if use_context:\n",
    "        context = tracked_vector_search_tool(query)\n",
    "        # Simulate RAG processing with context\n",
    "        time.sleep(0.5)  # Simulate processing time\n",
    "    else:\n",
    "        # Simulate direct answer without context\n",
    "        time.sleep(0.2)  # Simulate faster processing\n",
    "    \n",
    "    response_time = time.time() - start_time\n",
    "    metrics_tracker.log_user_query(query, use_context, response_time)\n",
    "    \n",
    "    # Export updated metrics\n",
    "    with open(\"rag_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics_tracker.export_metrics(), f)\n",
    "    \n",
    "    return \"Response time: {:.2f}s, Context used: {}\".format(response_time, use_context)\n",
    "\n",
    "# Simulate a few queries with and without context\n",
    "print(simulate_rag_query(\"What are the main topics discussed in the audio?\", use_context=True))\n",
    "print(simulate_rag_query(\"Who is speaking in the audio?\", use_context=True))\n",
    "print(simulate_rag_query(\"What is the background noise?\", use_context=False))\n",
    "print(simulate_rag_query(\"When was this recording made?\", use_context=False))\n",
    "\n",
    "print(f\"\\nOpen the dashboard at http://localhost:{dashboard_port} to see these metrics visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Step 13: IPFS Audio Content Discovery and Processing\n",
    "\n",
    "This step implements functionality to discover and retrieve audio content stored on IPFS by examining metadata. This allows the AI to expand its knowledge base by incorporating audio content from decentralized storage systems, effectively giving it access to a wider range of audio sources beyond local files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IPFS Audio Content Discovery and Processing ---\n",
    "\n",
    "%pip install -q ipfshttpclient requests\n",
    "\n",
    "import ipfshttpclient\n",
    "import requests\n",
    "import json\n",
    "import tempfile\n",
    "import os\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "def search_ipfs_audio_content(gateway_url=\"https://ipfs.io/api/v0\", query=None, limit=10):\n",
    "    \"\"\"\n",
    "    Search for audio content on IPFS using metadata\n",
    "    \n",
    "    Args:\n",
    "        gateway_url: IPFS gateway URL\n",
    "        query: Optional search terms\n",
    "        limit: Maximum number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of dicts containing CIDs and metadata for audio files\n",
    "    \"\"\"\n",
    "    print(f\"Searching IPFS for audio content{f' related to {query}' if query else ''}...\")\n",
    "    audio_files = []\n",
    "    \n",
    "    try:\n",
    "        # Try to connect to a local IPFS daemon first\n",
    "        try:\n",
    "            client = ipfshttpclient.connect()\n",
    "            print(\"Connected to local IPFS daemon\")\n",
    "        except:\n",
    "            print(\"No local IPFS daemon found, using gateway API\")\n",
    "            client = None\n",
    "            \n",
    "        # Method 1: If we have a local client, use it to search the DHT\n",
    "        if client:\n",
    "            # Get list of pins if no query provided\n",
    "            if not query:\n",
    "                pins = client.pin.ls(type=\"recursive\")\n",
    "                for pin in pins[\"Keys\"]:\n",
    "                    try:\n",
    "                        # Get metadata for each pin\n",
    "                        metadata = client.object.get(pin)\n",
    "                        if is_audio_by_metadata(metadata):\n",
    "                            audio_files.append({\n",
    "                                \"cid\": pin,\n",
    "                                \"metadata\": metadata,\n",
    "                                \"source\": \"local_ipfs\"\n",
    "                            })\n",
    "                            if len(audio_files) >= limit:\n",
    "                                break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error examining pin {pin}: {e}\")\n",
    "            else:\n",
    "                # If query provided, search IPNS/IPFS links containing audio-related terms\n",
    "                # This is simplified as full DHT search requires more complex code\n",
    "                pass\n",
    "        \n",
    "        # Method 2: Use IPFS gateway API or a pinning service\n",
    "        # This is a simplified implementation that would need a specific gateway API supporting search\n",
    "        if query and len(audio_files) < limit:\n",
    "            search_url = f\"{gateway_url}/search?q={query}+audio+filetype:mp3+filetype:wav\"\n",
    "            try:\n",
    "                response = requests.get(search_url, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    results = response.json()\n",
    "                    for result in results:\n",
    "                        if is_audio_by_metadata(result.get(\"metadata\", {})):\n",
    "                            audio_files.append({\n",
    "                                \"cid\": result.get(\"cid\"),\n",
    "                                \"metadata\": result.get(\"metadata\", {}),\n",
    "                                \"source\": \"gateway\"\n",
    "                            })\n",
    "                            if len(audio_files) >= limit:\n",
    "                                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error searching gateway: {e}\")\n",
    "                \n",
    "        # Fallback: Use known audio CIDs for demonstration\n",
    "        if len(audio_files) == 0:\n",
    "            print(\"No audio files found via search, using example files for demonstration\")\n",
    "            # These are example CIDs - they would need to be replaced with actual audio CIDs\n",
    "            example_audio_cids = [\n",
    "                \"QmYwAPJzv5CZsnA625s3Xf2nemtYgPpHdWEz79ojWnPbdG/demo.mp3\",\n",
    "                \"QmZ4tDuvesekSs4qM5ZBKpXiZGun7S2CYtEZRB3DYXkjGx/audio_sample.wav\"\n",
    "            ]\n",
    "            audio_files = [{\"cid\": cid, \"metadata\": {\"type\": \"audio\"}, \"source\": \"example\"} for cid in example_audio_cids]\n",
    "            \n",
    "        print(f\"Found {len(audio_files)} audio files on IPFS\")\n",
    "        return audio_files\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error searching IPFS: {e}\")\n",
    "        return []\n",
    "\n",
    "def is_audio_by_metadata(metadata):\n",
    "    \"\"\"\n",
    "    Check if a file is an audio file based on its metadata\n",
    "    \n",
    "    Args:\n",
    "        metadata: File metadata dict\n",
    "        \n",
    "    Returns:\n",
    "        Boolean indicating if the file is likely an audio file\n",
    "    \"\"\"\n",
    "    # Check MIME type if available\n",
    "    mime_type = metadata.get(\"MimeType\", \"\")\n",
    "    if mime_type.startswith(\"audio/\"):\n",
    "        return True\n",
    "        \n",
    "    # Check file extension\n",
    "    name = metadata.get(\"Name\", \"\")\n",
    "    audio_extensions = ['.mp3', '.wav', '.ogg', '.flac', '.m4a', '.aac']\n",
    "    if any(name.lower().endswith(ext) for ext in audio_extensions):\n",
    "        return True\n",
    "        \n",
    "    # Check metadata tags\n",
    "    tags = metadata.get(\"Tags\", [])\n",
    "    audio_tags = [\"audio\", \"sound\", \"music\", \"recording\", \"voice\", \"speech\"]\n",
    "    if any(tag in audio_tags for tag in tags):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def retrieve_audio_from_ipfs(cid, gateway_url=\"https://ipfs.io/ipfs\"):\n",
    "    \"\"\"\n",
    "    Download audio file from IPFS\n",
    "    \n",
    "    Args:\n",
    "        cid: Content identifier for the IPFS file\n",
    "        gateway_url: IPFS gateway URL\n",
    "        \n",
    "    Returns:\n",
    "        Path to downloaded file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First try with a local IPFS client\n",
    "        try:\n",
    "            client = ipfshttpclient.connect()\n",
    "            print(f\"Retrieving {cid} using local IPFS node\")\n",
    "            \n",
    "            # Create temp file\n",
    "            fd, temp_path = tempfile.mkstemp(suffix=\".\" + cid.split(\".\")[-1] if \".\" in cid else \".mp3\")\n",
    "            os.close(fd)\n",
    "            \n",
    "            # Get the file from IPFS and save it\n",
    "            client.get(cid, target=temp_path)\n",
    "            return temp_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Local IPFS retrieval failed: {e}\")\n",
    "            \n",
    "            # Fall back to gateway\n",
    "            file_url = f\"{gateway_url}/{cid}\"\n",
    "            print(f\"Retrieving {file_url} using gateway\")\n",
    "            \n",
    "            response = requests.get(file_url, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                # Create temp file\n",
    "                fd, temp_path = tempfile.mkstemp(suffix=\".\" + cid.split(\".\")[-1] if \".\" in cid else \".mp3\")\n",
    "                with os.fdopen(fd, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=1024):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                return temp_path\n",
    "            else:\n",
    "                print(f\"Failed to retrieve file: {response.status_code}\")\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving from IPFS: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_ipfs_audio_content(query=None, limit=3):\n",
    "    \"\"\"\n",
    "    Main function to search for, retrieve, and process audio content from IPFS\n",
    "    \n",
    "    Args:\n",
    "        query: Optional search query\n",
    "        limit: Maximum number of files to process\n",
    "        \n",
    "    Returns:\n",
    "        List of processed documents\n",
    "    \"\"\"\n",
    "    print(f\"Searching for{''+(' '+query if query else '')} audio content on IPFS...\")\n",
    "    audio_files = search_ipfs_audio_content(query=query, limit=limit)\n",
    "    \n",
    "    processed_documents = []\n",
    "    \n",
    "    for i, audio_file in enumerate(audio_files):\n",
    "        print(f\"\\nProcessing IPFS audio file {i+1}/{len(audio_files)}: {audio_file['cid']}\")\n",
    "        \n",
    "        # Retrieve the audio file\n",
    "        local_path = retrieve_audio_from_ipfs(audio_file['cid'])\n",
    "        if not local_path:\n",
    "            print(\"Failed to retrieve audio file\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            print(f\"Downloaded to {local_path}, transcribing...\")\n",
    "            \n",
    "            # Transcribe using our existing Whisper model\n",
    "            result = model.transcribe(local_path)\n",
    "            text_content = result[\"text\"]\n",
    "            \n",
    "            # Create document\n",
    "            ipfs_doc = AudioDocument(text_content)\n",
    "            ipfs_doc.metadata = {\n",
    "                \"source\": \"ipfs\",\n",
    "                \"cid\": audio_file['cid'],\n",
    "                \"ipfs_metadata\": audio_file['metadata']\n",
    "            }\n",
    "            \n",
    "            # Add to our collection\n",
    "            processed_documents.append(ipfs_doc)\n",
    "            \n",
    "            # Process audio segments\n",
    "            segments = result[\"segments\"]\n",
    "            relevant_segments = find_relevant_audio_segments(query if query else \"audio\", \n",
    "                                                            segments, collection, embedding_model)\n",
    "            \n",
    "            # Save segments (optional)\n",
    "            if relevant_segments:\n",
    "                query_term = query if query else \"ipfs_audio\"\n",
    "                save_audio_segments(local_path, relevant_segments, \n",
    "                                  f\"ipfs_{sanitize_filename(query_term)}_{i}\")\n",
    "            \n",
    "            # Clean up the temporary file\n",
    "            try:\n",
    "                os.unlink(local_path)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing audio file: {e}\")\n",
    "            # Clean up on error\n",
    "            try:\n",
    "                os.unlink(local_path)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # If we found and processed any documents, add them to our RAG system\n",
    "    if processed_documents:\n",
    "        print(f\"\\nAdding {len(processed_documents)} IPFS audio documents to RAG system...\")\n",
    "        \n",
    "        # Split documents into chunks\n",
    "        ipfs_docs = text_splitter.split_documents(processed_documents)\n",
    "        \n",
    "        # Attach sound tags (we'll just use generic ones since we don't have the specific audio)\n",
    "        for doc in ipfs_docs:\n",
    "            doc.page_content = f\"Transcription from IPFS: {doc.page_content}\\nSource: IPFS CID {doc.metadata.get('cid', 'unknown')}\"\n",
    "        \n",
    "        # Create embeddings\n",
    "        doc_texts = [doc.page_content for doc in ipfs_docs]\n",
    "        document_embeddings = embedding_model.encode(doc_texts, convert_to_numpy=True)\n",
    "        \n",
    "        # Add to ChromaDB\n",
    "        start_idx = collection.count()\n",
    "        for i, embedding in enumerate(document_embeddings):\n",
    "            collection.add(\n",
    "                ids=[f\"ipfs_{start_idx + i}\"],\n",
    "                embeddings=[embedding.tolist()],\n",
    "                metadatas=[{\"text\": doc_texts[i], \"source\": \"ipfs\"}]\n",
    "            )\n",
    "        \n",
    "        print(f\"Successfully added {len(ipfs_docs)} IPFS audio document chunks to the RAG system\")\n",
    "        \n",
    "        # Update metrics\n",
    "        metrics_tracker.log_embedding_creation(len(document_embeddings))\n",
    "    else:\n",
    "        print(\"No IPFS audio documents were processed\")\n",
    "    \n",
    "    return processed_documents\n",
    "\n",
    "# Example usage - try to find audio related to music\n",
    "# Uncomment to execute\n",
    "# ipfs_docs = process_ipfs_audio_content(query=\"music\", limit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating IPFS Audio Content Integration\n",
    "\n",
    "Let's demonstrate how to use the IPFS audio discovery and integration functionality with a simple example. You can customize the search query to find specific types of audio content on IPFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Demo: Integrate IPFS Audio Content with RAG ---\n",
    "\n",
    "# Search for interview audio on IPFS and process it\n",
    "ipfs_docs = process_ipfs_audio_content(query=\"interview\", limit=1)\n",
    "\n",
    "# If we found any documents, test a query that might use this new knowledge\n",
    "if ipfs_docs:\n",
    "    print(\"\\n--- Testing RAG with IPFS content ---\")\n",
    "    test_query = \"What interviews are available on IPFS?\"\n",
    "    print(f\"Query: {test_query}\")\n",
    "    \n",
    "    # Get context from our RAG system (which now includes IPFS content)\n",
    "    context = vector_search_tool(test_query)\n",
    "    print(f\"Retrieved context:\\n{context}\\n\")\n",
    "    \n",
    "    # Generate a response using our LLM\n",
    "    prompt = f\"\"\"\n",
    "You are an AI assistant with knowledge about audio content.\n",
    "Based on the following context, please answer the question: {test_query}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    \n",
    "    response = llm(prompt, max_tokens=500)\n",
    "    print(f\"LLM Response:\\n{response['choices'][0]['text']}\")\n",
    "else:\n",
    "    print(\"\\nNo IPFS content was retrieved. You can try again with a different query.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Update Metrics Dashboard with IPFS Integration ---\n",
    "\n",
    "# Extend our RAGMetricsTracker class to include IPFS metrics\n",
    "class IPFSMetrics:\n",
    "    def __init__(self):\n",
    "        self.audio_files_discovered = 0\n",
    "        self.audio_files_processed = 0\n",
    "        self.retrieval_errors = 0\n",
    "        self.processing_errors = 0\n",
    "        self.last_search_query = \"\"\n",
    "        self.last_search_results = 0\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"audio_files_discovered\": self.audio_files_discovered,\n",
    "            \"audio_files_processed\": self.audio_files_processed,\n",
    "            \"retrieval_errors\": self.retrieval_errors,\n",
    "            \"processing_errors\": self.processing_errors,\n",
    "            \"last_search_query\": self.last_search_query,\n",
    "            \"last_search_results\": self.last_search_results\n",
    "        }\n",
    "\n",
    "# Add IPFS metrics to our tracker\n",
    "metrics_tracker.ipfs_metrics = IPFSMetrics()\n",
    "\n",
    "# Update the export_metrics method to include IPFS metrics\n",
    "original_export_metrics = metrics_tracker.export_metrics\n",
    "\n",
    "def extended_export_metrics():\n",
    "    metrics = original_export_metrics()\n",
    "    metrics[\"ipfs_metrics\"] = metrics_tracker.ipfs_metrics.to_dict()\n",
    "    return metrics\n",
    "\n",
    "# Replace the original method\n",
    "metrics_tracker.export_metrics = extended_export_metrics\n",
    "\n",
    "# Update the dashboard code to display IPFS metrics\n",
    "dashboard_code = \"\"\"\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "st.set_page_config(page_title=\"RAG Pipeline Monitor\", layout=\"wide\")\n",
    "\n",
    "st.title(\"📊 Agentic RAG Pipeline Monitoring\")\n",
    "\n",
    "# Create metrics columns\n",
    "col1, col2, col3, col4 = st.columns(4)\n",
    "\n",
    "# Initialize session state\n",
    "if 'last_update' not in st.session_state:\n",
    "    st.session_state.last_update = time.time()\n",
    "    st.session_state.metrics_history = []\n",
    "\n",
    "# Function to fetch metrics from the metrics endpoint\n",
    "def get_metrics():\n",
    "    try:\n",
    "        response = requests.get(METRICS_API_URL)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            st.error(f\"Failed to fetch metrics: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error fetching metrics: {e}\")\n",
    "        return None\n",
    "\n",
    "# Set the metrics API endpoint\n",
    "METRICS_API_URL = \"http://127.0.0.1:METRICS_PORT/metrics\"\n",
    "\n",
    "metrics = get_metrics()\n",
    "if metrics:\n",
    "    # Store metrics in history\n",
    "    st.session_state.metrics_history.append(metrics)\n",
    "    if len(st.session_state.metrics_history) > 20:\n",
    "        st.session_state.metrics_history.pop(0)\n",
    "    \n",
    "    # Display metrics in columns\n",
    "    with col1:\n",
    "        st.subheader(\"Audio Processing\")\n",
    "        st.metric(\"Files Processed\", metrics[\"audio_metrics\"][\"processed_files\"])\n",
    "        st.metric(\"Total Duration (sec)\", round(metrics[\"audio_metrics\"][\"total_duration\"], 2))\n",
    "        st.metric(\"Segments Extracted\", metrics[\"audio_metrics\"][\"segments_extracted\"])\n",
    "    \n",
    "    with col2:\n",
    "        st.subheader(\"ChromaDB Operations\")\n",
    "        st.metric(\"Total Queries\", metrics[\"chromadb_metrics\"][\"queries\"])\n",
    "        st.metric(\"Embeddings Created\", metrics[\"chromadb_metrics\"][\"embeddings_created\"])\n",
    "        st.metric(\"Avg Query Time (ms)\", round(metrics[\"chromadb_metrics\"][\"avg_query_time\"] * 1000, 2))\n",
    "    \n",
    "    with col3:\n",
    "        st.subheader(\"RAG Performance\")\n",
    "        st.metric(\"User Queries\", metrics[\"rag_metrics\"][\"total_queries\"])\n",
    "        rag_usage = metrics[\"rag_metrics\"][\"context_used\"]\n",
    "        direct_answers = metrics[\"rag_metrics\"][\"direct_answers\"]\n",
    "        total = rag_usage + direct_answers\n",
    "        \n",
    "        if total > 0:\n",
    "            rag_percentage = (rag_usage / total) * 100\n",
    "            st.metric(\"RAG Usage\", f\"{rag_percentage:.1f}%\")\n",
    "        else:\n",
    "            st.metric(\"RAG Usage\", \"0%\")\n",
    "    \n",
    "    # New IPFS metrics column\n",
    "    with col4:\n",
    "        st.subheader(\"IPFS Integration\")\n",
    "        if \"ipfs_metrics\" in metrics:\n",
    "            ipfs_metrics = metrics[\"ipfs_metrics\"]\n",
    "            st.metric(\"Files Discovered\", ipfs_metrics[\"audio_files_discovered\"])\n",
    "            st.metric(\"Files Processed\", ipfs_metrics[\"audio_files_processed\"])\n",
    "            \n",
    "            # Show search info if available\n",
    "            if ipfs_metrics[\"last_search_query\"]:\n",
    "                st.text(f\"Last search: {ipfs_metrics['last_search_query']}\")\n",
    "                st.text(f\"Results: {ipfs_metrics['last_search_results']}\")\n",
    "        else:\n",
    "            st.text(\"No IPFS metrics available\")\n",
    "    \n",
    "    # Time series chart of audio processing\n",
    "    st.subheader(\"Audio Processing History\")\n",
    "    if metrics[\"audio_metrics\"][\"history\"]:\n",
    "        audio_df = pd.DataFrame(metrics[\"audio_metrics\"][\"history\"])\n",
    "        if not audio_df.empty:\n",
    "            audio_df['timestamp'] = pd.to_datetime(audio_df['timestamp'])\n",
    "            fig = px.scatter(audio_df, x='timestamp', y='transcription_time', \n",
    "                             size='duration', color='segments',\n",
    "                             title=\"Audio Transcription Performance\")\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    # Query visualization\n",
    "    st.subheader(\"Recent User Queries\")\n",
    "    if metrics[\"rag_metrics\"][\"query_history\"]:\n",
    "        query_df = pd.DataFrame(metrics[\"rag_metrics\"][\"query_history\"])\n",
    "        if not query_df.empty:\n",
    "            query_df['timestamp'] = pd.to_datetime(query_df['timestamp'])\n",
    "            query_df['used_context_label'] = query_df['used_context'].map({True: \"RAG\", False: \"Direct\"})\n",
    "            \n",
    "            fig = px.scatter(query_df, x='timestamp', y='response_time',\n",
    "                            color='used_context_label', hover_data=['query'],\n",
    "                            title=\"Query Response Times\")\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "            \n",
    "            # Display recent queries in a table\n",
    "            st.dataframe(query_df[['timestamp', 'query', 'used_context_label', 'response_time']]\n",
    "                        .rename(columns={'used_context_label': 'Method', 'response_time': 'Time (sec)'}))\n",
    "    \n",
    "    st.text(f\"Last updated: {{time.strftime('%Y-%m-%d %H:%M:%S')}}\")\n",
    "    st.text(f\"Dashboard port: {dashboard_port}, Metrics API port: {metrics_port}\")\n",
    "    \n",
    "    # Auto-refresh every 5 seconds\n",
    "    time.sleep(5)\n",
    "    st.experimental_rerun()\n",
    "else:\n",
    "    st.warning(\"Failed to fetch metrics data. Make sure the metrics endpoint is running.\")\n",
    "    st.text(f\"Looking for metrics at: {{METRICS_API_URL}}\")\n",
    "    time.sleep(5)\n",
    "    st.experimental_rerun()\n",
    "\"\"\"\n",
    "\n",
    "# Write the updated Streamlit app to a new file\n",
    "with open(\"rag_dashboard_with_ipfs.py\", \"w\") as f:\n",
    "    f.write(dashboard_code)\n",
    "\n",
    "print(\"Updated dashboard code with IPFS metrics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
