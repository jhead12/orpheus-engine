{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Retrieval-Augmented Generation (RAG) with Local Llama 2 & ChromaDB\n",
    "\n",
    "## Overview\n",
    "This notebook implements an **Agentic Retrieval-Augmented Generation (RAG) pipeline**. It focuses on transcribing audio data, potentially from an Omi streaming device, storing both the transcription and audio, and then using the transcription with a local **Ai Studio** model and **ChromaDB** for intelligent question-answering. The system determines whether additional context is needed before generating responses.\n",
    "\n",
    "### Key Features:\n",
    "- **Audio Transcription Workflow** for processing data from devices like Omi.\n",
    "- **Storage of Audio and Transcriptions** for AI processing.\n",
    "- **Llama 2 Model** for high-quality text generation.\n",
    "- **ChromaDB Vector Store** for efficient semantic search on transcriptions.\n",
    "- **Dynamic Context Retrieval** to improve answer accuracy.\n",
    "- **Two Answering Modes**:\n",
    "  - With RAG (Retrieves relevant document content before responding).\n",
    "  - Without RAG (Directly generates responses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force Python to use the latest system sqlite3 (if available)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Unset any pysqlite3 monkeypatching\n",
    "if \"pysqlite3\" in sys.modules:\n",
    "    del sys.modules[\"pysqlite3\"]\n",
    "if \"sqlite3\" in sys.modules:\n",
    "    del sys.modules[\"sqlite3\"]\n",
    "\n",
    "try:\n",
    "    import pysqlite3\n",
    "    sys.modules[\"sqlite3\"] = pysqlite3\n",
    "    sys.modules[\"pysqlite3\"] = pysqlite3\n",
    "    print(\"Using pysqlite3 as sqlite3 backend.\")\n",
    "except ImportError:\n",
    "    print(\"pysqlite3 not found, using default sqlite3 module.\")\n",
    "\n",
    "import sqlite3\n",
    "print(\"Loaded sqlite3 version:\", sqlite3.sqlite_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if you have not installed them already\n",
    "%pip install -r requirements.txt --verbose --quiet\n",
    "%pip install -q --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Step 1: Model Setup\n",
    "\n",
    "We will set up **Llama 2 (7B)** for text generation. If the model is not found locally, it will be downloaded from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q huggingface-hub\n",
    "\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "MODEL_FILENAME = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "MODEL_DIR = \"model\"\n",
    "EXPECTED_PATH = os.path.join(MODEL_DIR, MODEL_FILENAME)\n",
    "\n",
    "# Ensure model directory exists\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Check if model already exists\n",
    "if os.path.exists(EXPECTED_PATH):\n",
    "    print(f\"Model already exists at: {EXPECTED_PATH}\")\n",
    "    model_path = EXPECTED_PATH\n",
    "else:\n",
    "    print(\"Model not found locally. Downloading Llama 2 model...\")\n",
    "    \n",
    "    # Download the model\n",
    "    model_path = hf_hub_download(\n",
    "        repo_id=\"TheBloke/Llama-2-7B-Chat-GGUF\",\n",
    "        filename=MODEL_FILENAME,\n",
    "        local_dir=MODEL_DIR\n",
    "    )\n",
    "    print(f\"Model downloaded to: {model_path}\")\n",
    "\n",
    "print(f\"Using model at: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q llama-cpp-python\n",
    "# Check if the model file exists\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "# Import the Llama class from llama_cpp\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Initialize the model with the local path and GPU acceleration\n",
    "llm = Llama(\n",
    "    model_path=EXPECTED_PATH,\n",
    "    temperature=0.25,\n",
    "    max_tokens=2000,\n",
    "    n_ctx=4096,\n",
    "    top_p=1.0,\n",
    "    verbose=False,\n",
    "    n_gpu_layers=30,  # Utilize some available GPU layers\n",
    "    n_batch=512,      # Optimize batch size for parallel processing\n",
    "    f16_kv=True,      # Enable half-precision for key/value cache\n",
    "    use_mlock=True,   # Lock memory to prevent swapping\n",
    "    use_mmap=True     # Utilize memory mapping for faster loading\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📄 Step 2: Loading, Transcribing, and Storing Audio Data\n",
    "\n",
    "This step outlines the process for loading audio data (e.g., from an Omi streaming device), transcribing it, and preparing it for storage and further processing. Both the raw audio and its transcription are valuable assets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load the Audio File Document ---\n",
    "# --- Load the Audio File Document and Audio Collection System ---\n",
    "\n",
    "# Install whisper if not already installed\n",
    "%pip install -q openai-whisper\n",
    "\n",
    "# Install ffmpeg-python bindings if not already installed\n",
    "%pip install -q ffmpeg-python\n",
    "\n",
    "import shutil\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Check if ffmpeg is available and working\n",
    "try:\n",
    "    subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, check=True)\n",
    "    print(\"ffmpeg found and working.\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"ffmpeg not found. Please install ffmpeg and ensure it's in your PATH.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error running ffmpeg: {{e}}\")\n",
    "    raise RuntimeError(\"ffmpeg is not working correctly.\") from e\n",
    "\n",
    "import whisper\n",
    "\n",
    "# No need to manually set ffmpeg_dir if ffmpeg is installed system-wide\n",
    "\n",
    "# Define the Audio File file path\n",
    "AUDIO_PATH = \"./data/tester.mp3\"\n",
    "# Check if the file exists\n",
    "print(f\"Loading AUDIO from: {AUDIO_PATH}\")\n",
    "\n",
    "# Define the audio collection system\n",
    "AUDIO_COLLECTION_SYSTEM = \"MyAudioSystem\"\n",
    "\n",
    "# Load and transcribe the audio file using whisper\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(AUDIO_PATH)\n",
    "text_content = result[\"text\"]\n",
    "\n",
    "# For compatibility with the rest of your code, wrap the text in a document-like object\n",
    "class AudioDocument:\n",
    "    def __init__(self, text):\n",
    "        self.page_content = text\n",
    "        self.metadata = {}  # Optionally add metadata if needed\n",
    "    def getPageText(self):\n",
    "        return self.page_content\n",
    "\n",
    "documents = [AudioDocument(text_content)]\n",
    "\n",
    "print(f\"Successfully loaded {len(documents)} document(s) from the AUDIO.\")\n",
    "# Initialize an empty list for the audio files\n",
    "audio_files = []\n",
    "\n",
    "# Iterate through each document\n",
    "for document in documents:\n",
    "    # Get the current page's text content\n",
    "    text_content = document.getPageText()\n",
    "\n",
    "    # Extract relevant information from the text, e.g., keywords or phrases\n",
    "    def extractRelevantInfo(text):\n",
    "        # Placeholder: just return the text itself\n",
    "        return text\n",
    "    extracted_info = extractRelevantInfo(text_content)\n",
    "\n",
    "    # Define a placeholder for createAudioFile\n",
    "    def createAudioFile(system, info):\n",
    "        # Placeholder: just return a tuple for demonstration\n",
    "        return (system, info)\n",
    "\n",
    "    # Create an audio file based on the extracted information\n",
    "    audio_file = createAudioFile(AUDIO_COLLECTION_SYSTEM, extracted_info)\n",
    "\n",
    "    # Add the audio file to the collection system's list\n",
    "    audio_files.append(audio_file)\n",
    "print(f\"Successfully loaded {len(documents)} document(s) from the AUDIO and created {len(audio_files)} audio file(s).\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Audio Event Detection (Sound Tagging) ---\n",
    "\n",
    "%pip install -q torchaudio\n",
    "%pip install -q panns-inference\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from panns_inference import AudioTagging\n",
    "\n",
    "# Initialize the PANNs audio tagging model\n",
    "panns_model = AudioTagging(checkpoint_path=None, device='cpu')  # Uses default Cnn14 weights\n",
    "\n",
    "# Load and preprocess audio\n",
    "waveform, sr = torchaudio.load(AUDIO_PATH)\n",
    "if sr != 32000:\n",
    "    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=32000)(waveform)\n",
    "    sr = 32000\n",
    "\n",
    "# PANNs expects mono audio\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "\n",
    "# --- Load the full AudioSet class labels for PANNs output mapping ---\n",
    "# Download the official label file from: https://github.com/qiuqiangkong/panns-inference/blob/master/panns_inference/labels/audioset_labels.txt\n",
    "# If you cannot download, here is a fallback: print the indices instead of labels.\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "LABELS_CSV_PATH = \"class_labels_indices.csv\"\n",
    "labels = None\n",
    "if os.path.exists(LABELS_CSV_PATH):\n",
    "    with open(LABELS_CSV_PATH, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        labels = [row['display_name'] for row in reader]\n",
    "else:\n",
    "    print(\"WARNING: AudioSet class label CSV not found. Will print indices instead of class names.\")\n",
    "    labels = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = panns_model.inference(waveform)\n",
    "    clipwise_output = output[0]\n",
    "    topk = torch.topk(torch.tensor(clipwise_output[0]), 3)\n",
    "    if labels and max(topk.indices.tolist()) < len(labels):\n",
    "        sound_tags = [labels[i] for i in topk.indices.tolist()]\n",
    "    else:\n",
    "        sound_tags = [f\"Class index {i}\" for i in topk.indices.tolist()]\n",
    "\n",
    "print(\"Detected sound types:\", sound_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✂️ Step 3: Chunking Audio Transcriptions for RAG\n",
    "\n",
    "The transcribed text from the audio data is split into **small overlapping chunks** (approximately **500 characters**). These chunks are then used for embedding and storage in ChromaDB to enable semantic search for the RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Split the Audio Content into Manageable Chunks with Sound Tags ---\n",
    "%pip install -q langchain\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "text_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "\n",
    "# Split the transcription into chunks\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Attach sound tags to each chunk as metadata\n",
    "for doc in docs:\n",
    "    doc.page_content = f\"Transcription: {doc.page_content}\\nSound tags: {', '.join(sound_tags)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Step 4: Initializing the Embedding Model\n",
    "\n",
    "To convert text into numerical representations for efficient similarity search, we use **all-MiniLM-L6-v2** from `sentence-transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize the Embedding Model ---\n",
    "%pip install -q sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Define the embedding model name\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "print(f\"Successfully loaded embedding model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Step 5: Computing Embeddings for Document Chunks\n",
    "\n",
    "Each chunk is converted into a **vector representation** using our embedding model. This allows us to perform **semantic similarity searches** later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute Embeddings for Each Text Chunk ---\n",
    "\n",
    "# Extract text content from each chunk\n",
    "doc_texts = [doc.page_content for doc in docs]\n",
    "\n",
    "# Compute embeddings for the extracted text chunks\n",
    "document_embeddings = embedding_model.encode(doc_texts, convert_to_numpy=True)\n",
    "\n",
    "# Display the result\n",
    "print(\"Successfully computed embeddings for each text chunk.\")\n",
    "print(f\"Embeddings Shape: {document_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🗄️ Step 6: Storing Audio Transcription Embeddings in ChromaDB\n",
    "\n",
    "We initialize **ChromaDB**, a high-performance **vector database**, and store our computed embeddings to enable efficient retrieval of relevant text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize and Populate the Chroma Vector Database ---\n",
    "\n",
    "# Define Chroma database path and collection name\n",
    "CHROMA_DB_PATH = \"./chroma_db\"\n",
    "COLLECTION_NAME = \"document_embeddings\"\n",
    "\n",
    "# Initialize Chroma client\n",
    "import chromadb\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)\n",
    "\n",
    "# Add document embeddings to the Chroma collection\n",
    "for i, embedding in enumerate(document_embeddings):\n",
    "    collection.add(\n",
    "        ids=[str(i)],  # Chroma requires string IDs\n",
    "        embeddings=[embedding.tolist()],\n",
    "        metadatas=[{\"text\": doc_texts[i]}]\n",
    "    )\n",
    "\n",
    "print(\"Successfully populated Chroma database with document embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔎 Step 7: Implementing Vector Search Tool\n",
    "\n",
    "To retrieve relevant text passages from the database, we define a **vector search function** that finds the most relevant chunks based on a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Searches the Chroma database for relevant text chunks based on the query.\n",
    "    Computes the query embedding, retrieves the top 5 most relevant text chunks,\n",
    "    and returns them as a formatted string.\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.encode(query, convert_to_numpy=True).tolist()\n",
    "    TOP_K = 5\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=TOP_K\n",
    "    )\n",
    "    retrieved_chunks = [metadata[\"text\"] for metadata in results[\"metadatas\"][0]]\n",
    "    return \"\\n\\n\".join(retrieved_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Step 8: Context Need Assessment\n",
    "\n",
    "Instead of always retrieving context, we determine if the query **requires external document context** before generating a response. This creates an agentic workflow that makes autonomous decisions to complete the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_audio_segments(query: str):\n",
    "    # Retrieve top relevant chunks\n",
    "    results = collection.query(\n",
    "        query_embeddings=[embedding_model.encode(query, convert_to_numpy=True).tolist()],\n",
    "        n_results=5\n",
    "    )\n",
    "    # Get the matched text chunks\n",
    "    matched_texts = [metadata[\"text\"] for metadata in results[\"metadatas\"][0]]\n",
    "\n",
    "    # If you have segment info, match text to segment\n",
    "    relevant_segments = []\n",
    "    for segment in result[\"segments\"]:\n",
    "        for text in matched_texts:\n",
    "            if segment[\"text\"].strip() in text:\n",
    "                relevant_segments.append(segment)\n",
    "    return relevant_segments\n",
    "\n",
    "# Example usage:\n",
    "relevant_segments = find_relevant_audio_segments(\"vocal techniques\")\n",
    "for seg in relevant_segments:\n",
    "    print(f\"Start: {seg['start']}s, End: {seg['end']}s, Text: {seg['text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 Step 9: Answer Generation with Agentic RAG\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
