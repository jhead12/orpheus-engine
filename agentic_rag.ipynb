{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Retrieval-Augmented Generation (RAG) with Local Llama 2 & ChromaDB\n",
    "\n",
    "## Overview\n",
    "This notebook implements an **Agentic Retrieval-Augmented Generation (RAG) pipeline**. It focuses on transcribing audio data, potentially from an Omi streaming device, storing both the transcription and audio, and then using the transcription with a local **Ai Studio** model and **ChromaDB** for intelligent question-answering. The system determines whether additional context is needed before generating responses.\n",
    "\n",
    "### Key Features:\n",
    "- **Audio Transcription Workflow** for processing data from devices like Omi.\n",
    "- **Storage of Audio and Transcriptions** for AI processing.\n",
    "- **Llama 2 Model** for high-quality text generation.\n",
    "- **ChromaDB Vector Store** for efficient semantic search on transcriptions.\n",
    "- **Dynamic Context Retrieval** to improve answer accuracy.\n",
    "- **Two Answering Modes**:\n",
    "  - With RAG (Retrieves relevant document content before responding).\n",
    "  - Without RAG (Directly generates responses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pysqlite3 as sqlite3 backend.\n",
      "Loaded sqlite3 version: 3.46.1\n"
     ]
    }
   ],
   "source": [
    "# Force Python to use the latest system sqlite3 (if available)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Unset any pysqlite3 monkeypatching\n",
    "if \"pysqlite3\" in sys.modules:\n",
    "    del sys.modules[\"pysqlite3\"]\n",
    "if \"sqlite3\" in sys.modules:\n",
    "    del sys.modules[\"sqlite3\"]\n",
    "\n",
    "try:\n",
    "    import pysqlite3\n",
    "    sys.modules[\"sqlite3\"] = pysqlite3\n",
    "    sys.modules[\"pysqlite3\"] = pysqlite3\n",
    "    print(\"Using pysqlite3 as sqlite3 backend.\")\n",
    "except ImportError:\n",
    "    print(\"pysqlite3 not found, using default sqlite3 module.\")\n",
    "\n",
    "import sqlite3\n",
    "print(\"Loaded sqlite3 version:\", sqlite3.sqlite_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(38731) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub>=0.19.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (0.31.4)\n",
      "Requirement already satisfied: llama-cpp-python>=0.2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (0.3.9)\n",
      "Collecting pysqlite3>=0.5.0 (from -r requirements.txt (line 6))\n",
      "  Downloading pysqlite3-0.5.4.tar.gz (40 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: openai-whisper>=20231117 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (20240930)\n",
      "Requirement already satisfied: ffmpeg-python>=0.2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (0.2.0)\n",
      "Requirement already satisfied: torchaudio>=2.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (2.7.0)\n",
      "Requirement already satisfied: panns-inference==0.1.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (0.1.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (2.7.0)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (3.9.2)\n",
      "Requirement already satisfied: langchain>=0.0.235 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 17)) (0.3.7)\n",
      "Collecting sentence-transformers==2.2.2 (from -r requirements.txt (line 18))\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: chromadb>=0.4.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 19)) (0.6.2)\n",
      "Requirement already satisfied: Flask>=2.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 22)) (3.1.0)\n",
      "Requirement already satisfied: uvicorn>=0.17.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 23)) (0.30.6)\n",
      "Requirement already satisfied: fastapi>=0.95.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 24)) (0.111.0)\n",
      "Requirement already satisfied: requests>=2.28.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 25)) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 26)) (1.0.1)\n",
      "Requirement already satisfied: IPython>=8.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 29)) (8.27.0)\n",
      "Requirement already satisfied: streamlit>=1.28.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 30)) (1.43.2)\n",
      "Requirement already satisfied: plotly>=5.15.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 31)) (5.24.1)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 32)) (2.2.3)\n",
      "Collecting ipfshttpclient==0.7.0 (from -r requirements.txt (line 35))\n",
      "  Downloading ipfshttpclient-0.7.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: boto3>=1.28.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 36)) (1.35.53)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 39)) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 40)) (1.13.1)\n",
      "Requirement already satisfied: librosa in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from panns-inference==0.1.1->-r requirements.txt (line 12)) (0.11.0)\n",
      "Requirement already satisfied: torchlibrosa in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from panns-inference==0.1.1->-r requirements.txt (line 12)) (0.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 18)) (4.52.2)\n",
      "Requirement already satisfied: tqdm in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 18)) (4.67.1)\n",
      "Collecting torchvision (from sentence-transformers==2.2.2->-r requirements.txt (line 18))\n",
      "  Downloading torchvision-0.22.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: scikit-learn in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 18)) (1.5.1)\n",
      "Requirement already satisfied: nltk in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 18)) (3.9.1)\n",
      "Requirement already satisfied: sentencepiece in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 18)) (0.2.0)\n",
      "Collecting multiaddr>=0.0.7 (from ipfshttpclient==0.7.0->-r requirements.txt (line 35))\n",
      "  Downloading multiaddr-0.0.9-py2.py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: filelock in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 18)) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 18)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 18)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 18)) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 18)) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 18)) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.19.0->-r requirements.txt (line 4)) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.19.0->-r requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from llama-cpp-python>=0.2.0->-r requirements.txt (line 5)) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from llama-cpp-python>=0.2.0->-r requirements.txt (line 5)) (3.1.6)\n",
      "Requirement already satisfied: numba in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from openai-whisper>=20231117->-r requirements.txt (line 9)) (0.60.0)\n",
      "Requirement already satisfied: more-itertools in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from openai-whisper>=20231117->-r requirements.txt (line 9)) (10.3.0)\n",
      "Requirement already satisfied: tiktoken in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from openai-whisper>=20231117->-r requirements.txt (line 9)) (0.9.0)\n",
      "Requirement already satisfied: future in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from ffmpeg-python>=0.2.0->-r requirements.txt (line 10)) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 13)) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 13)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 13)) (3.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 14)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 14)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 14)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 14)) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 14)) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 14)) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 14)) (2.9.0.post0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langchain>=0.0.235->-r requirements.txt (line 17)) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langchain>=0.0.235->-r requirements.txt (line 17)) (3.11.8)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langchain>=0.0.235->-r requirements.txt (line 17)) (0.3.60)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langchain>=0.0.235->-r requirements.txt (line 17)) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langchain>=0.0.235->-r requirements.txt (line 17)) (0.1.147)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langchain>=0.0.235->-r requirements.txt (line 17)) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langchain>=0.0.235->-r requirements.txt (line 17)) (9.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from requests>=2.28.0->-r requirements.txt (line 25)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from requests>=2.28.0->-r requirements.txt (line 25)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from requests>=2.28.0->-r requirements.txt (line 25)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from requests>=2.28.0->-r requirements.txt (line 25)) (2025.1.31)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.235->-r requirements.txt (line 17)) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.235->-r requirements.txt (line 17)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.235->-r requirements.txt (line 17)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.235->-r requirements.txt (line 17)) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.235->-r requirements.txt (line 17)) (6.0.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.235->-r requirements.txt (line 17)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.235->-r requirements.txt (line 17)) (1.20.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain>=0.0.235->-r requirements.txt (line 17)) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain>=0.0.235->-r requirements.txt (line 17)) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain>=0.0.235->-r requirements.txt (line 17)) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain>=0.0.235->-r requirements.txt (line 17)) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain>=0.0.235->-r requirements.txt (line 17)) (1.0.0)\n",
      "Requirement already satisfied: anyio in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain>=0.0.235->-r requirements.txt (line 17)) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain>=0.0.235->-r requirements.txt (line 17)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain>=0.0.235->-r requirements.txt (line 17)) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.0.235->-r requirements.txt (line 17)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.0.235->-r requirements.txt (line 17)) (2.23.4)\n",
      "Requirement already satisfied: build>=1.0.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 19)) (1.2.2.post1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 19)) (0.7.6)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 19)) (4.0.1)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 19)) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 19)) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 19)) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 19)) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 19)) (1.27.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 19)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 19)) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 19)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 19)) (1.67.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 19)) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 19)) (0.15.4)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 19)) (32.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 19)) (5.1.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 19)) (13.9.4)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from Flask>=2.0.0->-r requirements.txt (line 22)) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from Flask>=2.0.0->-r requirements.txt (line 22)) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from Flask>=2.0.0->-r requirements.txt (line 22)) (8.1.8)\n",
      "Requirement already satisfied: blinker>=1.9 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from Flask>=2.0.0->-r requirements.txt (line 22)) (1.9.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fastapi>=0.95.0->-r requirements.txt (line 24)) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fastapi>=0.95.0->-r requirements.txt (line 24)) (0.0.7)\n",
      "Requirement already satisfied: python-multipart>=0.0.7 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fastapi>=0.95.0->-r requirements.txt (line 24)) (0.0.18)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fastapi>=0.95.0->-r requirements.txt (line 24)) (5.10.0)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fastapi>=0.95.0->-r requirements.txt (line 24)) (2.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain>=0.0.235->-r requirements.txt (line 17)) (1.3.1)\n",
      "Requirement already satisfied: decorator in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from IPython>=8.0.0->-r requirements.txt (line 29)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from IPython>=8.0.0->-r requirements.txt (line 29)) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from IPython>=8.0.0->-r requirements.txt (line 29)) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from IPython>=8.0.0->-r requirements.txt (line 29)) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from IPython>=8.0.0->-r requirements.txt (line 29)) (2.19.1)\n",
      "Requirement already satisfied: stack-data in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from IPython>=8.0.0->-r requirements.txt (line 29)) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from IPython>=8.0.0->-r requirements.txt (line 29)) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from IPython>=8.0.0->-r requirements.txt (line 29)) (4.8.0)\n",
      "Requirement already satisfied: wcwidth in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython>=8.0.0->-r requirements.txt (line 29)) (0.2.13)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from streamlit>=1.28.0->-r requirements.txt (line 30)) (5.5.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from streamlit>=1.28.0->-r requirements.txt (line 30)) (5.5.2)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from streamlit>=1.28.0->-r requirements.txt (line 30)) (4.25.8)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from streamlit>=1.28.0->-r requirements.txt (line 30)) (19.0.1)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from streamlit>=1.28.0->-r requirements.txt (line 30)) (0.10.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from streamlit>=1.28.0->-r requirements.txt (line 30)) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from streamlit>=1.28.0->-r requirements.txt (line 30)) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from streamlit>=1.28.0->-r requirements.txt (line 30)) (6.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from pandas>=2.0.0->-r requirements.txt (line 32)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from pandas>=2.0.0->-r requirements.txt (line 32)) (2025.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit>=1.28.0->-r requirements.txt (line 30)) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit>=1.28.0->-r requirements.txt (line 30)) (1.32.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.28.0->-r requirements.txt (line 30)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.28.0->-r requirements.txt (line 30)) (5.0.2)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.53 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from boto3>=1.28.0->-r requirements.txt (line 36)) (1.35.99)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from boto3>=1.28.0->-r requirements.txt (line 36)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from boto3>=1.28.0->-r requirements.txt (line 36)) (0.10.4)\n",
      "Requirement already satisfied: six>=1.5 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->-r requirements.txt (line 14)) (1.17.0)\n",
      "Requirement already satisfied: pyproject_hooks in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from build>=1.0.3->chromadb>=0.4.0->-r requirements.txt (line 19)) (1.2.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from email_validator>=2.0.0->fastapi>=0.95.0->-r requirements.txt (line 24)) (2.7.0)\n",
      "Requirement already satisfied: rich-toolkit>=0.11.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from fastapi-cli>=0.0.2->fastapi>=0.95.0->-r requirements.txt (line 24)) (0.14.7)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->IPython>=8.0.0->-r requirements.txt (line 29)) (0.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jinja2>=2.11.3->llama-cpp-python>=0.2.0->-r requirements.txt (line 5)) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.28.0->-r requirements.txt (line 30)) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.28.0->-r requirements.txt (line 30)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.28.0->-r requirements.txt (line 30)) (0.23.1)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 19)) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 19)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 19)) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 19)) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 19)) (0.10)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 19)) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 19)) (4.9)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 19)) (0.6.1)\n",
      "Collecting varint (from multiaddr>=0.0.7->ipfshttpclient==0.7.0->-r requirements.txt (line 35))\n",
      "  Downloading varint-1.0.2.tar.gz (1.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting base58 (from multiaddr>=0.0.7->ipfshttpclient==0.7.0->-r requirements.txt (line 35))\n",
      "  Downloading base58-2.1.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting netaddr (from multiaddr>=0.0.7->ipfshttpclient==0.7.0->-r requirements.txt (line 35))\n",
      "  Downloading netaddr-1.3.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: coloredlogs in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r requirements.txt (line 19)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r requirements.txt (line 19)) (25.2.10)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 19)) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 19)) (8.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 19)) (3.21.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 19)) (1.17.2)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 19)) (1.63.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 19)) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.27.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 19)) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-sdk>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 19)) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.48b0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.4.0->-r requirements.txt (line 19)) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.48b0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.4.0->-r requirements.txt (line 19)) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.48b0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.4.0->-r requirements.txt (line 19)) (0.48b0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.4.0->-r requirements.txt (line 19)) (3.8.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->IPython>=8.0.0->-r requirements.txt (line 29)) (0.7.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb>=0.4.0->-r requirements.txt (line 19)) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb>=0.4.0->-r requirements.txt (line 19)) (1.9.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->chromadb>=0.4.0->-r requirements.txt (line 19)) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.4.0->-r requirements.txt (line 19)) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->-r requirements.txt (line 13)) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from typer>=0.9.0->chromadb>=0.4.0->-r requirements.txt (line 19)) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 19)) (0.6.4)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 19)) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 19)) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 19)) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.4.0->-r requirements.txt (line 19)) (10.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from librosa->panns-inference==0.1.1->-r requirements.txt (line 12)) (3.0.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from librosa->panns-inference==0.1.1->-r requirements.txt (line 12)) (1.4.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from librosa->panns-inference==0.1.1->-r requirements.txt (line 12)) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from librosa->panns-inference==0.1.1->-r requirements.txt (line 12)) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from librosa->panns-inference==0.1.1->-r requirements.txt (line 12)) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from librosa->panns-inference==0.1.1->-r requirements.txt (line 12)) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from librosa->panns-inference==0.1.1->-r requirements.txt (line 12)) (1.0.3)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from numba->openai-whisper>=20231117->-r requirements.txt (line 9)) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from pooch>=1.1->librosa->panns-inference==0.1.1->-r requirements.txt (line 12)) (3.10.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers==2.2.2->-r requirements.txt (line 18)) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from soundfile>=0.12.1->librosa->panns-inference==0.1.1->-r requirements.txt (line 12)) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->panns-inference==0.1.1->-r requirements.txt (line 12)) (2.21)\n",
      "Requirement already satisfied: executing in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from stack-data->IPython>=8.0.0->-r requirements.txt (line 29)) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from stack-data->IPython>=8.0.0->-r requirements.txt (line 29)) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages (from stack-data->IPython>=8.0.0->-r requirements.txt (line 29)) (0.2.2)\n",
      "Downloading ipfshttpclient-0.7.0-py3-none-any.whl (82 kB)\n",
      "Downloading multiaddr-0.0.9-py2.py3-none-any.whl (16 kB)\n",
      "Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
      "Downloading netaddr-1.3.0-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.22.0-cp312-cp312-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sentence-transformers, pysqlite3, varint\n",
      "\u001b[33m  DEPRECATION: Building 'sentence-transformers' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'sentence-transformers'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=7542cebf2adb1f30de7f92b1971b8844ae5f677647ab37e4b63a3fe2843391c8\n",
      "  Stored in directory: /Users/jeldonmusic/Library/Caches/pip/wheels/d9/3b/21/aa025e9c81a6cda4b8358756a756677b0969b4bc69be6dd5da\n",
      "\u001b[33m  DEPRECATION: Building 'pysqlite3' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pysqlite3'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for pysqlite3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pysqlite3: filename=pysqlite3-0.5.4-cp312-cp312-macosx_11_0_arm64.whl size=36910 sha256=6ed104b82d642eee63eaf5f763b1deeb0a739c863176de8f5b95b6c8f4956f42\n",
      "  Stored in directory: /Users/jeldonmusic/Library/Caches/pip/wheels/5f/02/a6/284262355044b549376150fa1166bbd8106585da0c15280db3\n",
      "\u001b[33m  DEPRECATION: Building 'varint' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'varint'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for varint (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for varint: filename=varint-1.0.2-py3-none-any.whl size=1962 sha256=d988821f9f8fdce26a81290bcc89e2eec697aab55ccc4f7e9106079417258759\n",
      "  Stored in directory: /Users/jeldonmusic/Library/Caches/pip/wheels/9c/81/f4/3d93402a47e78b0e782062a5819b07b424427af0c4ed6655ab\n",
      "Successfully built sentence-transformers pysqlite3 varint\n",
      "Installing collected packages: varint, pysqlite3, netaddr, base58, multiaddr, torchvision, ipfshttpclient, sentence-transformers\n",
      "\u001b[2K  Attempting uninstall: sentence-transformers0mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6/8\u001b[0m [ipfshttpclient]\n",
      "\u001b[2K    Found existing installation: sentence-transformers 3.3.10mâ”â”â”â”\u001b[0m \u001b[32m7/8\u001b[0m [sentence-transformers]\n",
      "\u001b[2K    Uninstalling sentence-transformers-3.3.1:90mâ•º\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m \u001b[32m7/8\u001b[0m [sentence-transformers]\n",
      "\u001b[2K      Successfully uninstalled sentence-transformers-3.3.1â”â”â”â”\u001b[0m \u001b[32m7/8\u001b[0m [sentence-transformers]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8/8\u001b[0m [sentence-transformers]ence-transformers]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "open-webui 0.5.7 requires sentence-transformers==3.3.1, but you have sentence-transformers 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed base58-2.1.1 ipfshttpclient-0.7.0 multiaddr-0.0.9 netaddr-1.3.0 pysqlite3-0.5.4 sentence-transformers-2.2.2 torchvision-0.22.0 varint-1.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if you have not installed them already\n",
    "%pip install -r requirements.txt --verbose --quiet\n",
    "%pip install -q --upgrade pip\n",
    "\n",
    "# Example output (replace Codespaces path with $HOME):\n",
    "# Requirement already satisfied: matplotlib>=3.7.0 in $HOME/.local/lib/python3.12/site-packages (from -r requirements.txt (line 21)) (3.10.1)\n",
    "# Requirement already satisfied: IPython>=8.0.0 in $HOME/.local/lib/python3.12/site-packages (from -r requirements.txt (line 22)) (9.0.2)\n",
    "# Requirement already satisfied: plotly>=5.15.0 in $HOME/.local/lib/python3.12/site-packages (from -r requirements.txt (line 24)) (6.0.1)\n",
    "# Requirement already satisfied: pandas>=2.0.0 in $HOME/.local/lib/python3.12/site-packages (from -r requirements.txt (line 25)) (2.2.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Step 1: Model Setup\n",
    "\n",
    "We will set up **Llama 2 (7B)** for text generation. If the model is not found locally, it will be downloaded from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(43578) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Model already exists at: model/llama-2-7b-chat.Q4_K_M.gguf\n",
      "Using model at: model/llama-2-7b-chat.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "%pip install -q huggingface-hub\n",
    "\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "MODEL_FILENAME = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "MODEL_DIR = \"model\"\n",
    "EXPECTED_PATH = os.path.join(MODEL_DIR, MODEL_FILENAME)\n",
    "\n",
    "# Ensure model directory exists\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Check if model already exists\n",
    "if os.path.exists(EXPECTED_PATH):\n",
    "    print(f\"Model already exists at: {EXPECTED_PATH}\")\n",
    "    model_path = EXPECTED_PATH\n",
    "else:\n",
    "    print(\"Model not found locally. Downloading Llama 2 model...\")\n",
    "    \n",
    "    # Download the model - Fixed: removed url parameter and added correct parameters\n",
    "    model_path = hf_hub_download(\n",
    "        repo_id=\"TheBloke/Llama-2-7B-Chat-GGUF\",\n",
    "        filename=MODEL_FILENAME,\n",
    "        local_dir=MODEL_DIR\n",
    "    )\n",
    "    print(f\"Model downloaded to: {model_path}\")\n",
    "\n",
    "print(f\"Using model at: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q llama-cpp-python\n",
    "# Check if the model file exists\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "# Import the Llama class from llama_cpp\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Initialize the model with the local path and GPU acceleration\n",
    "llm = Llama(\n",
    "    model_path=EXPECTED_PATH,\n",
    "    temperature=0.25,\n",
    "    max_tokens=2000,\n",
    "    n_ctx=4096,\n",
    "    top_p=1.0,\n",
    "    verbose=False,\n",
    "    n_gpu_layers=30,  # Utilize some available GPU layers\n",
    "    n_batch=512,      # Optimize batch size for parallel processing\n",
    "    f16_kv=True,      # Enable half-precision for key/value cache\n",
    "    use_mlock=True,   # Lock memory to prevent swapping\n",
    "    use_mmap=True     # Utilize memory mapping for faster loading\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“„ Step 2: Loading, Transcribing, and Storing Audio Data\n",
    "\n",
    "This step outlines the process for loading audio data (e.g., from an Omi streaming device), transcribing it, and preparing it for storage and further processing. Both the raw audio and its transcription are valuable assets.\n",
    "\n",
    "### Updated Workflow:\n",
    "- Load audio data from files or streams.\n",
    "- Transcribe audio to text using the LLM model.\n",
    "- Extract and tag audio segments based on content.\n",
    "- Store audio and transcription in a structured format.\n",
    "- (Optional) Save audio segments as separate files for download or sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(48954) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(49037) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(49059) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg found and working.\n",
      "Loading AUDIO from: ./data/tester.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/PRO-BLADE/opt/anaconda3/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 1 document(s) from the AUDIO.\n",
      "Successfully loaded 1 document(s) from the AUDIO and created 1 audio file(s).\n"
     ]
    }
   ],
   "source": [
    "# --- Load the Audio File Document ---\n",
    "# --- Load the Audio File Document and Audio Collection System ---\n",
    "\n",
    "# Install whisper if not already installed\n",
    "%pip install -q openai-whisper\n",
    "\n",
    "# Install ffmpeg-python bindings if not already installed\n",
    "%pip install -q ffmpeg-python\n",
    "\n",
    "import shutil\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Check if ffmpeg is available and working\n",
    "try:\n",
    "    subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, check=True)\n",
    "    print(\"ffmpeg found and working.\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"ffmpeg not found. Please install ffmpeg and ensure it's in your PATH.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error running ffmpeg: {{e}}\")\n",
    "    raise RuntimeError(\"ffmpeg is not working correctly.\") from e\n",
    "\n",
    "import whisper\n",
    "\n",
    "# No need to manually set ffmpeg_dir if ffmpeg is installed system-wide\n",
    "\n",
    "# Define the Audio File file path\n",
    "AUDIO_PATH = \"./data/tester.mp3\"\n",
    "# Check if the file exists\n",
    "print(f\"Loading AUDIO from: {AUDIO_PATH}\")\n",
    "\n",
    "# Define the audio collection system\n",
    "AUDIO_COLLECTION_SYSTEM = \"MyAudioSystem\"\n",
    "\n",
    "# Load and transcribe the audio file using whisper\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(AUDIO_PATH)\n",
    "text_content = result[\"text\"]\n",
    "\n",
    "# For compatibility with the rest of your code, wrap the text in a document-like object\n",
    "class AudioDocument:\n",
    "    def __init__(self, text):\n",
    "        self.page_content = text\n",
    "        self.metadata = {}  # Optionally add metadata if needed\n",
    "    def getPageText(self):\n",
    "        return self.page_content\n",
    "\n",
    "documents = [AudioDocument(text_content)]\n",
    "\n",
    "print(f\"Successfully loaded {len(documents)} document(s) from the AUDIO.\")\n",
    "# Initialize an empty list for the audio files\n",
    "audio_files = []\n",
    "\n",
    "# Iterate through each document\n",
    "for document in documents:\n",
    "    # Get the current page's text content\n",
    "    text_content = document.getPageText()\n",
    "\n",
    "    # Extract relevant information from the text, e.g., keywords or phrases\n",
    "    def extractRelevantInfo(text):\n",
    "        # Placeholder: just return the text itself\n",
    "        return text\n",
    "    extracted_info = extractRelevantInfo(text_content)\n",
    "\n",
    "    # Define a placeholder for createAudioFile\n",
    "    def createAudioFile(system, info):\n",
    "        # Placeholder: just return a tuple for demonstration\n",
    "        return (system, info)\n",
    "\n",
    "    # Create an audio file based on the extracted information\n",
    "    audio_file = createAudioFile(AUDIO_COLLECTION_SYSTEM, extracted_info)\n",
    "\n",
    "    # Add the audio file to the collection system's list\n",
    "    audio_files.append(audio_file)\n",
    "print(f\"Successfully loaded {len(documents)} document(s) from the AUDIO and created {len(audio_files)} audio file(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Checkpoint path: /home/codespace/panns_data/Cnn14_mAP=0.431.pth\n",
      "Using CPU.\n",
      "Detected sound types: ['Speech', 'Inside, small room', 'Burping, eructation']\n"
     ]
    }
   ],
   "source": [
    "# Get Omi data from Webhook\n",
    "\n",
    "# Initialize variables for Omi data from Webhook\n",
    "response = None  # Will store webhook response\n",
    "rag_percentage = 0.0  # Initialize RAG percentage\n",
    "ipfs_metrics = {}  # Initialize IPFS metrics dictionary\n",
    "\n",
    "# --- Audio Event Detection (Sound Tagging) ---\n",
    "\n",
    "%pip install -q torchaudio\n",
    "%pip install -q panns-inference\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from panns_inference import AudioTagging\n",
    "\n",
    "# Initialize the PANNs audio tagging model\n",
    "panns_model = AudioTagging(checkpoint_path=None, device='cpu')  # Uses default Cnn14 weights\n",
    "\n",
    "# Load and preprocess audio\n",
    "waveform, sr = torchaudio.load(AUDIO_PATH)\n",
    "if sr != 32000:\n",
    "    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=32000)(waveform)\n",
    "    sr = 32000\n",
    "\n",
    "# PANNs expects mono audio\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "# --- Load the full AudioSet class labels for PANNs output mapping ---\n",
    "import os\n",
    "import csv\n",
    "\n",
    "LABELS_CSV_PATH = \"class_labels_indices.csv\"\n",
    "labels = None\n",
    "if os.path.exists(LABELS_CSV_PATH):\n",
    "    with open(LABELS_CSV_PATH, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        labels = [row['display_name'] for row in reader]\n",
    "else:\n",
    "    print(\"WARNING: AudioSet class label CSV not found. Will print indices instead of class names.\")\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = panns_model.inference(waveform)\n",
    "    clipwise_output = output[0]\n",
    "    topk = torch.topk(torch.tensor(clipwise_output[0]), 3)\n",
    "    sound_tags = []\n",
    "    if labels and max(topk.indices.tolist()) < len(labels):\n",
    "        sound_tags = [labels[i] for i in topk.indices.tolist()]\n",
    "    else:\n",
    "        sound_tags = [f\"Class index {i}\" for i in topk.indices.tolist()]\n",
    "\n",
    "print(\"Detected sound types:\", sound_tags)\n",
    "\n",
    "# Example output (replace Codespaces path with $HOME):\n",
    "# Checkpoint path: $HOME/panns_data/Cnn14_mAP=0.431.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ Step 3: Chunking Audio Transcriptions for RAG\n",
    "\n",
    "The transcribed text from the audio data is split into **small overlapping chunks** (approximately **500 characters**). These chunks are then used for embedding and storage in ChromaDB to enable semantic search for the RAG pipeline.\n",
    "\n",
    "### Updated Workflow:\n",
    "- Split transcriptions into smaller chunks to manage context size.\n",
    "- Overlap chunks slightly to ensure continuity and context preservation.\n",
    "- Tag chunks with relevant metadata (e.g., sound types, timestamps).\n",
    "- Store chunks in ChromaDB with embeddings for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# --- Split the Audio Content into Manageable Chunks with Sound Tags ---\n",
    "%pip install -q langchain\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "text_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "\n",
    "# Split the transcription into chunks\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Attach sound tags to each chunk as metadata\n",
    "for doc in docs:\n",
    "    doc.page_content = f\"Transcription: {doc.page_content}\\nSound tags: {', '.join(sound_tags)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Step 4: Initializing the Embedding Model\n",
    "\n",
    "To convert text into numerical representations for efficient similarity search, we use **all-MiniLM-L6-v2** from `sentence-transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_cached_models' from 'transformers.utils' (/home/codespace/.python/current/lib/python3.12/site-packages/transformers/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall -q sentence-transformers\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall -q transformers\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Define the embedding model name\u001b[39;00m\n\u001b[32m      8\u001b[39m MODEL_NAME = \u001b[33m\"\u001b[39m\u001b[33mall-MiniLM-L6-v2\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/sentence_transformers/__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m2.6.1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m __MODEL_HUB_ORGANIZATION__ = \u001b[33m\"\u001b[39m\u001b[33msentence-transformers\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset, ParallelSentencesDataset\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mLoggingHandler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mSentenceTransformer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/sentence_transformers/datasets/__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mDenoisingAutoEncoderDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DenoisingAutoEncoderDataset\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mNoDuplicatesDataLoader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mParallelSentencesDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mSentencesDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mSentenceLabelDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceLabelDataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/sentence_transformers/datasets/ParallelSentencesDataset.py:4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgzip\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InputExample\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:36\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     26\u001b[39m     import_from_string,\n\u001b[32m     27\u001b[39m     batch_to_device,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     get_device_name,\n\u001b[32m     34\u001b[39m )\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quantize_embeddings\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Transformer, Pooling, Normalize\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_card_templates\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelCardTemplate\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/sentence_transformers/models/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mTransformer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Transformer\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mAsym\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Asym\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mBoW\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BoW\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModel, AutoTokenizer, AutoConfig, T5Config, MT5Config\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Optional, Union, Tuple\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/transformers/utils/import_utils.py:2045\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/transformers/utils/import_utils.py:2075\u001b[39m, in \u001b[36m_get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/transformers/utils/import_utils.py:2073\u001b[39m, in \u001b[36m_get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:36\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenization_utils_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TOKENIZER_CONFIG_FILE\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     cached_file,\n\u001b[32m     30\u001b[39m     extract_commit_hash,\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m     logging,\n\u001b[32m     35\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mencoder_decoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EncoderDecoderConfig\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LazyAutoMapping\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     39\u001b[39m     CONFIG_MAPPING_NAMES,\n\u001b[32m     40\u001b[39m     AutoConfig,\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     replace_list_option_in_docstrings,\n\u001b[32m     44\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/transformers/models/__init__.py:15\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     albert,\n\u001b[32m     17\u001b[39m     align,\n\u001b[32m     18\u001b[39m     altclip,\n\u001b[32m     19\u001b[39m     audio_spectrogram_transformer,\n\u001b[32m     20\u001b[39m     auto,\n\u001b[32m     21\u001b[39m     autoformer,\n\u001b[32m     22\u001b[39m     bark,\n\u001b[32m     23\u001b[39m     bart,\n\u001b[32m     24\u001b[39m     barthez,\n\u001b[32m     25\u001b[39m     bartpho,\n\u001b[32m     26\u001b[39m     beit,\n\u001b[32m     27\u001b[39m     bert,\n\u001b[32m     28\u001b[39m     bert_generation,\n\u001b[32m     29\u001b[39m     bert_japanese,\n\u001b[32m     30\u001b[39m     bertweet,\n\u001b[32m     31\u001b[39m     big_bird,\n\u001b[32m     32\u001b[39m     bigbird_pegasus,\n\u001b[32m     33\u001b[39m     biogpt,\n\u001b[32m     34\u001b[39m     bit,\n\u001b[32m     35\u001b[39m     blenderbot,\n\u001b[32m     36\u001b[39m     blenderbot_small,\n\u001b[32m     37\u001b[39m     blip,\n\u001b[32m     38\u001b[39m     blip_2,\n\u001b[32m     39\u001b[39m     bloom,\n\u001b[32m     40\u001b[39m     bridgetower,\n\u001b[32m     41\u001b[39m     bros,\n\u001b[32m     42\u001b[39m     byt5,\n\u001b[32m     43\u001b[39m     camembert,\n\u001b[32m     44\u001b[39m     canine,\n\u001b[32m     45\u001b[39m     chinese_clip,\n\u001b[32m     46\u001b[39m     clap,\n\u001b[32m     47\u001b[39m     clip,\n\u001b[32m     48\u001b[39m     clipseg,\n\u001b[32m     49\u001b[39m     clvp,\n\u001b[32m     50\u001b[39m     code_llama,\n\u001b[32m     51\u001b[39m     codegen,\n\u001b[32m     52\u001b[39m     cohere,\n\u001b[32m     53\u001b[39m     conditional_detr,\n\u001b[32m     54\u001b[39m     convbert,\n\u001b[32m     55\u001b[39m     convnext,\n\u001b[32m     56\u001b[39m     convnextv2,\n\u001b[32m     57\u001b[39m     cpm,\n\u001b[32m     58\u001b[39m     cpmant,\n\u001b[32m     59\u001b[39m     ctrl,\n\u001b[32m     60\u001b[39m     cvt,\n\u001b[32m     61\u001b[39m     data2vec,\n\u001b[32m     62\u001b[39m     dbrx,\n\u001b[32m     63\u001b[39m     deberta,\n\u001b[32m     64\u001b[39m     deberta_v2,\n\u001b[32m     65\u001b[39m     decision_transformer,\n\u001b[32m     66\u001b[39m     deformable_detr,\n\u001b[32m     67\u001b[39m     deit,\n\u001b[32m     68\u001b[39m     deprecated,\n\u001b[32m     69\u001b[39m     depth_anything,\n\u001b[32m     70\u001b[39m     deta,\n\u001b[32m     71\u001b[39m     detr,\n\u001b[32m     72\u001b[39m     dialogpt,\n\u001b[32m     73\u001b[39m     dinat,\n\u001b[32m     74\u001b[39m     dinov2,\n\u001b[32m     75\u001b[39m     distilbert,\n\u001b[32m     76\u001b[39m     dit,\n\u001b[32m     77\u001b[39m     donut,\n\u001b[32m     78\u001b[39m     dpr,\n\u001b[32m     79\u001b[39m     dpt,\n\u001b[32m     80\u001b[39m     efficientformer,\n\u001b[32m     81\u001b[39m     efficientnet,\n\u001b[32m     82\u001b[39m     electra,\n\u001b[32m     83\u001b[39m     encodec,\n\u001b[32m     84\u001b[39m     encoder_decoder,\n\u001b[32m     85\u001b[39m     ernie,\n\u001b[32m     86\u001b[39m     ernie_m,\n\u001b[32m     87\u001b[39m     esm,\n\u001b[32m     88\u001b[39m     falcon,\n\u001b[32m     89\u001b[39m     fastspeech2_conformer,\n\u001b[32m     90\u001b[39m     flaubert,\n\u001b[32m     91\u001b[39m     flava,\n\u001b[32m     92\u001b[39m     fnet,\n\u001b[32m     93\u001b[39m     focalnet,\n\u001b[32m     94\u001b[39m     fsmt,\n\u001b[32m     95\u001b[39m     funnel,\n\u001b[32m     96\u001b[39m     fuyu,\n\u001b[32m     97\u001b[39m     gemma,\n\u001b[32m     98\u001b[39m     git,\n\u001b[32m     99\u001b[39m     glpn,\n\u001b[32m    100\u001b[39m     gpt2,\n\u001b[32m    101\u001b[39m     gpt_bigcode,\n\u001b[32m    102\u001b[39m     gpt_neo,\n\u001b[32m    103\u001b[39m     gpt_neox,\n\u001b[32m    104\u001b[39m     gpt_neox_japanese,\n\u001b[32m    105\u001b[39m     gpt_sw3,\n\u001b[32m    106\u001b[39m     gptj,\n\u001b[32m    107\u001b[39m     gptsan_japanese,\n\u001b[32m    108\u001b[39m     graphormer,\n\u001b[32m    109\u001b[39m     grounding_dino,\n\u001b[32m    110\u001b[39m     groupvit,\n\u001b[32m    111\u001b[39m     herbert,\n\u001b[32m    112\u001b[39m     hubert,\n\u001b[32m    113\u001b[39m     ibert,\n\u001b[32m    114\u001b[39m     idefics,\n\u001b[32m    115\u001b[39m     idefics2,\n\u001b[32m    116\u001b[39m     imagegpt,\n\u001b[32m    117\u001b[39m     informer,\n\u001b[32m    118\u001b[39m     instructblip,\n\u001b[32m    119\u001b[39m     jamba,\n\u001b[32m    120\u001b[39m     jukebox,\n\u001b[32m    121\u001b[39m     kosmos2,\n\u001b[32m    122\u001b[39m     layoutlm,\n\u001b[32m    123\u001b[39m     layoutlmv2,\n\u001b[32m    124\u001b[39m     layoutlmv3,\n\u001b[32m    125\u001b[39m     layoutxlm,\n\u001b[32m    126\u001b[39m     led,\n\u001b[32m    127\u001b[39m     levit,\n\u001b[32m    128\u001b[39m     lilt,\n\u001b[32m    129\u001b[39m     llama,\n\u001b[32m    130\u001b[39m     llava,\n\u001b[32m    131\u001b[39m     llava_next,\n\u001b[32m    132\u001b[39m     longformer,\n\u001b[32m    133\u001b[39m     longt5,\n\u001b[32m    134\u001b[39m     luke,\n\u001b[32m    135\u001b[39m     lxmert,\n\u001b[32m    136\u001b[39m     m2m_100,\n\u001b[32m    137\u001b[39m     mamba,\n\u001b[32m    138\u001b[39m     marian,\n\u001b[32m    139\u001b[39m     markuplm,\n\u001b[32m    140\u001b[39m     mask2former,\n\u001b[32m    141\u001b[39m     maskformer,\n\u001b[32m    142\u001b[39m     mbart,\n\u001b[32m    143\u001b[39m     mbart50,\n\u001b[32m    144\u001b[39m     mega,\n\u001b[32m    145\u001b[39m     megatron_bert,\n\u001b[32m    146\u001b[39m     megatron_gpt2,\n\u001b[32m    147\u001b[39m     mgp_str,\n\u001b[32m    148\u001b[39m     mistral,\n\u001b[32m    149\u001b[39m     mixtral,\n\u001b[32m    150\u001b[39m     mluke,\n\u001b[32m    151\u001b[39m     mobilebert,\n\u001b[32m    152\u001b[39m     mobilenet_v1,\n\u001b[32m    153\u001b[39m     mobilenet_v2,\n\u001b[32m    154\u001b[39m     mobilevit,\n\u001b[32m    155\u001b[39m     mobilevitv2,\n\u001b[32m    156\u001b[39m     mpnet,\n\u001b[32m    157\u001b[39m     mpt,\n\u001b[32m    158\u001b[39m     mra,\n\u001b[32m    159\u001b[39m     mt5,\n\u001b[32m    160\u001b[39m     musicgen,\n\u001b[32m    161\u001b[39m     musicgen_melody,\n\u001b[32m    162\u001b[39m     mvp,\n\u001b[32m    163\u001b[39m     nat,\n\u001b[32m    164\u001b[39m     nezha,\n\u001b[32m    165\u001b[39m     nllb,\n\u001b[32m    166\u001b[39m     nllb_moe,\n\u001b[32m    167\u001b[39m     nougat,\n\u001b[32m    168\u001b[39m     nystromformer,\n\u001b[32m    169\u001b[39m     olmo,\n\u001b[32m    170\u001b[39m     oneformer,\n\u001b[32m    171\u001b[39m     openai,\n\u001b[32m    172\u001b[39m     opt,\n\u001b[32m    173\u001b[39m     owlv2,\n\u001b[32m    174\u001b[39m     owlvit,\n\u001b[32m    175\u001b[39m     patchtsmixer,\n\u001b[32m    176\u001b[39m     patchtst,\n\u001b[32m    177\u001b[39m     pegasus,\n\u001b[32m    178\u001b[39m     pegasus_x,\n\u001b[32m    179\u001b[39m     perceiver,\n\u001b[32m    180\u001b[39m     persimmon,\n\u001b[32m    181\u001b[39m     phi,\n\u001b[32m    182\u001b[39m     phobert,\n\u001b[32m    183\u001b[39m     pix2struct,\n\u001b[32m    184\u001b[39m     plbart,\n\u001b[32m    185\u001b[39m     poolformer,\n\u001b[32m    186\u001b[39m     pop2piano,\n\u001b[32m    187\u001b[39m     prophetnet,\n\u001b[32m    188\u001b[39m     pvt,\n\u001b[32m    189\u001b[39m     pvt_v2,\n\u001b[32m    190\u001b[39m     qdqbert,\n\u001b[32m    191\u001b[39m     qwen2,\n\u001b[32m    192\u001b[39m     qwen2_moe,\n\u001b[32m    193\u001b[39m     rag,\n\u001b[32m    194\u001b[39m     realm,\n\u001b[32m    195\u001b[39m     recurrent_gemma,\n\u001b[32m    196\u001b[39m     reformer,\n\u001b[32m    197\u001b[39m     regnet,\n\u001b[32m    198\u001b[39m     rembert,\n\u001b[32m    199\u001b[39m     resnet,\n\u001b[32m    200\u001b[39m     roberta,\n\u001b[32m    201\u001b[39m     roberta_prelayernorm,\n\u001b[32m    202\u001b[39m     roc_bert,\n\u001b[32m    203\u001b[39m     roformer,\n\u001b[32m    204\u001b[39m     rwkv,\n\u001b[32m    205\u001b[39m     sam,\n\u001b[32m    206\u001b[39m     seamless_m4t,\n\u001b[32m    207\u001b[39m     seamless_m4t_v2,\n\u001b[32m    208\u001b[39m     segformer,\n\u001b[32m    209\u001b[39m     seggpt,\n\u001b[32m    210\u001b[39m     sew,\n\u001b[32m    211\u001b[39m     sew_d,\n\u001b[32m    212\u001b[39m     siglip,\n\u001b[32m    213\u001b[39m     speech_encoder_decoder,\n\u001b[32m    214\u001b[39m     speech_to_text,\n\u001b[32m    215\u001b[39m     speech_to_text_2,\n\u001b[32m    216\u001b[39m     speecht5,\n\u001b[32m    217\u001b[39m     splinter,\n\u001b[32m    218\u001b[39m     squeezebert,\n\u001b[32m    219\u001b[39m     stablelm,\n\u001b[32m    220\u001b[39m     starcoder2,\n\u001b[32m    221\u001b[39m     superpoint,\n\u001b[32m    222\u001b[39m     swiftformer,\n\u001b[32m    223\u001b[39m     swin,\n\u001b[32m    224\u001b[39m     swin2sr,\n\u001b[32m    225\u001b[39m     swinv2,\n\u001b[32m    226\u001b[39m     switch_transformers,\n\u001b[32m    227\u001b[39m     t5,\n\u001b[32m    228\u001b[39m     table_transformer,\n\u001b[32m    229\u001b[39m     tapas,\n\u001b[32m    230\u001b[39m     time_series_transformer,\n\u001b[32m    231\u001b[39m     timesformer,\n\u001b[32m    232\u001b[39m     timm_backbone,\n\u001b[32m    233\u001b[39m     trocr,\n\u001b[32m    234\u001b[39m     tvlt,\n\u001b[32m    235\u001b[39m     tvp,\n\u001b[32m    236\u001b[39m     udop,\n\u001b[32m    237\u001b[39m     umt5,\n\u001b[32m    238\u001b[39m     unispeech,\n\u001b[32m    239\u001b[39m     unispeech_sat,\n\u001b[32m    240\u001b[39m     univnet,\n\u001b[32m    241\u001b[39m     upernet,\n\u001b[32m    242\u001b[39m     videomae,\n\u001b[32m    243\u001b[39m     vilt,\n\u001b[32m    244\u001b[39m     vipllava,\n\u001b[32m    245\u001b[39m     vision_encoder_decoder,\n\u001b[32m    246\u001b[39m     vision_text_dual_encoder,\n\u001b[32m    247\u001b[39m     visual_bert,\n\u001b[32m    248\u001b[39m     vit,\n\u001b[32m    249\u001b[39m     vit_hybrid,\n\u001b[32m    250\u001b[39m     vit_mae,\n\u001b[32m    251\u001b[39m     vit_msn,\n\u001b[32m    252\u001b[39m     vitdet,\n\u001b[32m    253\u001b[39m     vitmatte,\n\u001b[32m    254\u001b[39m     vits,\n\u001b[32m    255\u001b[39m     vivit,\n\u001b[32m    256\u001b[39m     wav2vec2,\n\u001b[32m    257\u001b[39m     wav2vec2_bert,\n\u001b[32m    258\u001b[39m     wav2vec2_conformer,\n\u001b[32m    259\u001b[39m     wav2vec2_phoneme,\n\u001b[32m    260\u001b[39m     wav2vec2_with_lm,\n\u001b[32m    261\u001b[39m     wavlm,\n\u001b[32m    262\u001b[39m     whisper,\n\u001b[32m    263\u001b[39m     x_clip,\n\u001b[32m    264\u001b[39m     xglm,\n\u001b[32m    265\u001b[39m     xlm,\n\u001b[32m    266\u001b[39m     xlm_prophetnet,\n\u001b[32m    267\u001b[39m     xlm_roberta,\n\u001b[32m    268\u001b[39m     xlm_roberta_xl,\n\u001b[32m    269\u001b[39m     xlnet,\n\u001b[32m    270\u001b[39m     xmod,\n\u001b[32m    271\u001b[39m     yolos,\n\u001b[32m    272\u001b[39m     yoso,\n\u001b[32m    273\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/transformers/models/depth_anything/__init__.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2024 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfile_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LazyModule, is_torch_available\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OptionalDependencyNotAvailable\n\u001b[32m     20\u001b[39m _import_structure = {\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfiguration_depth_anything\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mDEPTH_ANYTHING_PRETRAINED_CONFIG_ARCHIVE_MAP\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDepthAnythingConfig\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     22\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/transformers/file_utils.py:26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Backward compatibility imports, to make sure all those objects can be found in file_utils\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     27\u001b[39m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[32m     28\u001b[39m     CONFIG_NAME,\n\u001b[32m     29\u001b[39m     DUMMY_INPUTS,\n\u001b[32m     30\u001b[39m     DUMMY_MASK,\n\u001b[32m     31\u001b[39m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[32m     32\u001b[39m     ENV_VARS_TRUE_VALUES,\n\u001b[32m     33\u001b[39m     FEATURE_EXTRACTOR_NAME,\n\u001b[32m     34\u001b[39m     FLAX_WEIGHTS_NAME,\n\u001b[32m     35\u001b[39m     HF_MODULES_CACHE,\n\u001b[32m     36\u001b[39m     HUGGINGFACE_CO_PREFIX,\n\u001b[32m     37\u001b[39m     HUGGINGFACE_CO_RESOLVE_ENDPOINT,\n\u001b[32m     38\u001b[39m     MODEL_CARD_NAME,\n\u001b[32m     39\u001b[39m     MULTIPLE_CHOICE_DUMMY_INPUTS,\n\u001b[32m     40\u001b[39m     PYTORCH_PRETRAINED_BERT_CACHE,\n\u001b[32m     41\u001b[39m     PYTORCH_TRANSFORMERS_CACHE,\n\u001b[32m     42\u001b[39m     S3_BUCKET_PREFIX,\n\u001b[32m     43\u001b[39m     SENTENCEPIECE_UNDERLINE,\n\u001b[32m     44\u001b[39m     SPIECE_UNDERLINE,\n\u001b[32m     45\u001b[39m     TF2_WEIGHTS_NAME,\n\u001b[32m     46\u001b[39m     TF_WEIGHTS_NAME,\n\u001b[32m     47\u001b[39m     TORCH_FX_REQUIRED_VERSION,\n\u001b[32m     48\u001b[39m     TRANSFORMERS_CACHE,\n\u001b[32m     49\u001b[39m     TRANSFORMERS_DYNAMIC_MODULE_NAME,\n\u001b[32m     50\u001b[39m     USE_JAX,\n\u001b[32m     51\u001b[39m     USE_TF,\n\u001b[32m     52\u001b[39m     USE_TORCH,\n\u001b[32m     53\u001b[39m     WEIGHTS_INDEX_NAME,\n\u001b[32m     54\u001b[39m     WEIGHTS_NAME,\n\u001b[32m     55\u001b[39m     ContextManagers,\n\u001b[32m     56\u001b[39m     DummyObject,\n\u001b[32m     57\u001b[39m     EntryNotFoundError,\n\u001b[32m     58\u001b[39m     ExplicitEnum,\n\u001b[32m     59\u001b[39m     ModelOutput,\n\u001b[32m     60\u001b[39m     PaddingStrategy,\n\u001b[32m     61\u001b[39m     PushToHubMixin,\n\u001b[32m     62\u001b[39m     RepositoryNotFoundError,\n\u001b[32m     63\u001b[39m     RevisionNotFoundError,\n\u001b[32m     64\u001b[39m     TensorType,\n\u001b[32m     65\u001b[39m     _LazyModule,\n\u001b[32m     66\u001b[39m     add_code_sample_docstrings,\n\u001b[32m     67\u001b[39m     add_end_docstrings,\n\u001b[32m     68\u001b[39m     add_start_docstrings,\n\u001b[32m     69\u001b[39m     add_start_docstrings_to_model_forward,\n\u001b[32m     70\u001b[39m     cached_property,\n\u001b[32m     71\u001b[39m     copy_func,\n\u001b[32m     72\u001b[39m     default_cache_path,\n\u001b[32m     73\u001b[39m     define_sagemaker_information,\n\u001b[32m     74\u001b[39m     get_cached_models,\n\u001b[32m     75\u001b[39m     get_file_from_repo,\n\u001b[32m     76\u001b[39m     get_torch_version,\n\u001b[32m     77\u001b[39m     has_file,\n\u001b[32m     78\u001b[39m     http_user_agent,\n\u001b[32m     79\u001b[39m     is_apex_available,\n\u001b[32m     80\u001b[39m     is_bs4_available,\n\u001b[32m     81\u001b[39m     is_coloredlogs_available,\n\u001b[32m     82\u001b[39m     is_datasets_available,\n\u001b[32m     83\u001b[39m     is_detectron2_available,\n\u001b[32m     84\u001b[39m     is_faiss_available,\n\u001b[32m     85\u001b[39m     is_flax_available,\n\u001b[32m     86\u001b[39m     is_ftfy_available,\n\u001b[32m     87\u001b[39m     is_g2p_en_available,\n\u001b[32m     88\u001b[39m     is_in_notebook,\n\u001b[32m     89\u001b[39m     is_ipex_available,\n\u001b[32m     90\u001b[39m     is_librosa_available,\n\u001b[32m     91\u001b[39m     is_offline_mode,\n\u001b[32m     92\u001b[39m     is_onnx_available,\n\u001b[32m     93\u001b[39m     is_pandas_available,\n\u001b[32m     94\u001b[39m     is_phonemizer_available,\n\u001b[32m     95\u001b[39m     is_protobuf_available,\n\u001b[32m     96\u001b[39m     is_psutil_available,\n\u001b[32m     97\u001b[39m     is_py3nvml_available,\n\u001b[32m     98\u001b[39m     is_pyctcdecode_available,\n\u001b[32m     99\u001b[39m     is_pytesseract_available,\n\u001b[32m    100\u001b[39m     is_pytorch_quantization_available,\n\u001b[32m    101\u001b[39m     is_rjieba_available,\n\u001b[32m    102\u001b[39m     is_sagemaker_dp_enabled,\n\u001b[32m    103\u001b[39m     is_sagemaker_mp_enabled,\n\u001b[32m    104\u001b[39m     is_scipy_available,\n\u001b[32m    105\u001b[39m     is_sentencepiece_available,\n\u001b[32m    106\u001b[39m     is_seqio_available,\n\u001b[32m    107\u001b[39m     is_sklearn_available,\n\u001b[32m    108\u001b[39m     is_soundfile_availble,\n\u001b[32m    109\u001b[39m     is_spacy_available,\n\u001b[32m    110\u001b[39m     is_speech_available,\n\u001b[32m    111\u001b[39m     is_tensor,\n\u001b[32m    112\u001b[39m     is_tensorflow_probability_available,\n\u001b[32m    113\u001b[39m     is_tf2onnx_available,\n\u001b[32m    114\u001b[39m     is_tf_available,\n\u001b[32m    115\u001b[39m     is_timm_available,\n\u001b[32m    116\u001b[39m     is_tokenizers_available,\n\u001b[32m    117\u001b[39m     is_torch_available,\n\u001b[32m    118\u001b[39m     is_torch_bf16_available,\n\u001b[32m    119\u001b[39m     is_torch_cuda_available,\n\u001b[32m    120\u001b[39m     is_torch_fx_available,\n\u001b[32m    121\u001b[39m     is_torch_fx_proxy,\n\u001b[32m    122\u001b[39m     is_torch_mps_available,\n\u001b[32m    123\u001b[39m     is_torch_tf32_available,\n\u001b[32m    124\u001b[39m     is_torch_xla_available,\n\u001b[32m    125\u001b[39m     is_torchaudio_available,\n\u001b[32m    126\u001b[39m     is_training_run_on_sagemaker,\n\u001b[32m    127\u001b[39m     is_vision_available,\n\u001b[32m    128\u001b[39m     replace_return_docstrings,\n\u001b[32m    129\u001b[39m     requires_backends,\n\u001b[32m    130\u001b[39m     to_numpy,\n\u001b[32m    131\u001b[39m     to_py_obj,\n\u001b[32m    132\u001b[39m     torch_only_method,\n\u001b[32m    133\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'get_cached_models' from 'transformers.utils' (/home/codespace/.python/current/lib/python3.12/site-packages/transformers/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "# --- Initialize the Embedding Model ---\n",
    "%pip install -q sentence-transformers\n",
    "%pip install -q transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Define the embedding model name\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "print(f\"Successfully loaded embedding model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Step 5: Computing Embeddings for Document Chunks\n",
    "\n",
    "Each chunk is converted into a **vector representation** using our embedding model. This allows us to perform **semantic similarity searches** later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m doc_texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Compute embeddings for the extracted text chunks\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m document_embeddings = \u001b[43membedding_model\u001b[49m.encode(doc_texts, convert_to_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Display the result\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSuccessfully computed embeddings for each text chunk.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'embedding_model' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Compute Embeddings for Each Text Chunk ---\n",
    "\n",
    "# Extract text content from each chunk\n",
    "doc_texts = [doc.page_content for doc in docs]\n",
    "\n",
    "# Compute embeddings for the extracted text chunks\n",
    "document_embeddings = embedding_model.encode(doc_texts, convert_to_numpy=True)\n",
    "\n",
    "# Display the result\n",
    "print(\"Successfully computed embeddings for each text chunk.\")\n",
    "print(f\"Embeddings Shape: {document_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—„ï¸ Step 6: Storing Audio Transcription Embeddings in ChromaDB\n",
    "\n",
    "We initialize **ChromaDB**, a high-performance **vector database**, and store our computed embeddings to enable efficient retrieval of relevant text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize and Populate the Chroma Vector Database ---\n",
    "\n",
    "# Define Chroma database path and collection name\n",
    "CHROMA_DB_PATH = \"./chroma_db\"\n",
    "COLLECTION_NAME = \"document_embeddings\"\n",
    "\n",
    "# Initialize Chroma client\n",
    "import chromadb\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)\n",
    "\n",
    "# Add document embeddings to the Chroma collection\n",
    "for i, embedding in enumerate(document_embeddings):\n",
    "    collection.add(\n",
    "        ids=[str(i)],  # Chroma requires string IDs\n",
    "        embeddings=[embedding.tolist()],\n",
    "        metadatas=[{\"text\": doc_texts[i]}]\n",
    "    )\n",
    "\n",
    "print(\"Successfully populated Chroma database with document embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”Ž Step 7: Implementing Vector Search Tool\n",
    "\n",
    "To retrieve relevant text passages from the database, we define a **vector search function** that finds the most relevant chunks based on a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Searches the Chroma database for relevant text chunks based on the query.\n",
    "    Computes the query embedding, retrieves the top 5 most relevant text chunks,\n",
    "    and returns them as a formatted string.\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.encode(query, convert_to_numpy=True).tolist()\n",
    "    TOP_K = 5\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=TOP_K\n",
    "    )\n",
    "    retrieved_chunks = [metadata[\"text\"] for metadata in results[\"metadatas\"][0]]\n",
    "    return \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "print(vector_search_tool(\"example query\"))  # Test the search tool with a sample query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– Step 8: Context Need Assessment\n",
    "\n",
    "Instead of always retrieving context, we determine if the query **requires external document context** before generating a response. This creates an agentic workflow that makes autonomous decisions to complete the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import re\n",
    "\n",
    "# --- Audio Segment Retrieval and Saving ---\n",
    "def find_relevant_audio_segments(query, segments, collection, embedding_model, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieve audio segments whose text contains the query string (case-insensitive substring match).\n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "        segments (list): List of segment dicts with 'text', 'start', 'end', 'id'.\n",
    "        collection: ChromaDB collection (not used in this version).\n",
    "        embedding_model: Embedding model (not used in this version).\n",
    "        top_k (int): Not used in this version.\n",
    "    Returns:\n",
    "        List of relevant segment dicts.\n",
    "    \"\"\"\n",
    "    query_lower = query.lower()\n",
    "    relevant_segments = [segment for segment in segments if query_lower in segment[\"text\"].lower()]\n",
    "    print(f\"Total relevant segments found: {len(relevant_segments)}\")\n",
    "    return relevant_segments\n",
    "\n",
    "# Function to sanitize filenames\n",
    "def sanitize_filename(name):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", name).strip()\n",
    "\n",
    "# Function to save audio segments\n",
    "def save_audio_segments(audio_path, segments, query, output_base=\"audio_clips\"):\n",
    "    \"\"\"\n",
    "    Save relevant audio segments as separate files.\n",
    "    \"\"\"\n",
    "    folder_name = sanitize_filename(query)\n",
    "    output_dir = os.path.join(output_base, folder_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    for seg in segments:\n",
    "        start_sample = int(seg['start'] * sr)\n",
    "        end_sample = int(seg['end'] * sr)\n",
    "        clip_waveform = waveform[:, start_sample:end_sample]\n",
    "        out_path = os.path.join(output_dir, f\"segment_{{seg['id']}}_{{int(seg['start'])}}-{{int(seg['end'])}}.wav\")\n",
    "        torchaudio.save(out_path, clip_waveform, sr)\n",
    "        print(f\"Saved: {out_path}\")\n",
    "\n",
    "# Example usage:\n",
    "# segments = result[\"segments\"]  # Ensure you have this from the Whisper transcription\n",
    "# relevant_segments = find_relevant_audio_segments(\"audio\", segments, collection, embedding_model)\n",
    "# save_audio_segments(AUDIO_PATH, relevant_segments, \"audio\")\n",
    "# for seg in relevant_segments:\n",
    "#     print(f\"Start: {seg['start']}s, End: {seg['end']}s, Text: {seg['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Step 9: update OEW-MAIN audio library with the audio created from the search.\n",
    "\n",
    "This step will integrate the audio files created or identified in this pipeline into the main audio library of the OEW system. This ensures that all processed and relevant audio content is available for the user's projects and can be easily accessed or downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽšï¸ Step 10: DAW Integration & Audio Rendering\n",
    "\n",
    "Now that relevant audio segments have been extracted, we can integrate them into a Digital Audio Workstation (DAW) environment. This step demonstrates how to load, visualize, and play back audio clips, enabling further editing or composition. Below, we render audio waveforms and provide playback controls for the extracted segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Render and Play Extracted Audio Segments in Notebook ---\n",
    "\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import os\n",
    "\n",
    "def render_audio_segments(folder_path):\n",
    "    \"\"\"\n",
    "    Display waveforms and playback controls for all audio clips in the given folder.\n",
    "    \"\"\"\n",
    "    audio_files = [f for f in os.listdir(folder_path) if f.endswith(\".wav\")]\n",
    "    if not audio_files:\n",
    "        print(\"No audio clips found in:\", folder_path)\n",
    "        return\n",
    "    for audio_file in sorted(audio_files):\n",
    "        file_path = os.path.join(folder_path, audio_file)\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        plt.figure(figsize=(10, 2))\n",
    "        plt.title(audio_file)\n",
    "        plt.plot(waveform.t().numpy())\n",
    "        plt.xlabel(\"Sample\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.show()\n",
    "        display(ipd.Audio(file_path))\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Example usage: visualize and play audio clips for the last query\n",
    "render_audio_segments(os.path.join(\"audio_clips\", sanitize_filename(\"audio\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¤ Step 11: Exporting Audio Clips\n",
    "\n",
    "After rendering and reviewing audio segments, you may want to export them for sharing, further processing, or integration with other systems. Below are common export options:\n",
    "- **IPFS**: Upload audio to the InterPlanetary File System for decentralized access.\n",
    "- **AWS S3**: Store audio in an Amazon S3 bucket for scalable cloud storage.\n",
    "- **Direct Download**: Save audio to a temporary folder for local or web-based download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export Audio Clips: IPFS, AWS S3, or Direct Download ---\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "# Optional: install required packages for IPFS and AWS S3\n",
    "# %pip install ipfshttpclient boto3\n",
    "\n",
    "def export_audio_clips(folder_path, method=\"direct\", **kwargs):\n",
    "    \"\"\"\n",
    "    Export audio clips using the specified method.\n",
    "    method: \"ipfs\", \"aws\", or \"direct\"\n",
    "    kwargs: Additional arguments for each method.\n",
    "    Returns a list of export URLs or file paths.\n",
    "    \"\"\"\n",
    "    audio_files = [f for f in os.listdir(folder_path) if f.endswith(\".wav\")]\n",
    "    exported = []\n",
    "    if method == \"ipfs\":\n",
    "        # Example: Upload to IPFS (requires ipfshttpclient)\n",
    "        import ipfshttpclient\n",
    "        client = ipfshttpclient.connect()\n",
    "        for audio_file in audio_files:\n",
    "            file_path = os.path.join(folder_path, audio_file)\n",
    "            res = client.add(file_path)\n",
    "            ipfs_url = f\"https://ipfs.io/ipfs/{{res['Hash']}}\"\n",
    "            exported.append(ipfs_url)\n",
    "            print(f\"Exported to IPFS: {{ipfs_url}}\")\n",
    "    elif method == \"aws\":\n",
    "        # Example: Upload to AWS S3 (requires boto3)\n",
    "        import boto3\n",
    "        s3 = boto3.client(\"s3\")\n",
    "        bucket = kwargs.get(\"bucket\")\n",
    "        prefix = kwargs.get(\"prefix\", \"\")\n",
    "        for audio_file in audio_files:\n",
    "            file_path = os.path.join(folder_path, audio_file)\n",
    "            s3_key = os.path.join(prefix, audio_file)\n",
    "            s3.upload_file(file_path, bucket, s3_key)\n",
    "            s3_url = f\"https://{{bucket}}.s3.amazonaws.com/{{s3_key}}\"\n",
    "            exported.append(s3_url)\n",
    "            print(f\"Exported to S3: {{s3_url}}\")\n",
    "    elif method == \"direct\":\n",
    "        # Copy files to a temporary directory for download\n",
    "        temp_dir = tempfile.mkdtemp(prefix=\"audio_export_\")\n",
    "        for audio_file in audio_files:\n",
    "            src = os.path.join(folder_path, audio_file)\n",
    "            dst = os.path.join(temp_dir, audio_file)\n",
    "            shutil.copy2(src, dst)\n",
    "            exported.append(dst)\n",
    "            print(f\"Copied to temp folder: {{dst}}\")\n",
    "        print(f\"All files available in: {{temp_dir}}\")\n",
    "    else:\n",
    "        raise ValueError(\"Unknown export method.\")\n",
    "    return exported\n",
    "\n",
    "# Example usage:\n",
    "# Export to IPFS (requires running IPFS daemon and ipfshttpclient)\n",
    "# export_audio_clips(os.path.join(\"audio_clips\", sanitize_filename(\"audio\")), method=\"ipfs\")\n",
    "\n",
    "# Export to AWS S3 (requires AWS credentials)\n",
    "# export_audio_clips(os.path.join(\"audio_clips\", sanitize_filename(\"audio\")), method=\"aws\", bucket=\"your-bucket\", prefix=\"exports/\")\n",
    "\n",
    "# Export for direct download (local temp folder)\n",
    "export_audio_clips(os.path.join(\"audio_clips\", sanitize_filename(\"audio\")), method=\"direct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 12: RAG Pipeline Monitoring Dashboard\n",
    "\n",
    "To efficiently track and monitor our audio processing pipeline, we'll implement a simple web dashboard that provides:\n",
    "\n",
    "- **Audio Processing Metrics**: Track transcription times, file sizes, and segment counts\n",
    "- **ChromaDB Interactions**: Monitor query frequency, embedding operations, and retrieval times \n",
    "- **RAG Performance**: Visualize context usage decisions, response generation times, and user queries\n",
    "\n",
    "This dashboard runs on `http://127.0.0.1:5001/` and provides real-time insights into the operation of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Implement RAG Pipeline Monitoring Dashboard ---\n",
    "%pip install -q streamlit plotly pandas\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import threading\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import socket\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# Function to find an available port starting from a given port\n",
    "def find_available_port(start_port):\n",
    "    port = start_port\n",
    "    max_port = start_port + 100  # Try 100 ports at most\n",
    "    \n",
    "    while port < max_port:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            try:\n",
    "                s.bind(('127.0.0.1', port))\n",
    "                return port\n",
    "            except OSError:\n",
    "                port += 1\n",
    "    \n",
    "    raise RuntimeError(f\"Could not find available port starting from {start_port}\")\n",
    "\n",
    "# Create a class to track metrics across the entire pipeline\n",
    "class RAGMetricsTracker:\n",
    "    def __init__(self, max_history=100):\n",
    "        self.audio_metrics = {\n",
    "            \"processed_files\": 0,\n",
    "            \"total_duration\": 0,\n",
    "            \"avg_transcription_time\": 0,\n",
    "            \"segments_extracted\": 0,\n",
    "            \"history\": deque(maxlen=max_history)\n",
    "        }\n",
    "        self.chromadb_metrics = {\n",
    "            \"queries\": 0,\n",
    "            \"embeddings_created\": 0,\n",
    "            \"avg_query_time\": 0,\n",
    "            \"query_times\": deque(maxlen=max_history)\n",
    "        }\n",
    "        self.rag_metrics = {\n",
    "            \"total_queries\": 0,\n",
    "            \"context_used\": 0,  # Number of times RAG used additional context\n",
    "            \"direct_answers\": 0,  # Number of times RAG answered directly\n",
    "            \"query_history\": deque(maxlen=max_history)\n",
    "        }\n",
    "        self.timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    def log_audio_process(self, filename, duration, transcription_time, num_segments):\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        self.audio_metrics[\"processed_files\"] += 1\n",
    "        self.audio_metrics[\"total_duration\"] += duration\n",
    "        \n",
    "        # Update running average transcription time\n",
    "        prev_avg = self.audio_metrics[\"avg_transcription_time\"]\n",
    "        prev_count = self.audio_metrics[\"processed_files\"] - 1\n",
    "        self.audio_metrics[\"avg_transcription_time\"] = (prev_avg * prev_count + transcription_time) / self.audio_metrics[\"processed_files\"]\n",
    "        \n",
    "        self.audio_metrics[\"segments_extracted\"] += num_segments\n",
    "        \n",
    "        # Add to history\n",
    "        self.audio_metrics[\"history\"].append({\n",
    "            \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"filename\": filename,\n",
    "            \"duration\": duration,\n",
    "            \"transcription_time\": transcription_time,\n",
    "            \"segments\": num_segments\n",
    "        })\n",
    "    \n",
    "    def log_chromadb_query(self, query, num_results, query_time):\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        self.chromadb_metrics[\"queries\"] += 1\n",
    "        self.chromadb_metrics[\"query_times\"].append(query_time)\n",
    "        self.chromadb_metrics[\"avg_query_time\"] = sum(self.chromadb_metrics[\"query_times\"]) / len(self.chromadb_metrics[\"query_times\"])\n",
    "    \n",
    "    def log_embedding_creation(self, count):\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        self.chromadb_metrics[\"embeddings_created\"] += count\n",
    "    \n",
    "    def log_user_query(self, query, used_context, response_time):\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        self.rag_metrics[\"total_queries\"] += 1\n",
    "        if used_context:\n",
    "            self.rag_metrics[\"context_used\"] += 1\n",
    "        else:\n",
    "            self.rag_metrics[\"direct_answers\"] += 1\n",
    "            \n",
    "        self.rag_metrics[\"query_history\"].append({\n",
    "            \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"query\": query,\n",
    "            \"used_context\": used_context,\n",
    "            \"response_time\": response_time\n",
    "        })\n",
    "    \n",
    "    def export_metrics(self):\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"audio_metrics\": {\n",
    "                \"processed_files\": self.audio_metrics[\"processed_files\"],\n",
    "                \"total_duration\": self.audio_metrics[\"total_duration\"],\n",
    "                \"avg_transcription_time\": self.audio_metrics[\"avg_transcription_time\"],\n",
    "                \"segments_extracted\": self.audio_metrics[\"segments_extracted\"],\n",
    "                \"history\": list(self.audio_metrics[\"history\"])\n",
    "            },\n",
    "            \"chromadb_metrics\": {\n",
    "                \"queries\": self.chromadb_metrics[\"queries\"],\n",
    "                \"embeddings_created\": self.chromadb_metrics[\"embeddings_created\"],\n",
    "                \"avg_query_time\": self.chromadb_metrics[\"avg_query_time\"],\n",
    "                \"query_times\": list(self.chromadb_metrics[\"query_times\"])\n",
    "            },\n",
    "            \"rag_metrics\": {\n",
    "                \"total_queries\": self.rag_metrics[\"total_queries\"],\n",
    "                \"context_used\": self.rag_metrics[\"context_used\"],\n",
    "                \"direct_answers\": self.rag_metrics[\"direct_answers\"],\n",
    "                \"query_history\": list(self.rag_metrics[\"query_history\"])\n",
    "            },\n",
    "            \"timestamp\": self.timestamp\n",
    "        }\n",
    "\n",
    "# Find available ports for our services\n",
    "metrics_port = find_available_port(5002)\n",
    "dashboard_port = find_available_port(5001)\n",
    "print(f\"Selected ports: Dashboard={{dashboard_port}}, Metrics API={{metrics_port}}\")\n",
    "\n",
    "# Create a global metrics tracker instance\n",
    "metrics_tracker = RAGMetricsTracker()\n",
    "\n",
    "# Retroactively log the metrics from our previous operations\n",
    "metrics_tracker.log_audio_process(\n",
    "    filename=AUDIO_PATH,\n",
    "    duration=result[\"segments\"][-1][\"end\"] if len(result[\"segments\"]) > 0 else 0,\n",
    "    transcription_time=3.5,  # Placeholder value\n",
    "    num_segments=len(result[\"segments\"])\n",
    ")\n",
    "\n",
    "metrics_tracker.log_embedding_creation(len(document_embeddings))\n",
    "\n",
    "# Wrap our vector search function to track metrics\n",
    "original_vector_search = vector_search_tool\n",
    "\n",
    "def tracked_vector_search_tool(query: str) -> str:\n",
    "    start_time = time.time()\n",
    "    result = original_vector_search(query)\n",
    "    query_time = time.time() - start_time\n",
    "    metrics_tracker.log_chromadb_query(query, 5, query_time)\n",
    "    return result\n",
    "\n",
    "# Replace the original function with our tracked version\n",
    "vector_search_tool = tracked_vector_search_tool\n",
    "\n",
    "# Create the Streamlit dashboard\n",
    "def create_dashboard():\n",
    "    \"\"\"\n",
    "    Create the Streamlit dashboard for RAG pipeline monitoring\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the Streamlit app with dynamic port for metrics API\n",
    "    dashboard_code = f\"\"\"\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "st.set_page_config(page_title=\"RAG Pipeline Monitor\", layout=\"wide\")\n",
    "\n",
    "st.title(\"ðŸ“Š Agentic RAG Pipeline Monitoring\")\n",
    "\n",
    "# Create metrics columns\n",
    "col1, col2, col3, col4 = st.columns(4)\n",
    "\n",
    "# Initialize session state\n",
    "if 'last_update' not in st.session_state:\n",
    "    st.session_state.last_update = time.time()\n",
    "    st.session_state.metrics_history = []\n",
    "\n",
    "# Function to fetch metrics from the metrics endpoint\n",
    "def get_metrics():\n",
    "    try:\n",
    "        response = requests.get(METRICS_API_URL)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            st.error(f\"Failed to fetch metrics: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error fetching metrics: {e}\")\n",
    "        return None\n",
    "\n",
    "# Set the metrics API endpoint\n",
    "METRICS_API_URL = \"http://127.0.0.1:METRICS_PORT/metrics\"\n",
    "\n",
    "metrics = get_metrics()\n",
    "if metrics:\n",
    "    # Store metrics in history\n",
    "    st.session_state.metrics_history.append(metrics)\n",
    "    if len(st.session_state.metrics_history) > 20:\n",
    "        st.session_state.metrics_history.pop(0)\n",
    "    \n",
    "    # Display metrics in columns\n",
    "    with col1:\n",
    "        st.subheader(\"Audio Processing\")\n",
    "        st.metric(\"Files Processed\", metrics[\"audio_metrics\"][\"processed_files\"])\n",
    "        st.metric(\"Total Duration (sec)\", round(metrics[\"audio_metrics\"][\"total_duration\"], 2))\n",
    "        st.metric(\"Segments Extracted\", metrics[\"audio_metrics\"][\"segments_extracted\"])\n",
    "    \n",
    "    with col2:\n",
    "        st.subheader(\"ChromaDB Operations\")\n",
    "        st.metric(\"Total Queries\", metrics[\"chromadb_metrics\"][\"queries\"])\n",
    "        st.metric(\"Embeddings Created\", metrics[\"chromadb_metrics\"][\"embeddings_created\"])\n",
    "        st.metric(\"Avg Query Time (ms)\", round(metrics[\"chromadb_metrics\"][\"avg_query_time\"] * 1000, 2))\n",
    "    \n",
    "    with col3:\n",
    "        st.subheader(\"RAG Performance\")\n",
    "        st.metric(\"User Queries\", metrics[\"rag_metrics\"][\"total_queries\"])\n",
    "        rag_usage = metrics[\"rag_metrics\"][\"context_used\"]\n",
    "        direct_answers = metrics[\"rag_metrics\"][\"direct_answers\"]\n",
    "        total = rag_usage + direct_answers\n",
    "        \n",
    "        if total > 0:\n",
    "            rag_percentage = (rag_usage / total) * 100\n",
    "            st.metric(\"RAG Usage\", f\"{rag_percentage:.1f}%\")\n",
    "        else:\n",
    "            st.metric(\"RAG Usage\", \"0%\")\n",
    "    \n",
    "    # New IPFS metrics column\n",
    "    with col4:\n",
    "        st.subheader(\"IPFS Integration\")\n",
    "        if \"ipfs_metrics\" in metrics:\n",
    "            ipfs_metrics = metrics[\"ipfs_metrics\"]\n",
    "            st.metric(\"Files Discovered\", ipfs_metrics[\"audio_files_discovered\"])\n",
    "            st.metric(\"Files Processed\", ipfs_metrics[\"audio_files_processed\"])\n",
    "            \n",
    "            # Show search info if available\n",
    "            if ipfs_metrics[\"last_search_query\"]:\n",
    "                st.text(f\"Last search: {ipfs_metrics['last_search_query']}\")\n",
    "                st.text(f\"Results: {ipfs_metrics['last_search_results']}\")\n",
    "        else:\n",
    "            st.text(\"No IPFS metrics available\")\n",
    "    \n",
    "    # Time series chart of audio processing\n",
    "    st.subheader(\"Audio Processing History\")\n",
    "    if metrics[\"audio_metrics\"][\"history\"]:\n",
    "        audio_df = pd.DataFrame(metrics[\"audio_metrics\"][\"history\"])\n",
    "        if not audio_df.empty:\n",
    "            audio_df['timestamp'] = pd.to_datetime(audio_df['timestamp'])\n",
    "            fig = px.scatter(audio_df, x='timestamp', y='transcription_time', \n",
    "                             size='duration', color='segments',\n",
    "                             title=\"Audio Transcription Performance\")\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    # Query visualization\n",
    "    st.subheader(\"Recent User Queries\")\n",
    "    if metrics[\"rag_metrics\"][\"query_history\"]:\n",
    "        query_df = pd.DataFrame(metrics[\"rag_metrics\"][\"query_history\"])\n",
    "        if not query_df.empty:\n",
    "            query_df['timestamp'] = pd.to_datetime(query_df['timestamp'])\n",
    "            query_df['used_context_label'] = query_df['used_context'].map({True: \"RAG\", False: \"Direct\"})\n",
    "            \n",
    "            fig = px.scatter(query_df, x='timestamp', y='response_time',\n",
    "                            color='used_context_label', hover_data=['query'],\n",
    "                            title=\"Query Response Times\")\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "            \n",
    "            # Display recent queries in a table\n",
    "            st.dataframe(query_df[['timestamp', 'query', 'used_context_label', 'response_time']]\n",
    "                        .rename(columns={{'used_context_label': 'Method', 'response_time': 'Time (sec)'}}))\n",
    "    \n",
    "    st.text(f\"Last updated: {{time.strftime('%Y-%m-%d %H:%M:%S')}}\")\n",
    "    st.text(f\"Dashboard port: {{dashboard_port}}, Metrics API port: {{metrics_port}}\")\n",
    "    \n",
    "    # Auto-refresh every 5 seconds\n",
    "    time.sleep(5)\n",
    "    st.experimental_rerun()\n",
    "else:\n",
    "    st.warning(\"Failed to fetch metrics data. Make sure the metrics endpoint is running.\")\n",
    "    st.text(f\"Looking for metrics at: {{METRICS_API_URL}}\")\n",
    "    time.sleep(5)\n",
    "    st.experimental_rerun()\n",
    "\"\"\"\n",
    "    \n",
    "    # Write the Streamlit app to a file\n",
    "    with open(\"rag_dashboard.py\", \"w\") as f:\n",
    "        f.write(dashboard_code)\n",
    "    \n",
    "    # Create a simple Flask server to serve the metrics with dynamic port\n",
    "    flask_code = f\"\"\"\n",
    "from flask import Flask, jsonify\n",
    "import json\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/metrics')\n",
    "def get_metrics():\n",
    "    try:\n",
    "        with open('rag_metrics.json', 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        return jsonify(metrics)\n",
    "    except Exception as e:\n",
    "        return jsonify({{\"error\": str(e)}}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='127.0.0.1', port={metrics_port})\n",
    "\"\"\"\n",
    "    \n",
    "    with open(\"metrics_server.py\", \"w\") as f:\n",
    "        f.write(flask_code)\n",
    "    \n",
    "    # Export the current metrics to a JSON file\n",
    "    with open(\"rag_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics_tracker.export_metrics(), f)\n",
    "    \n",
    "    # Start the flask server in a separate thread\n",
    "    def run_flask():\n",
    "        os.system(\"python metrics_server.py\")\n",
    "    \n",
    "    flask_thread = threading.Thread(target=run_flask)\n",
    "    flask_thread.daemon = True\n",
    "    flask_thread.start()\n",
    "    \n",
    "    # Start the Streamlit dashboard with the selected port\n",
    "    print(\"Starting RAG pipeline dashboard...\")\n",
    "    os.system(f\"streamlit run rag_dashboard.py --server.port={{dashboard_port}}\")\n",
    "\n",
    "# Run the dashboard in a separate thread to avoid blocking the notebook\n",
    "dashboard_thread = threading.Thread(target=create_dashboard)\n",
    "dashboard_thread.daemon = True\n",
    "dashboard_thread.start()\n",
    "\n",
    "print(f\"RAG pipeline monitoring dashboard is starting...\")\n",
    "print(f\"Access the dashboard at http://localhost:{{dashboard_port}}\")\n",
    "print(f\"Metrics API running at http://localhost:{{metrics_port}}/metrics\")\n",
    "\n",
    "# Example of RAG metrics tracking:\n",
    "print(\"\\nExample of RAG metrics tracking:\")\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "# Example of tracking a query to demonstrate the logging\n",
    "def simulate_rag_query(query, use_context=True):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if use_context:\n",
    "        context = tracked_vector_search_tool(query)\n",
    "        # Simulate RAG processing with context\n",
    "        time.sleep(0.5)  # Simulate processing time\n",
    "    else:\n",
    "        # Simulate direct answer without context\n",
    "        time.sleep(0.2)  # Simulate faster processing\n",
    "    \n",
    "    response_time = time.time() - start_time\n",
    "    metrics_tracker.log_user_query(query, use_context, response_time)\n",
    "    \n",
    "    # Export updated metrics\n",
    "    with open(\"rag_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics_tracker.export_metrics(), f)\n",
    "    \n",
    "    return \"Response time: {:.2f}s, Context used: {}\".format(response_time, use_context)\n",
    "\n",
    "# Simulate a few queries with and without context\n",
    "print(simulate_rag_query(\"What are the main topics discussed in the audio?\", use_context=True))\n",
    "print(simulate_rag_query(\"Who is speaking in the audio?\", use_context=True))\n",
    "print(simulate_rag_query(\"What is the background noise?\", use_context=False))\n",
    "print(simulate_rag_query(\"When was this recording made?\", use_context=False))\n",
    "\n",
    "print(f\"\\nOpen the dashboard at http://localhost:{{dashboard_port}} to see these metrics visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Step 13: IPFS Audio Content Discovery and Processing\n",
    "\n",
    "This step implements functionality to discover and retrieve audio content stored on IPFS by examining metadata. This allows the AI to expand its knowledge base by incorporating audio content from decentralized storage systems, effectively giving it access to a wider range of audio sources beyond local files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IPFS Audio Content Discovery and Processing ---\n",
    "\n",
    "%pip install -q ipfshttpclient requests\n",
    "\n",
    "import ipfshttpclient\n",
    "import requests\n",
    "import json\n",
    "import tempfile\n",
    "import os\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "def search_ipfs_audio_content(gateway_url=\"https://ipfs.io/api/v0\", query=None, limit=10):\n",
    "    \"\"\"\n",
    "    Search for audio content on IPFS using metadata\n",
    "    \n",
    "    Args:\n",
    "        gateway_url: IPFS gateway URL\n",
    "        query: Optional search terms\n",
    "        limit: Maximum number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of dicts containing CIDs and metadata for audio files\n",
    "    \"\"\"\n",
    "    print(f\"Searching IPFS for audio content{f' related to {query}' if query else ''}...\")\n",
    "    audio_files = []\n",
    "    \n",
    "    try:\n",
    "        # Try to connect to a local IPFS daemon first\n",
    "        try:\n",
    "            client = ipfshttpclient.connect()\n",
    "            print(\"Connected to local IPFS daemon\")\n",
    "        except:\n",
    "            print(\"No local IPFS daemon found, using gateway API\")\n",
    "            client = None\n",
    "            \n",
    "        # Method 1: If we have a local client, use it to search the DHT\n",
    "        if client:\n",
    "            # Get list of pins if no query provided\n",
    "            if not query:\n",
    "                pins = client.pin.ls(type=\"recursive\")\n",
    "                for pin in pins[\"Keys\"]:\n",
    "                    try:\n",
    "                        # Get metadata for each pin\n",
    "                        metadata = client.object.get(pin)\n",
    "                        if is_audio_by_metadata(metadata):\n",
    "                            audio_files.append({\n",
    "                                \"cid\": pin,\n",
    "                                \"metadata\": metadata,\n",
    "                                \"source\": \"local_ipfs\"\n",
    "                            })\n",
    "                            if len(audio_files) >= limit:\n",
    "                                break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error examining pin {pin}: {e}\")\n",
    "            else:\n",
    "                # If query provided, search IPNS/IPFS links containing audio-related terms\n",
    "                # This is simplified as full DHT search requires more complex code\n",
    "                pass\n",
    "        \n",
    "        # Method 2: Use IPFS gateway API or a pinning service\n",
    "        # This is a simplified implementation that would need a specific gateway API supporting search\n",
    "        if query and len(audio_files) < limit:\n",
    "            search_url = f\"{gateway_url}/search?q={query}+audio+filetype:mp3+filetype:wav\"\n",
    "            try:\n",
    "                response = requests.get(search_url, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    results = response.json()\n",
    "                    for result in results:\n",
    "                        if is_audio_by_metadata(result.get(\"metadata\", {})):\n",
    "                            audio_files.append({\n",
    "                                \"cid\": result.get(\"cid\"),\n",
    "                                \"metadata\": result.get(\"metadata\", {}),\n",
    "                                \"source\": \"gateway\"\n",
    "                            })\n",
    "                            if len(audio_files) >= limit:\n",
    "                                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error searching gateway: {e}\")\n",
    "                \n",
    "        # Fallback: Use known audio CIDs for demonstration\n",
    "        if len(audio_files) == 0:\n",
    "            print(\"No audio files found via search, using example files for demonstration\")\n",
    "            # These are example CIDs - they would need to be replaced with actual audio CIDs\n",
    "            example_audio_cids = [\n",
    "                \"QmYwAPJzv5CZsnA625s3Xf2nemtYgPpHdWEz79ojWnPbdG/demo.mp3\",\n",
    "                \"QmZ4tDuvesekSs4qM5ZBKpXiZGun7S2CYtEZRB3DYXkjGx/audio_sample.wav\"\n",
    "            ]\n",
    "            audio_files = [{\"cid\": cid, \"metadata\": {\"type\": \"audio\"}, \"source\": \"example\"} for cid in example_audio_cids]\n",
    "            \n",
    "        print(f\"Found {len(audio_files)} audio files on IPFS\")\n",
    "        return audio_files\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error searching IPFS: {e}\")\n",
    "        return []\n",
    "\n",
    "def is_audio_by_metadata(metadata):\n",
    "    \"\"\"\n",
    "    Check if a file is an audio file based on its metadata\n",
    "    \n",
    "    Args:\n",
    "        metadata: File metadata dict\n",
    "        \n",
    "    Returns:\n",
    "        Boolean indicating if the file is likely an audio file\n",
    "    \"\"\"\n",
    "    # Check MIME type if available\n",
    "    mime_type = metadata.get(\"MimeType\", \"\")\n",
    "    if mime_type.startswith(\"audio/\"):\n",
    "        return True\n",
    "        \n",
    "    # Check file extension\n",
    "    name = metadata.get(\"Name\", \"\")\n",
    "    audio_extensions = ['.mp3', '.wav', '.ogg', '.flac', '.m4a', '.aac']\n",
    "    if any(name.lower().endswith(ext) for ext in audio_extensions):\n",
    "        return True\n",
    "        \n",
    "    # Check metadata tags\n",
    "    tags = metadata.get(\"Tags\", [])\n",
    "    audio_tags = [\"audio\", \"sound\", \"music\", \"recording\", \"voice\", \"speech\"]\n",
    "    if any(tag in audio_tags for tag in tags):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def retrieve_audio_from_ipfs(cid, gateway_url=\"https://ipfs.io/ipfs\"):\n",
    "    \"\"\"\n",
    "    Download audio file from IPFS\n",
    "    \n",
    "    Args:\n",
    "        cid: Content identifier for the IPFS file\n",
    "        gateway_url: IPFS gateway URL\n",
    "        \n",
    "    Returns:\n",
    "        Path to downloaded file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First try with a local IPFS client\n",
    "        try:\n",
    "            client = ipfshttpclient.connect()\n",
    "            print(f\"Retrieving {{cid}} using local IPFS node\")\n",
    "            \n",
    "            # Create temp file\n",
    "            fd, temp_path = tempfile.mkstemp(suffix=\".\" + cid.split(\".\")[-1] if \".\" in cid else \".mp3\")\n",
    "            os.close(fd)\n",
    "            \n",
    "            # Get the file from IPFS and save it\n",
    "            client.get(cid, target=temp_path)\n",
    "            return temp_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Local IPFS retrieval failed: {e}\")\n",
    "            \n",
    "            # Fall back to gateway\n",
    "            file_url = f\"{gateway_url}/{cid}\"\n",
    "            print(f\"Retrieving {{file_url}} using gateway\")\n",
    "            \n",
    "            response = requests.get(file_url, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                # Create temp file\n",
    "                fd, temp_path = tempfile.mkstemp(suffix=\".\" + cid.split(\".\")[-1] if \".\" in cid else \".mp3\")\n",
    "                with os.fdopen(fd, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=1024):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                return temp_path\n",
    "            else:\n",
    "                print(f\"Failed to retrieve file: {response.status_code}\")\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving from IPFS: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_ipfs_audio_content(query=None, limit=3):\n",
    "    \"\"\"\n",
    "    Main function to search for, retrieve, and process audio content from IPFS\n",
    "    \n",
    "    Args:\n",
    "        query: Optional search query\n",
    "        limit: Maximum number of files to process\n",
    "        \n",
    "    Returns:\n",
    "        List of processed documents\n",
    "    \"\"\"\n",
    "    print(f\"Searching for{''+(' '+query if query else '')} audio content on IPFS...\")\n",
    "    audio_files = search_ipfs_audio_content(query=query, limit=limit)\n",
    "    \n",
    "    processed_documents = []\n",
    "    \n",
    "    for i, audio_file in enumerate(audio_files):\n",
    "        print(f\"\\nProcessing IPFS audio file {{i+1}}/{{len(audio_files)}}: {{audio_file['cid']}}\")\n",
    "        \n",
    "        # Retrieve the audio file\n",
    "        local_path = retrieve_audio_from_ipfs(audio_file['cid'])\n",
    "        if not local_path:\n",
    "            print(\"Failed to retrieve audio file\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            print(f\"Downloaded to {{local_path}}, transcribing...\")\n",
    "            \n",
    "            # Transcribe using our existing Whisper model\n",
    "            result = model.transcribe(local_path)\n",
    "            text_content = result[\"text\"]\n",
    "            \n",
    "            # Create document\n",
    "            ipfs_doc = AudioDocument(text_content)\n",
    "            ipfs_doc.metadata = {\n",
    "                \"source\": \"ipfs\",\n",
    "                \"cid\": audio_file['cid'],\n",
    "                \"ipfs_metadata\": audio_file['metadata']\n",
    "            }\n",
    "            \n",
    "            # Add to our collection\n",
    "            processed_documents.append(ipfs_doc)\n",
    "            \n",
    "            # Process audio segments\n",
    "            segments = result[\"segments\"]\n",
    "            relevant_segments = find_relevant_audio_segments(query if query else \"audio\", \n",
    "                                                            segments, collection, embedding_model)\n",
    "            \n",
    "            # Save segments (optional)\n",
    "            if relevant_segments:\n",
    "                query_term = query if query else \"ipfs_audio\"\n",
    "                save_audio_segments(local_path, relevant_segments, \n",
    "                                  f\"ipfs_{{sanitize_filename(query_term)}}_{{i}}\")\n",
    "            \n",
    "            # Clean up the temporary file\n",
    "            try:\n",
    "                os.unlink(local_path)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing audio file: {e}\")\n",
    "            # Clean up on error\n",
    "            try:\n",
    "                os.unlink(local_path)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # If we found and processed any documents, add them to our RAG system\n",
    "    if processed_documents:\n",
    "        print(f\"\\nAdding {{len(processed_documents)}} IPFS audio documents to RAG system...\")\n",
    "        \n",
    "        # Split documents into chunks\n",
    "        ipfs_docs = text_splitter.split_documents(processed_documents)\n",
    "        \n",
    "        # Attach sound tags (we'll just use generic ones since we don't have the specific audio)\n",
    "        for doc in ipfs_docs:\n",
    "            doc.page_content = f\"Transcription from IPFS: {{doc.page_content}}\\nSource: IPFS CID {{doc.metadata.get('cid', 'unknown')}}\"\n",
    "        \n",
    "        # Create embeddings\n",
    "        doc_texts = [doc.page_content for doc in ipfs_docs]\n",
    "        document_embeddings = embedding_model.encode(doc_texts, convert_to_numpy=True)\n",
    "        \n",
    "        # Add to ChromaDB\n",
    "        start_idx = collection.count()\n",
    "        for i, embedding in enumerate(document_embeddings):\n",
    "            collection.add(\n",
    "                ids=[f\"ipfs_{{start_idx + i}}\"],\n",
    "                embeddings=[embedding.tolist()],\n",
    "                metadatas=[{\"text\": doc_texts[i], \"source\": \"ipfs\"}]\n",
    "            )\n",
    "        \n",
    "        print(f\"Successfully added {{len(ipfs_docs)}} IPFS audio document chunks to the RAG system\")\n",
    "        \n",
    "        # Update metrics\n",
    "        metrics_tracker.log_embedding_creation(len(document_embeddings))\n",
    "    else:\n",
    "        print(\"No IPFS audio documents were processed\")\n",
    "    \n",
    "    return processed_documents\n",
    "\n",
    "# Example usage - try to find audio related to music\n",
    "# Uncomment to execute\n",
    "# ipfs_docs = process_ipfs_audio_content(query=\"music\", limit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating IPFS Audio Content Integration\n",
    "\n",
    "Let's demonstrate how to use the IPFS audio discovery and integration functionality with a simple example. You can customize the search query to find specific types of audio content on IPFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Demo: Integrate IPFS Audio Content with RAG ---\n",
    "\n",
    "# Search for interview audio on IPFS and process it\n",
    "ipfs_docs = process_ipfs_audio_content(query=\"interview\", limit=1)\n",
    "\n",
    "# If we found any documents, test a query that might use this new knowledge\n",
    "if ipfs_docs:\n",
    "    print(\"\\n--- Testing RAG with IPFS content ---\")\n",
    "    test_query = \"What interviews are available on IPFS?\"\n",
    "    print(f\"Query: {test_query}\")\n",
    "    \n",
    "    # Get context from our RAG system (which now includes IPFS content)\n",
    "    context = vector_search_tool(test_query)\n",
    "    print(f\"Retrieved context:\\n{context}\\n\")\n",
    "    \n",
    "    # Generate a response using our LLM\n",
    "    prompt = f\"\"\"\n",
    "You are an AI assistant with knowledge about audio content.\n",
    "Based on the following context, please answer the question: {test_query}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    \n",
    "    response = llm(prompt, max_tokens=500)\n",
    "    print(f\"LLM Response:\\n{response['choices'][0]['text']}\")\n",
    "else:\n",
    "    print(\"\\nNo IPFS content was retrieved. You can try again with a different query.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Update Metrics Dashboard with IPFS Integration ---\n",
    "\n",
    "# Extend our RAGMetricsTracker class to include IPFS metrics\n",
    "class IPFSMetrics:\n",
    "    def __init__(self):\n",
    "        self.audio_files_discovered = 0\n",
    "        self.audio_files_processed = 0\n",
    "        self.retrieval_errors = 0\n",
    "        self.processing_errors = 0\n",
    "        self.last_search_query = \"\"\n",
    "        self.last_search_results = 0\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"audio_files_discovered\": self.audio_files_discovered,\n",
    "            \"audio_files_processed\": self.audio_files_processed,\n",
    "            \"retrieval_errors\": self.retrieval_errors,\n",
    "            \"processing_errors\": self.processing_errors,\n",
    "            \"last_search_query\": self.last_search_query,\n",
    "            \"last_search_results\": self.last_search_results\n",
    "        }\n",
    "\n",
    "# Add IPFS metrics to our tracker\n",
    "metrics_tracker.ipfs_metrics = IPFSMetrics()\n",
    "\n",
    "# Update the export_metrics method to include IPFS metrics\n",
    "original_export_metrics = metrics_tracker.export_metrics\n",
    "\n",
    "def extended_export_metrics():\n",
    "    metrics = original_export_metrics()\n",
    "    metrics[\"ipfs_metrics\"] = metrics_tracker.ipfs_metrics.to_dict()\n",
    "    return metrics\n",
    "\n",
    "# Replace the original method\n",
    "metrics_tracker.export_metrics = extended_export_metrics\n",
    "\n",
    "# Update the dashboard code to display IPFS metrics\n",
    "dashboard_code = \"\"\"\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "st.set_page_config(page_title=\"RAG Pipeline Monitor\", layout=\"wide\")\n",
    "\n",
    "st.title(\"ðŸ“Š Agentic RAG Pipeline Monitoring\")\n",
    "\n",
    "# Create metrics columns\n",
    "col1, col2, col3, col4 = st.columns(4)\n",
    "\n",
    "# Initialize session state\n",
    "if 'last_update' not in st.session_state:\n",
    "    st.session_state.last_update = time.time()\n",
    "    st.session_state.metrics_history = []\n",
    "\n",
    "# Function to fetch metrics from the metrics endpoint\n",
    "def get_metrics():\n",
    "    try:\n",
    "        response = requests.get(METRICS_API_URL)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            st.error(f\"Failed to fetch metrics: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error fetching metrics: {e}\")\n",
    "        return None\n",
    "\n",
    "# Set the metrics API endpoint\n",
    "METRICS_API_URL = \"http://127.0.0.1:METRICS_PORT/metrics\"\n",
    "\n",
    "metrics = get_metrics()\n",
    "if metrics:\n",
    "    # Store metrics in history\n",
    "    st.session_state.metrics_history.append(metrics)\n",
    "    if len(st.session_state.metrics_history) > 20:\n",
    "        st.session_state.metrics_history.pop(0)\n",
    "    \n",
    "    # Display metrics in columns\n",
    "    with col1:\n",
    "        st.subheader(\"Audio Processing\")\n",
    "        st.metric(\"Files Processed\", metrics[\"audio_metrics\"][\"processed_files\"])\n",
    "        st.metric(\"Total Duration (sec)\", round(metrics[\"audio_metrics\"][\"total_duration\"], 2))\n",
    "        st.metric(\"Segments Extracted\", metrics[\"audio_metrics\"][\"segments_extracted\"])\n",
    "    \n",
    "    with col2:\n",
    "        st.subheader(\"ChromaDB Operations\")\n",
    "        st.metric(\"Total Queries\", metrics[\"chromadb_metrics\"][\"queries\"])\n",
    "        st.metric(\"Embeddings Created\", metrics[\"chromadb_metrics\"][\"embeddings_created\"])\n",
    "        st.metric(\"Avg Query Time (ms)\", round(metrics[\"chromadb_metrics\"][\"avg_query_time\"] * 1000, 2))\n",
    "    \n",
    "    with col3:\n",
    "        st.subheader(\"RAG Performance\")\n",
    "        st.metric(\"User Queries\", metrics[\"rag_metrics\"][\"total_queries\"])\n",
    "        rag_usage = metrics[\"rag_metrics\"][\"context_used\"]\n",
    "        direct_answers = metrics[\"rag_metrics\"][\"direct_answers\"]\n",
    "        total = rag_usage + direct_answers\n",
    "        \n",
    "        if total > 0:\n",
    "            rag_percentage = (rag_usage / total) * 100\n",
    "            st.metric(\"RAG Usage\", f\"{rag_percentage:.1f}%\")\n",
    "        else:\n",
    "            st.metric(\"RAG Usage\", \"0%\")\n",
    "    \n",
    "    # New IPFS metrics column\n",
    "    with col4:\n",
    "        st.subheader(\"IPFS Integration\")\n",
    "        if \"ipfs_metrics\" in metrics:\n",
    "            ipfs_metrics = metrics[\"ipfs_metrics\"]\n",
    "            st.metric(\"Files Discovered\", ipfs_metrics[\"audio_files_discovered\"])\n",
    "            st.metric(\"Files Processed\", ipfs_metrics[\"audio_files_processed\"])\n",
    "            \n",
    "            # Show search info if available\n",
    "            if ipfs_metrics[\"last_search_query\"]:\n",
    "                st.text(f\"Last search: {ipfs_metrics['last_search_query']}\")\n",
    "                st.text(f\"Results: {ipfs_metrics['last_search_results']}\")\n",
    "        else:\n",
    "            st.text(\"No IPFS metrics available\")\n",
    "    \n",
    "    # Time series chart of audio processing\n",
    "    st.subheader(\"Audio Processing History\")\n",
    "    if metrics[\"audio_metrics\"][\"history\"]:\n",
    "        audio_df = pd.DataFrame(metrics[\"audio_metrics\"][\"history\"])\n",
    "        if not audio_df.empty:\n",
    "            audio_df['timestamp'] = pd.to_datetime(audio_df['timestamp'])\n",
    "            fig = px.scatter(audio_df, x='timestamp', y='transcription_time', \n",
    "                             size='duration', color='segments',\n",
    "                             title=\"Audio Transcription Performance\")\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    # Query visualization\n",
    "    st.subheader(\"Recent User Queries\")\n",
    "    if metrics[\"rag_metrics\"][\"query_history\"]:\n",
    "        query_df = pd.DataFrame(metrics[\"rag_metrics\"][\"query_history\"])\n",
    "        if not query_df.empty:\n",
    "            query_df['timestamp'] = pd.to_datetime(query_df['timestamp'])\n",
    "            query_df['used_context_label'] = query_df['used_context'].map({True: \"RAG\", False: \"Direct\"})\n",
    "            \n",
    "            fig = px.scatter(query_df, x='timestamp', y='response_time',\n",
    "                            color='used_context_label', hover_data=['query'],\n",
    "                            title=\"Query Response Times\")\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "            \n",
    "            # Display recent queries in a table\n",
    "            st.dataframe(query_df[['timestamp', 'query', 'used_context_label', 'response_time']]\n",
    "                        .rename(columns={'used_context_label': 'Method', 'response_time': 'Time (sec)'}))\n",
    "    \n",
    "    st.text(f\"Last updated: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    st.text(f\"Dashboard port: {dashboard_port}, Metrics API port: {metrics_port}\")\n",
    "    \n",
    "    # Auto-refresh every 5 seconds\n",
    "    time.sleep(5)\n",
    "    st.experimental_rerun()\n",
    "else:\n",
    "    st.warning(\"Failed to fetch metrics data. Make sure the metrics endpoint is running.\")\n",
    "    st.text(f\"Looking for metrics at: {METRICS_API_URL}\")\n",
    "    time.sleep(5)\n",
    "    st.experimental_rerun()\n",
    "\"\"\"\n",
    "\n",
    "# Write the updated Streamlit app to a new file\n",
    "with open(\"rag_dashboard_with_ipfs.py\", \"w\") as f:\n",
    "    f.write(dashboard_code)\n",
    "\n",
    "print(\"Updated dashboard code with IPFS metrics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
